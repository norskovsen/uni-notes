* TA Instructor Info
	- Frederik Hvilshøj
	- Email: fhvilshoj@gmail.com

* Important
** Dictionary 
- *RHS:* Right Hand Side

** Jacobian 
- If we have a function $f(z): \mathbb{R}^{a} \rightarrow \mathbb{R}^b$ such that $f(z) = [f_1(z),\dots, f_b(z)]$ then the Jacobian is the matrix 
\begin{equation}
  J_{i,j} = \frac{\partial f_i}{\partial z_j}
\end{equation} 
of size $b\times a$. 

* The Learning Problem (1)
** Components of Learning
- The main components for the learning problem
	- An input $x$
	- The unknown target function $f: \mathcal X \to \mathcal Y$
		- $\mathcal X$ is the input space
		- $\mathcal Y$ is the output space
	- There is a set of data $\mathcal D$ of input output examples $(x_1, y_1), \cdots, (x_N, y_N)$
		- where $y_n = f(x_n)$ for $n = 1,\dots,N$
		- often referred to as data points
	- The learning algorithm that uses dataset $\mathcal D$ to pick a formula $g: \mathcal X \to \mathcal Y$ that approximates $f$
		- choose $g$ from a set of candidate formulas under consideration which is called the hypothesis set $\mathcal H$
		- $\mathcal H$ could be the set of all linear formulas 
§
** A Simple Learning Model 
- The hypothesis set and learning model is referred informally to as the /learning model/

- A simple learning model (the /perceptron/)
	- Let $\mathcal X = \mathbb R ^d$ where $\mathcal X = \mathbb R ^d$ is the $d$-dimensional Euclidean space be the input space
	- Let $\mathcal Y = \{+1, -1\}$ be the output space
	- The hypothesis set $\mathcal H$ is specified through a functional form that all $h \in \mathcal H$ share
		- The functional form $h(x)$ chosen is to give weights to the different coordinates of $x$ which reflects their importance
	- The weighted score is compared to a threshold value which decides whether the output is $+1$ or $-1$
\begin{equation}
  \begin{split} 
    \text{Approve credit if} \ \sum_{i=1}^d w_ix_i &> \text{threshold} \\
    \text{Deny credit if} \ \sum_{i=1}^d w_ix_i &< \text{threshold}
  \end{split}
\end{equation}
This can be written mode compactly as 
\begin{equation}
  h(x) = \text{sign} \bigg( \bigg( \sum_{i=1}^d w_ix_i \bigg) + b \bigg)
\end{equation}
where $x_1, \dots, x_n$ are the components of the vector $\pmb x$ and $b$ is the threshold

- The bias can be added as the first weight $w_0 = b$ and adding a fixed input $x_0 = 1$ gives us the same result
	- With this convention the previous equation can be written as
\begin{equation}
  h(\pmb x) = \text{sign}(\pmb w^T \pmb x)
\end{equation}
where $\pmb w$ is the weight vector and $\pmb x$ is the input vector

- The /perceptron learning algorithm/ will determine $\pmb w$ based on the data
	- To use this the data should be linearly separable
		- Means that there is a $\pmb w$ which achieve the correct decision $h(\pmb x_n)=y_n$ on all data examples
	- The algorithm finds $\pmb w$ using the following simple iterative method
		- At iteration $t$ where $t = 0,1,2, \dots$ , there is a current value of the weight vector $\pmb w(t)$
		- The algorithm picks an example from $(x_1, y_1), \cdots, (x_N, y_N)$ $(x(t), y(t))$ and uses it to update $\pmb w(t)$ 
		- The update rule is  $\pmb w(t+1) = \pmb w(t) + y(t)\pmb x(t)$


- The learning algorithm is guaranteed to arrive at the right solution at the end

** Types of Learning 
*** Supervised Learning
- The training data contains explicit examples of the correct output for given inputs
- There are two variations of this protocol
	- *Active learning:* where the data set is acquired though queries that we make
		- We get to choose a point $\pmb x$ in the input space and the supervisor reports to us the target value for $\pmb x$
		- This opens up a strategic chosen $\pmb x$
	- *Online learning:* the data set is given to the algorithm one example at a time
		- This happens when we have streaming data that the algorithm has to process on the run
		- Useful for limitations of computing at storage
		- Can also be used in other paradigms of learning
		- e.g. a movie recommendation system

*** Reinforcement Learning
- When training data does not explicitly contain the correct output for each input we are no longer in the supervised setting
- The training example does not contain the target output
	- It contains so possible outputs of how good that output is
- The training examples in reinforcement learning are of the form
#+BEGIN_CENTER
	( input, some output, grade for this output )
#+END_CENTER 
- The examples does not say how good the inputs would have been for other settings
- Can e.g. be useful for learning to play a game
	
*** Unsupervised Learning
- In unsupervised learning the data does not contain any output information at all
	- We are just given input examples $\pmb x_1, \dots, \pmb x_N$

- The decision regions in unsupervised learning are the same as the one in supervised learning without label
- It can be viewed as a task of spontaneously finding patterns and structure in input data
- It can be a precursor to supervised learning 

** Linear Regression and Orthogonal Projections
- *Lemma 1.* Given a vector $y$ the closest point in $V$ to $y$, i.e. $\arg\min_v \in V : ||v-y||_2^2$  is the orthogonal projection of $y$ onto $V$.
- The optimal weight vector $w$ is found using the following formula 
\begin{equation}
  w = (X^TX)^{-1}X^Ty
\end{equation}
where $X$ is a $n \times d$ data matrix where each row is an input point and a $n \times 1$ vector y of targets

** Is Learning Feasible 
*** General
- To see the relationship between the data $\mathcal D$ and the data outside the *Hoeffding Inequality* is used
	- It states for a random variable $\nu$ in terms of the parameter $\mu$ and the sample size $N$ that 
\begin{equation}
      \mathbb P[|\nu - \mu > \epsilon] \geq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
- This shows that as one increase the sample size $\nu$ gets closer to $\mu$ for some small number $\epsilon$ 

- The error rate within the sample is called the *in-sample error*
\begin{equation}
  \begin{split} 
    E_\text{in}(h) &= \text{(fraction of $D$ where $f$ and $h$ disagree)} \\
      &= \frac1N \sum_{n=1}^N\llbracket h(\pmb x_n \ne f(\pmb x_n) \rrbracket
  \end{split}
\end{equation}
- where $\llbracket \text{statement} \rrbracket = 1$ if the statement is true and $0$ otherwise

- The *out-of-sample error* is defined as 
\begin{equation}
  E_\text{out}(h)=\mathbb P [h(\pmb x) \ne f(\pmb x)]
\end{equation}

- Using in-sample and out-of-sample error the *Hoeffding Inequality* can be written as 
\begin{equation}
        \mathbb [|E_\text{in}(h)-E_\text{out}(h) > \epsilon] \leq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
- where $N$ is the number of training examples

*** Feasibility of Learning 
 - $\mathcal D$ does not deterministic tell us something about $f$ outside of $\mathcal D$ but it gives us a probabilistic answer
 - Since the *Hoeffding Inequality* tells us that $E_\text{in}(g) \approx E_\text{out}(g)$ for a large enough $N$ $E_\text{in}(g)$ seems like a good proxy for $E_\text{out}(g)$ 

 - The feasibility of learning is split into two questions
	 1. Can we make sure that $E_\text{out}(g)$ is close enough to $E_\text{in}(g)$
	 2. Can we make $E_\text{in}(g)$ small enough?

 - *The complexity of* $\mathcal H$: If the number of hypothesis $M$ goes up we run more risk that $E_\text{in}(g)$ will be a poor estimator of $E_\text{out}(g)$
	 - $M$ can be though of as a measure of the complexity of the hypothesis set $\mathcal H$ that we use
	 - The bigger the $M$ the higher the change of finding a small enough $E_\text{in}(g)$ becomes  

 - *The complexity of* $f$: A more complex $f$ is harder to learn
	 - A more complex hypothesis makes the likelihood that the $E_\text{in}(g)$ and $E_\text{out}(g)$ are approximately the same smaller
	 - If the target function $f$ is two hard one may not be able to learn it at all
	 - Most target functions in real life are not too complex

** Error and Noise 
*** Error Measures 
- An *error measure* quantifies how well each hypothesis $h$ in the target function $f$ 
\begin{equation}
	\text{Error} = E(h,f)
\end{equation}

- While $E(h,f)$ is based on the entirety of $h$ and $f$ is almost universally defined based on the error of individual points $\pmb x$
	- If we define a pointwise error measure $e(h(\pmb x), f(\pmb x))$ the overall error will be the average of the pointwise error
	- The defined error should be defined on the use of the application 

*** Noisy target
- In a real world application the data that one learns from is not generated from a deterministic target function but in a noisy way
	- Formally we have a *target distribution* $P(y \mid \pmb x)$ instead of a taget function
	- One can think of a *noisy target* as a deterministic target, plus added noise

* Training versus Testing (2)
** Theory of Generalization
*** General 
- The *generalization error* is the discrepancy between $E_\text{in}$ and $E_\text{out}$
	- The Hoeffding Inequality provides a way to characterize it with a probabilistic bound
	- The Hoeffding Inequality can be rephrased as follows: pick a tolerance level $\delta$ e.g. 0.05 and assert with probability at least $1-\delta$ that
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac1{2N} \ln \frac{2M}\delta} 
\end{equation}
- It is referred to as the *generalization bound* 

*** Effective Number of Hypotheses
- The *growth function* is the quantity that will formalize the effective number of hypotheses
	- It will replace $M$ when $M=\infty$ in the generalization bound

- *Definition 2.1.* Let $x_1, \dots, x_N \in \mathcal X$. The *dichotomies* generated by $\mathcal H$ on these points are defined by 
\begin{equation}
  \mathcal H (\pmb x_1, \dots, \pmb x_N= \{ (h(\pmb x_1), \dots, h(\pmb x_N ) \mid h \in \mathcal H)
\end{equation}

- *Definition 2.2.* The *growth function* is defined for a hypothesis set $\mathcal H$ by 
\begin{equation}
  m_\mathcal{H}(N) = \underset{\pmb x_1, \dots, \pmb x_N \in \mathcal X}{\text{max}} | \mathcal H(\pmb x_1, \dots, \pmb x_N)|
\end{equation}
- where $|\cdot|$ denotes the number of elements of the set

- That $\mathcal H$ can *shatter* $\pmb x_1, \dots, \pmb x_N$ signifies that $\mathcal H$ is as diverse as can be on this particular sample

- *Definition 2.3.* If no data set of size $k$ can be shattered by $\mathcal H$, then $k$ is said to be a break point for $\mathcal H$ 

- If $k$ is a break point, then $m_\mathcal{H}(k) < 2^k$ 

*** Bounding the growth function
- *Definition 2.4.* $B(N,k)$ is the maximum number of dichotomies on $N$ points such that no subset of size $k$ of the $N$ points can be shattered by these dichotomies 

- *Lemma 2.3.* (Sauer's Lemma)
\begin{equation}
  B(N,k) \leq \sum_{i=0}^{k-1}\binom N i
\end{equation}

- *Theorem 2.4.* If $m_\mathcal H (k) < 2^k$ for some value of $k$, then
\begin{equation}
  m_\mathcal H (n) \leq \sum_{i=0}^{k-1} \binom N i
\end{equation}
- for all $N$. The RHS is polynomial in $N$ of degree $k-1$ 

*** The VC Dimension 
- *Definition 2.5.* The *Vapnik-Chervonekis dimension* of a hypothesis set $\mathcal H$, denoted by $d_{vc}(\mathcal H)$ or simply $d_{vc}$ is the largest value of $N$ for which $m_\mathcal H (N) = 2^N$. If $m_\mathcal H (N) = 2^N$ for all $N$, then $d_{vc}(\mathcal H) = \infty$.

- No smaller breakpoint than $k = d_{vc} +1$ exists

\begin{equation}
    d_{vc} \geq N \iff \text{there \textbf{exists} $\mathcal D$ of size $N$ such that $\mathcal H$ shatters $\mathcal D$}
\end{equation}

- The VC dimension of a $d$ dimensional perceptron is $d+1$. 

*** The VC Generalization Bound 
- *Theorem 2.5.* (VC generalization bound). For any tolerance $\delta > 0$, 
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}
\end{equation}
- with probability $\geq 1-\delta$ 

** Interpreting the Generalization Bound
*** General
- The VC generalization bound is a universal result
	- It applies to all hypotheses set, learning algorithms, input spaces, probability distributions and binary target functions
	- The bound is quite loose
	- It can be used as a guideline for generalization
	- Learning models with lower $d_\text{vc}$ tend to better than those with higher $d_\text{vc}$ 

*** Sample Complexity 
- The *sample complexity* denotes how many training examples $N$ are needed to achieve a certain generalization performance
	- The performance is specified using two parameters $\epsilon$ and $\delta$ 
		- The error tolerance $\epsilon$ determines the allowed generalization error
		- The confidence parameter $\delta$ determines how often the error tolerance $\epsilon$ is violated
	- How fast $N$ grows as $\epsilon$ and $\delta$ become smaller indicates the amount of data needed for a good generalization

- From the VC generalization bound it follows that
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4m_\mathcal H (2N)}\delta)
\end{equation}
- If $m_\mathcal H(2N)$ is replaced by its generalization polynomial upper bound we get that
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4((2N)^{d_\text{vc}} +1)} \delta)
\end{equation}
- The numerical value for $N$ can be obtained using simple iterative methods

*** Penalty for Model Complexity
- Often we have a fixed dataset, we can the use the Generalization bound to find out what performance we can expect to get
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}  
\end{equation}
- We can again use the polynomial bound based on $d_\text{vc}$ instead of $m_\mathcal H(2n)$ 
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

- We often denote the second as $\Omega(N,\mathcal H, \delta)$ and call it the penalty
\begin{equation}
  \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

- More complex models help $E_\text{in}$ and hurt $\Omega(N,\mathcal H, \delta)$
	- The optimal model is one that minimizes a combination of the two terms

[[file:Training versus Testing (2)/screenshot_2018-09-12_19-20-23.png]]

*** The Test Set
- One often estimates $E_\text{out}$ by using a *test set* that the learning algorithm has not seen before
	- Called $E_\text{test}$

** Approximation-Generalization Tradeoff 
*** Bias and Variance 
- The bias variance decomposition out-of-sample error is 
\begin{equation}
  E_\text{out}(g^{(\mathcal D)}) = \mathbb E_{\pmb x} \big[ (g^{(\mathcal D)}(\pmb x) - f(\pmb x))^2 \big]
\end{equation}

- The function $\bar g (\pmb x)$ can be interpreted in the following operational way
	1. Generate many data sets $\mathcal D_1, \dots, \mathcal D_K$
	2. Apply the learning algortihm to each data set obtaining final hypotheses $g_1,\dots,g_K$.
	3. The average function for any $\pmb x$ is then estimated by $\bar g(\pmb x) \approx \frac1K \sum_{k=1}^K g_k(\pmb x)$ 

- The *bias* is $\text{bias}(\pmb x) = (\bar g(\pmb x) - f(\pmb x))^2$
	- It measures how much the average function deviates from the target function 

- The *variance* is 
\begin{equation}
	\text{var}(\pmb x)   = E_\mathcal D [(g^{\mathcal D}(x) - \bar g(\pmb x))^2]
\end{equation}
- Says how much the different hypotheses varies

- Since bias and variance cannot be computed in a real model
	- They are purly a conceptual tool used when developing a model

*** The Learning Curve
[[file:Training versus Testing (2)/screenshot_2018-09-12_20-08-30.png]]

- For a simpler model the learning curves converge more quickly but to worse ultimate performance
	- The in-sample error learning curve is increasing in $N$
	- The out-of-sample error learning curve is decreasing in $N$

* The Linear Model (3)
** Linear Regression
*** The algorithm 
- The linear regression algorithm is based on minimizing the squared error between $h(x)$ and $y$
\begin{equation}
  E_\text{out}(h) = \mathbb E[(h(\pmb x) - y)^2]
\end{equation}
where the expected value is taken with respect to the joint probability distribution $P(x,y)$ 

- The goal is to find an hypothesis that achieves a small $E_\text{out}(h)$
	- Since the distribution $P(\pmb x, y)$ is unknown $E_\text{out}(h)$ cannot be computed the in-sample version is therefore used instead
\begin{equation}
	E_\text{in}(h) = \frac{1}{n}\sum_{n=1}^N(h(\pmb x_n) - y_n)^2
\end{equation}

- In linear regression $h$ takes the form of a linear combination of the components of $x$ that is
\begin{equation}
  h(\pmb x) = \sum_{i=0}^dw_ix_i = \pmb w^T\pmb x
\end{equation}
	where $x_0 = 1$ an $\pmb x \in \{1\} \times \mathbb R ^d$ as usual and $\pmb w \in \mathbb R^{d+1}$ 

- For the special case of linear $h$, it is very useful to have a matrix representation of $E_\text{in}(h)$
	- First we define the data matrix $X \in \mathbb R^{N \times (d+1)}$ to be the $N \times (d+1)$ matrix whose rows are the inputs $x_n$ as row vector
	- Define the target vector $\pmb y \in \mathbb R^N$ to be the column vector whose components are the target values $y_n$

[[file:The Linear Model/screenshot_2018-08-28_14-54-58.png]]
** Logistic Regression
*** Predicting a Probability
- To predict a probability we want something which restricts the output to the probability range $[0,1]$, one choice that accomplishes this goal is the logistic regression model
\begin{equation}
    h(\pmb x) = \theta(\pmb w^T \pmb x)
\end{equation}
- Where $\theta$ is the /logistic/ function $\theta(s) = \frac{e^s}{1+e^s}$ whose output is between 0 and 1
	- The output can be interpret as a probability for a binary event
	- The logistic function $\theta$ is referred to as a *soft threshold* in contrast to the *hard threshold* in classification
	- It is also called a *sigmoid*

- When using Logistic Regression we are formally trying to learn the target function
\begin{equation}
  f(\pmb x) = \mathbb P [y=+1 \mid \pmb x]
\end{equation}

- The data given is generated by a noisy target $P(y \mid \pmb x)$ 
\begin{equation}
  \begin{equation*}
    P(y \mid \pmb x) = 
  		\begin{cases}
  			\mbox{$f(\pmb x)$} & \mbox{for $y=+1$} \\
  			\mbox{$1-f(\pmb x)$} & \mbox{for $y=-1$} 
  		\end{cases}
  \end{equation*}    
\end{equation}

- The standard *error measure* $e(h(\pmb x),y)$ used in logistic regression is based how likely it is that we would get this output $y$ from the input $\pmb x$. if the target distribution $P(y | \pmb x)$ was indeed captured by our hypothesis $h(x)$. 

- The *error measure* is $E_\text{in}(\pmb w) = \frac1N \sum_{n=1}^N\ln(1+e^{-y_n\pmb w^T x_n})$

*** Gradient Descent 
**** Batch Gradient Descent
- Gradient descent is a general technique for minimize a twice differentiable function
	- e.g. $E_\text{in}(\pmb w)$ in logistic regression
	- You start somewhere and go the steepest way down the surface
	- You may end up in a local minima
	- When using a convex function such as $E_{in}$ there is only one minima the global unique minimum

- When steeping in a direction you need that the step $\eta$ is not too small or too large
[[file:The Linear Model/screenshot_2018-09-03_17-19-24.png]]
- You typically want to choose $\eta_t = \eta || \bigtriangledown E_\text{IN} ||$ to obtain a good variable step size   

[[file:The Linear Model/screenshot_2018-09-03_17-24-49.png]]
- A typical good choice for $\eta$ is a fixed learning rate is around $0.1$ the 

[[file:The Linear Model/screenshot_2018-09-03_17-28-37.png]]

- *Initialization*
	- Most of the time the initializing the initial weights as zeros works well
	- It is in general safer to initialize the weights randomly
	- Choosing each weight independently from a Normal distribution with zero mean and small variance usually works well

- *Termination*
	- A simple approach would be to set and upper limit on the number of iterations
		- Does not guarantee anything on the quality of the final weights
	- A natural terminal criterion would be to stop ones $||\pmb g_t||$ drops below a certain threshold
		- Eventually this must happen but we do not know but we will now know when
	- For logistic regression a combination of the two termination conditions is used

**** Stochastic Gradient Descent (SGD)
- A sequential version of Batch Gradient Decent
	- Often beats the batch version in practise

- Instead of considering the full batch gradient on all $N$ training points, we consider a stochastic version of the gradient
	1. Pick a training data point $(\pmb x_n, y_n)$ at uniformly random
	2. Consider only the error on that point (in case of logistic regression)
\begin{equation}
   e_n(w) = \ln ( 1+ e^{-y_n \pmb w^T \pmb x_n})
\end{equation}

- The gradient needed is
\begin{equation}
  \nabla e_n (\pmb w) = \frac{-y_n\pmb x_n}{1+e^{-y_n \pmb w^T \pmb x_n}}
\end{equation}
- The weight update is $\pmb w \leftarrow \pmb w \cdot \eta \nabla e_n(\pmb w)$

** Nonlinear Transformation
*** The $\mathcal Z$ space
#+NAME: transformExample
#+CAPTION: Example of nonlinear transform
[[file:The Linear Model/screenshot_2018-09-03_18-22-01.png]]
- Using a nonlinear transformation we can convert data which is not linear separable into data that is
	- The space $\mathcal Z$ generated is called the *feature space*
	- The transformation from the original space $\mathcal X$ to $\mathcal Z$ is called a *feature transform*

- Any linear hypothesis $\tilde h$ in $\pmb z$ corresponds to a (possible nonlinear) hypothesis of
 $\pmb x$ given by $h(\pmb x) = \tidle h (\theta(\pmb x))$ where $\theta$ is a non linear transform
	- The set of these hypothesis is denoted by $\mathcal H_\theta$

#+NAME: /tmp/screenshot.png @ 2018-09-03 18:28:08
#+CAPTION: The nonlinear transform for separating non separable data.
[[file:The Linear Model/screenshot_2018-09-03_18-28-08.png]]

- The feature transform $\theta_Q$ is defined for degree-$Q$ curves in $\mathcal X$
	- It is called the Qth order polynomial transform
p- The power of the feature transform should be used with care, it may not be worth it to insist on linear separability and employ a highly complex surface
	- It is sometime better to tolerate a small $E_in$ than using a feature transform

*** Computation and generalization
- Computation is an issue because $\theta_Q$ maps a two dimensional vector $\pmb x$ to $\tilde d = \frac{Q(Q+3)}2$ dimensions, which increases the memory and computational cost
	- Things could get worse if $\pmb x$ is in a higher dimension to begin with

- The problem of generalization is another important issue
	- We will have a weaker guarantee that $E_\text{OUT}$ is small
	- It is sometime balanced  by the advantage we get in approximating the target better

[[file:The Linear Model (3)/screenshot_2018-09-04_07-51-06.png]]
* Multinomial/Softmax Regression
** Setup
- Multinomial/Softmax Regression generalizes logistic regression to handle $K$ classes instead of $2$
	- A target value $y$ is represented as a vector of length $K$ with all zeroes except one which is called a one-in-$K$ encoding
	- To store all the data points a matrix $Y$ of size $n \times K$ and the data matrix $X$ is unchanged
\begin{equation}
X=\begin{pmatrix} 
1&- & x_1^T & - \\
\vdots & \vdots & \vdots \\
1&- & x_n^T & - \\
\end{pmatrix}\in \mathbb{R}^{n \times d}\quad\quad 
y=\begin{pmatrix}
- & y_1^T & -\\
- & \vdots &- \\
- & y_n^T & -\end{pmatrix}\in\{0,1\}^{n\times K}
\end{equation}

- To generalize to $K$ classes we will use $K$ weight vectors $w_1,\dots,w_k$ each of length $d$, one for each class.
	- To classify data we can use the following algorith: Given data x, compute $w_i^\intercal x$ for $i=1,\dots, K$ and return the index of the largest value.
	- The list of weight vectors is packed into a matrix $W$ of size $d \times K$ by putting $w_1$ in column one and so on.
$$
W=\begin{pmatrix} 
(w_1)_1  & \dots & (w_K)_1 \\
\vdots & \vdots & \vdots \\
(w_1)_d  & \dots & (w_K)_d \\
\end{pmatrix}\in \mathbb{R}^{d \times n}
$$
- This way we can compute the weighed sum for each class by the vector matrix product $x^\intercal W$ and then pick argmax of that to do the classification. Pretty Neat!.

- Numpy example
#+BEGIN_SRC python
import numpy as np
# example with 3 classes and d = 10
W = np.random.rand(10, 3)
print('Shape w:', W.shape)
x = np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.0]).reshape(10, 1)
print('Shape x:', x.shape)
model_predictions = x.T @ W
print('model (unnormalzed log) predictions: - picke the larger one\n', model_predictions)
#+END_SRC

** Probabilistic Outputs
- Given a set of model parameters $W$ and a data point $x$ we want $P(y=i\mid x, W)$ for $i=1,\dots K$.

- *Softmax* is used in our probabilistic model
	- It takes as input a vector of length $K$ and outputs another vector of the same length $K$, that is a mapping from the $K$ input numbers into $K$ *probabilities*
\begin{equation}
\textrm{softmax}(x)_j =
\frac{e^{x_j}}
{\sum_{i=1}^K e^{x_i}}\quad
\textrm{ for }\quad j = 1, \dots, K.
\end{equation}
- where $\textrm{softmax}(x)_j$ denote the $j$'th entry in the vector.
	- The denominator acts as a normalization term that ensures that the probabilities sum to one 
	- The exponentiation ensures all numbers are positive.
	- We get the following derivatives:
\begin{equation}
\frac
{\partial \;\textrm{softmax}(x)_i}
{\partial x_j} =
(\delta_{i,j} - \textrm{softmax}(x)_j)
\textrm{softmax}(x)_i\quad\quad\text{where}\quad\quad
\delta_{ij}=\begin{cases}1 &\text{if }i=j\\
0 & \text{else}
\end{cases}
\end{equation}

- The following is our probabilistic model
\begin{equation}
p(y \mid x, W) =
\textrm{softmax}(W^\intercal x) =
 \left \{
\begin{array}{l l}
 \textrm{softmax}(W^\intercal x)_1 & \text{ if } y = e_1,  \\
 \vdots & \\
 \textrm{softmax}(W^\intercal x)_K & \text { if } y = e_K.
\end{array}
\right.
\end{equation}

- We compute the likelihood of the data given a fixed matrix of parameters.
	- The notation $[z]$ for the indicator function
$$
P(D \mid W) =
\prod_{(x,y)\in D}
\prod_{j=1}^K
\textrm{softmax}(W^\intercal x)_j^{[y_j=1]}
=
\prod_{(x,y)\in D}
y^\intercal
\textrm{softmax}(W^\intercal x)
.
$$

- This way of expressing is the same as we did for logistic regression.

** The Negative Log Likelihood
- The negative log likelihood of the data is minimized instead of maximizing the likelihood of the data and get a pointwise sum.
\begin{align}\textrm{NLL}(D\mid W) &=
-\sum_{(x,y)\in D}
\sum_{j=1}^K
[y_j=1]
\ln (\textrm{softmax}(W^\intercal x)_j)
\\
&=-\sum_{(x,y)\in D}
y^\intercal
\ln (\textrm{softmax}(W^\intercal x))
\end{align}


- In the last summation only one value will be nonzero:
\begin{equation}
  - \ln \textrm{softmax}(z)_j = \ln \left( \frac{e^{z_j}}{\sum_{i=1}^d e^{z_i}}\right) = - (z_j - \ln \sum_{i=1}^d e^{z_i})
\end{equation}

- The insample error is defined to be  $E_\textrm{in} = \frac{1}{|D|} \textrm{NLL}$
	- Cannot be solved for a 0 analytically
	- To apply stochastic mini-batch gradient descent as for Logistic Regression all you really need is the gradient of the negative log likelihood function.
		- The gradient is a *simple* generalization of the one used in logistic regression.
		- There is a set of parameters for each of $K$ classes, $W_j$ for $j=1,\ldots,K$
		- The gradient is 
$$
\nabla \textrm{NLL}(W) =
-X^\intercal
(Y - \textrm{softmax}(XW)),
$$

** Implementation Issues
*** Numerical Issues with Softmax
- There are some numerical issues with the softmax function

$$
\textrm{softmax}(x)_j = \frac{e^{x_j}}{\sum_{i=1}^K e^{x_i}} \textrm{ for } j=1,\ldots,K.
$$
- This is because this is a sum of exponentials and exponentiation of numbers tend to make them very large giving numerical problems.

- The problematic part is the logarithm of the sum of exponentials.
- We can move $e^c$ for any constant $c$ outside the sum easily, that is:
$$
\ln\left(\sum_i e^{x_i}\right) =
\ln\left(e^c \sum_i e^{x_i-c}\right) =
c + \ln\left(\sum_i e^{x_i -c}\right).
$$

- We need to find a good $c$, and we choose $c = \max_i x_i$
- Since $e^{x_i}$ is the dominant term in the sum. We are less concerned with values being inadvertently rounded to zero since that does not

*** One in k encoding
- Representing a number $k$ in $[1,\dots,k]$ as a vector of length may $K$ be quite cumbersome.
	- In general the input labels will just be a list/vector of numbers between 1 and k.
	- It is your job to transform it into a matrix if needed.
	- But this will be a very sparse matrix.
	- It may be worthwhile to consider whether it is possible to implement the operations with the matrix Y without actually creating the matrix. 

*** Always check your shapes
- If the shapes dont fit then
	- If trying to implement softmax it is very useful to ensure you have full control over the shapes of all matrices and vectors you use.
	- If there is a shape mismatch then clearly there is a larger issue.
	- Checking shapes is a very efficient heuristic for catching bugs.

*** Bias Variable
- If you need a Bias variable $b$ (remember $w^\intercal x + b$) for each class you need to add a columns of ones to $X$ and make $W$ a $d+1 \times K$ matrix.

* Overfitting (4)
** When Does Overfitting Occur?
*** General
#+NAME: overfittingExample
#+CAPTION: Example of overfitting
[[file:Overfitting (4)/screenshot_2018-09-17_10-02-37.png]]
- The main case of overfitting is when you pick the hypothesis with lower $E_{in}$ and it results in higher $E_{out}$
	- Means that $E_{in}$ alone is no longer a good guide for learning
	- A typical overfitting scenario is when a complex model uses its addition degress of freedom to "learn" the noise

*** Catalysts for Overfitting
[[file:Overfitting (4)/screenshot_2018-09-17_10-16-04.png]]
- On a finite data set the algorithm inadvertently uses some of the degrees of freedom to fit the noise
	- Can result in overfitting and a spurious final hypothesis

- There are two types of noise which that algorithm cannot differentiate
	- *Deterministic noise* will not change if the dataset was generated again
		- Is different depending on which model we use
		- Related to the bias
	- *Stochastic noise* will change if the dataset was generated again
		- Related to the variance

** Regularization
*** General
- *Regularization* is a way to combat overfitting
	- Constraints the learning algorithm to improve out-of-sample error
	- Especially when noise is present

- A view of regularization is thought the VC bound, which bounds $E_\text{out}$ using a model complexity penalty $\Omega(\mathcal H)$:
\begin{equation}
    E_\text{out}(h) \leq E_\text{in}(h) + \Omega(\mathcal H) \ \text{for all } h \in \mathcal H
\end{equation}
- We are better off fitting the data using a simple $\mathcal H$ 

- Instead on minimizing $E_\text{in}(h)$ alone one minimizes the combination of $E_\text{in}(h)$ and $\Omega(h)$
	- Avoids overfitting by constraining the learning algorithm to fit data well using a simple hypotheses

*** A Soft Order Constraint 
- A *Soft Order Constraint* can be defined as the hypotheses set 
\begin{equation}
    \mathcal{C}=\{h \mid h(\pmb x) = \pmb x ^T\pmb x, \pmb w^T \pmb w \leq C\}
\end{equation}
- Solving for $\pmb w_\text{reg}$:
	- If $\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C$ then $\pmb w_\text{reg} = \pmb w_\text{lin}$ since $\pmb w_\text{lin} \in \mathcal H(C)$ 
	- If $\pmb w_\text{lin} \notin \mathcal H(C)$ then not only is $\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C$ but $\pmb w_\text{lin}^T \pmb w_\text{lin} = C$
		- The weights $\pmb w$ must lie on the surface of thee $sphere $\pmb w^T \pmb w = C$ 

- If $\pmb w_\text{reg}$ is to be optimal then for some positive parameter $\lambda_C$
\begin{equation}
  \nabla E_\text{in}(\pmb w_\text{reg}) = - 2 \lambda_C\pmb w_\text{reg}
\end{equation}
- $\nabla E_\text{in}$ must be parallel to $\pmb w_\text{reg}$ 

For some $\lambda_C > 0$ $\pmb w_\text{reg}$ locally minimizes
\begin{equation}
    E_\text{in}(\pmb w) + \lambda_CW^TW
\end{equation}

*** Weight Decay and Augmented Error 
[[file:Overfitting (4)/screenshot_2018-09-18_08-20-44.png]]
- The *augmented error* is defined as
\begin{equation}
  E_\text{aug}(\pmb w) = E_\text{in}(\pmb w) + \lambda \pmb w^T \pmb w
\end{equation}
- where $\lambda \geq 0$ is now a free parameter

- The penalty term enforces a trade-off between making the in-sample error small and making the weights small
	- Is also known as the *weight decay* 
	- Minimizing the error together with the decay is known as *ridge regression*

- If we can find the optimal $\lambda^*$ we can minimize the out-of-sample error

- In general the *augmented error* for a hypothesis set $h \in \mathcal H$ is 
\begin{equation}
    E_\text{aug}(h,\lambda,\Omega) = E_\text{in}(h) + \frac\lambda N \Omega(h)
\end{equation}
- For weight decay $\Omega(h) = \pmb w^T\pmb w$
	- The need for regularization goes down as the number of data points goes up

*** Choosing a Regularizer
[[file:Overfitting (4)/screenshot_2018-09-18_08-32-17.png]]

- A uniform regularizer, is a penalizes the weights equally
	- encourages all weights to be small uniformly
	- Example $\Omega_\text{unif}(\pmb w)= \sum_{q=0}^{15}w_q^2$
- A lower-order regularizer
	- Pairs more attention to the higher order weights
	- Example $\Omega_\text{low}(\pmb w)= \sum_{q=0}^{15}qw_q^2$

- The price paid for overfitting is generally more severe than underfitting
- The optimal value for the regularization parameter increases with noise
- No regularizer will be ideal for all settings
	- Not even specific settings
	- The entire burden rest on picking the right $\lambda$
- Some for of regularization is necessary as learning is quite sensitive to stochastic and deterministic noise 

** Validation
*** The Validation Set
- *Validation* tries to estimate the out-of-sample error directly

- The idea of a *validation set* is almost identical to that of a test set
	- A subset of the data is removed and not used in training
	- Will be used to make certain choice in the learning process
		- Therefore not a test set

- The *validation set* is created and used in the following way
	1. Partition the data set $\mathcal D$ using into a training set $\mathcal D_\text{train}$ of size $(N-K)$ and a validation set $\mathcal D_\text{val}$ of size $K$
		 - Any partitioning method which does not depend on the data will do 
	2. Run the learning algorithm using the training set $\mathcal D_\text{train}$ to obtain a final hypothesis $g^- \in \mathcal H$
	3. The validation error is then computed for $g$ using the validation set $\mathcal D_\text{val}$
\begin{equation}
  E_\text{val}(g^-)=\frac1K\sum_{\pmb x_n \in \mathcal D_\text{val}} e(g^-(\pmb x_n),y_n)
\end{equation}
- where $e(g(\pmb x),y)$ is the pointwise error measure

- The validation error is an /unbiased/ estimate of $E_\text{out}$ because the final hypothesis $g^-$ was created independently of the validation set
	- The expected error of $E_\text{val}$ is $E_\text{out}$
	- If $K$ is neither too small nor too large $E_\text{val}$ is a good estimate of $E_\text{out}$
	- A rule of thumb in practise is to set $K=\frac N5$ 

- We should not output $g^-$ we should output $g$ which is trained on the entire hypothesis set $D$
	- To estimate $E_\text{out}$ we use that $E_\text{out}(g) \leq E_\text{out}(g^-)$ because of the learning curve
		- Is not rigorously proved
		- It is just very likely 

*** Model Selection 
[[file:Overfitting (4)/screenshot_2018-09-18_09-48-43.png]]
- The most important use of validation is for *model selection*
	- Choosing linear or nonlinear, polynomial or not...
	- It could any choice that affects the learning process

- It can be used to estimate the out-of-sample error for more than one model, suppose we have $M$ models $\mathcal H_1, \dots, \mathcal H_M$ 
	- Validation can be used to select one of these models
	- Use the training set $\mathcal D_\text{train}$ to learn the final hypothesis $g^-_m$ for each model
	- Evaluate each model on the validation set to obtain the validation errors $E_1,\dots,E_M$ where
\begin{equation}
  E_m = E_\text{val}(g^-_m); \text{ for } m = 1,\dots,M
\end{equation}
- Then just select the model with the lowest validation error.
- For suitable $K$ even $g^-_{m*}$ is better than in-sample selection of the model
- The validation error can also be used to select a lambda by using $(\mathcal H, \lambda_1),(\mathcal H, \lambda_2),\dots,(\mathcal H \lambda_M)$ as our $M$ different models
- The more one uses the validation set to fine tune the model the more the it becomes like the training set

*** Cross Validation
[[file:Overfitting (4)/screenshot_2018-09-18_20-20-44.png]]

- There are $N$ ways to partition a set of size $N-1$ and a validation set of size $1$. Let
\begin{equation}
  \mathcal D_n = (\pmb x_1, y_1), \dots, (\pmb x_{n-1}, y_{n-1}), (\pmb x_{n+1}, y_{n+1}), \dots, (\pmb x_N, y_N)
\end{equation}
- The final hypothesis learned from $\mathcal D_n$ is denoted $g_n^-$
- Let $e_n$ be the error made by $g_n^-$ on its validation set which is just a single point $\{(\pmb x_n, y_n)\}$
- The cross validation estimate is the average value of the $e_n\text{'s}$
\begin{equation}
  E_\text{cv}=\frac1N \sum_{n=1}^Ne_n
\end{equation}

- *Theorem 4.4.* $E_\text{cv}$ is an unbiased estimate of $\bar E_\text{out}(N-1)$
	- The expectation of the model performance, $\mathbb E[E_\text{out}])$, over data set of size $N-1$

- The cross validation estimate will on average be an upper estimate for the out-of-sample error: $E_\text{out}(g) \leq E_\text{cv}$

- Cross validation can be for model selection for a given set of models $\mathcal H_1, \dots, \mathcal H_M$ in the same way as validation set

[[file:Overfitting (4)/screenshot_2018-09-18_20-42-49.png]]

- To get cross validation for $M$ models and a data set $D$ of size $N$ is requires $MN$ rounds of learning
- If one could analytically obtain $E_\text{cv}$ it would be a big bonus
	- Analytical results are hard to come
	- An analytical method exists for linear models 

- The cross validation estimate can be analytically computed as 
\begin{equation}
  E_\text{cv} = \frac1N\sum_{n=1}^N(\frac{\hat y_n-y_n}{1-H_nn(\lambda)})^2
\end{equation}
- where $H(\lambda)=Z(Z^TZ+ \lambda I)^{-1}Z^T$ 

* Support Vector Machines 
** Notation
- The classifier considered will be a linear classifier for a binary classification problem with labels $y$ and features $x$
	- $x \in \{-1,1\}$
	- The classifier with parameters $w$ and $b$  is written as
\begin{equation}
  h_{w,b}(x)=g(w^Tx+b)
\end{equation}
- where g = 1 if $z \geq 0$ and $g(z)= -1$ otherwise

** Functional and geometric margins
- Given a training example $(x^{(i)},y^{(i)})$, the *functional margin* of $(w,b)$ is defined with respect to the training example
\begin{equation}
  \hat \gamma^{(i)}=y^{(i)}(w^Tx+b)
\end{equation}
- A large functional margin represents a confident and a correct prediction
- Given a training set $S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}$ the functional margin of $(w,b)$ with respect to $S$ is
\begin{equation}
  \hat \gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \hat \gamma^{(i)}
\end{equation}

- The *geometric margin* of $(w,b)$ with respect to a training example $(x^{(i)},y^{(i)})$ to be 
\begin{equation}
	\gamma^{(i)} = y{(i)} \Bigg( \bigg( \frac w{||w||} \bigg)^T x^{(i)} + \frac b{||w||}  \Bigg)
\end{equation}
- If $||w|| = 1$ then the functional margin equals the geometric margin 
- Given a training set $S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}$ the *geometric margin* of $(w,b)$ with respect to $S$ is
\begin{equation}
	\gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \gamma^{(i)}
\end{equation}

** The optimal margin classifier
- The problem of finding a decision boundary which has the largest geometric margins is the following optimisation problem
\begin{equation}
  \begin{split} 
    \max_{\gamma,w,b} \ &\gamma\\
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq \gamma, \ i= 1, \dots, m \\
    &||w|| = 1
  \end{split}
\end{equation}
- this can be be turned into the following problem using functional margins and rescaling it 
\begin{equation}
  \begin{split} 
   \min_{\gamma,w,b} \ &\frac12 ||w||^2\\ 
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq 1, \ i= 1, \dots, m \\
  \end{split}
\end{equation}
- It is called the *optimal margin classifier*

** Lagrange duality 
- Consider a problem of the following form:
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}
- The *Lagrangian* is defined to be 
\begin{equation}
  \mathcal L(w,\beta) = f(w) + \sum_{i=1}^l\beta_ih_i(w)
\end{equation}
- The $\beta_i$'s are called the *Lagrange multipliers*
	- We would the find and set $\mathcal L$'s partial derivatives to zero
\begin{equation}
  \frac{\partial \mathcal L}{\partial w_i} = 0; \frac{\partial \mathcal L}{\partial \beta_i} = 0
\end{equation}
and solve for $w$ and $\beta$ 

- The *primal* optimization problem is the following 
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ g_i(w) \leq 0, \ i=1,\dots, k
								&\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}

- The primal optimization problem is solved by defining the *generalized Lagrangian* 
\begin{equation}
  \mathcal L (w,\alpha,\beta) = f(w) + \sum_{i=1}^k \alpha_ig_i(w) + \sum_{i=1}^k \beta_ih_i(w)
\end{equation}
- The $\alpha_i$'s and $\beta_i$'s are the Lagrange multipliers.
- Consider the quantity
\begin{equation}
    \theta_\mathcal{P}(w) = \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
- If $w$ violates any of the primal constraints then one should be able to verify that 
\begin{equation}
    \theta_\mathcal P (w) = \infty
\end{equation}
- If $w$ does not violate the constraints then $\theta_\mathcal P(w) = f(w)$ and therefore the minimization problem
\begin{equation}
  \min_w\theta_\mathcal P(w) = \min_w \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
- is the same as the original problem
	- the objective is called the *value* of the primal problem and is denoted $p* = \min_w \theta _ \mathcal P (w)$

- If one define 
\begin{equation}
    \theta_D(\alpha,\beta) = \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
- The $\mathcal D$ subscript stands for dual
	- This can be used to pose the *dual* optimization problem
\begin{equation}
  \max_{\alpha,\beta: \alpha_i  \geq 0} \theta_\mathcal D (\alpha, \beta) = \max_{\alpha,\beta: \alpha_i  \geq 0} \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
- Which is exactly the same as the primal problem, except the order of the min and max has been exchanged
	- The solution to the dual problem is defined as $d^*$
	- It holds that $d^* \leq p^*$

- The KKT conditions on $w^*$, $\alpha^*$ and $\beta^*$ 
[[file:Support Vector Machines/screenshot_2018-09-24_17-52-15.png]]

- If some $w^*$, $\alpha^*$, $\beta^*$ satisfy the KKT conditions they are also a solution to the primal and dual problems

** Optimal margin classifiers 
- By using the KKT conditions obtain the following optimization problem, which gives us a decision boundary with the largest margins:
[[file:Support Vector Machines/screenshot_2018-09-25_07-46-53.png]]

- Where $w= \sum_{i=1}^m a_iy^{(i)}x^{(i)}$ 
	- The prediction $w^Tx+b$ can also be written as
[[file:Support Vector Machines/screenshot_2018-09-25_07-50-38.png]]
	- Where post of the inner products will be zero except for the support vectors

** Kernels
- Given a feature mapping $\phi$ we define the corresponding *Kernel* to be 
\begin{equation}
  K(x,z) = \phi(x)^T\phi(z)
\end{equation}
- Everywhere we previously had $\langle x, z \rangle$ we could simple replace it with $K(x,z)$ and the algorithm would now be learning using the features $\phi$  

- The *Gaussian kernel* which corresponds to an infinite dimensional feature mapping $\phi$ 
\begin{equation}
  K(x,z) = \exp(-\frac{||x-z||^2}{2\sigma^2})
\end{equation} 

- The matrix called the *Kernel matrix* is defined from some $m$ data points $\{x^{(1)}, \dots, x^{(m)}\}$ as the m-by-m matrix $K$ where the $(i,j)$ entry is given by $K_{ij}=K(x^{(i)},y^{(i)})$

- *Theorem (Mercer).* Let $K: \mathbb R^n \times \mathbb R^n \mapsto \mathbb R$ be given. Then for $K$ to be a valid (Mercer) kernel, it is necessary and sufficient that for any $\{x^{(1)}, \dots, x^{(m)}\}$, $m < \infty$, the corresponding kernel matrix is symmetric positive semi-definite 

** Regularization and the non-separable case
- To make the algorithm work for non-linearly separable datasets and being less sensitive to outliers, the optimization can be reformulated as follows using regularization:
[[file:Support Vector Machines/screenshot_2018-09-25_08-30-17.png]]
- Which means that examples are now permitted to have a functional margin less than 1

- By using some of the KKT conditions one can obtain the following dual form of problem 
[[file:Support Vector Machines/screenshot_2018-09-25_08-36-39.png]]

** The SMO algorithm
*** General
- The SMO algorithm gives an efficient way of solving the dual problem arising from the derivation of the SVM

*** Coordinate ascent 
#+NAME: coordinateAscent
#+CAPTION: Coordinate ascent example
[[file:Support Vector Machines/screenshot_2018-09-25_08-44-24.png]]

- If one is trying to solve the unconstrained optimization problem 
\begin{equation}
  \max_\alpha W(\alpha_1,\alpha_2, \dots,\alpha_m)
\end{equation}
- One can use the algorithm called *coordinate ascent:* 
[[file:Support Vector Machines/screenshot_2018-09-25_08-47-20.png]]
- Where one holds all the variables fixed except some $a_i$ 

*** SMO
- The SMO algorithm does the following 
[[file:Support Vector Machines/screenshot_2018-09-25_09-00-29.png]]
- To test for convergence, one can test whether the KKT conditions are satisfied within some /tol/
	- The tol is the convergence tolerance parameter normally set around $0.01$ to $0.001$ 
* Deep Feedforward Networks
** General 
- *Deep feedforward networks* are quintessential deep learning models 
	- The goal of a feedforward network is to approximate some function $f^*$
	- It defines a mapping $\pmb y = f(\pmb x; \pmb \theta)$ and learns the value of the parameters $\pmb \theta$ that result in the best function approximation

- It is called *feedforward* since information flows through the function being evaluated from $\pmb x$, through intermediate computations used to define $f$ and finally to the output $\pmb y$
	- When feedforward neural networks are extended to include feedback connections, they are called *recurrent neural networks*

- They are called *networks* because they typically are represented by composing together many different function
	- It is associated with a DAG describing ow the functions are composed together
	- The different functions are called *layers*
	- The overall length of the function chain gives the *depth* of the model
	- The final layer is called the *output layer*
	- During the training we drive $f(\pmb x)$ to match $f^*(\pmb x)$
	- The training data provides us with noisy, approximate examples of $f^*(\pmb x)$ evaluated at different training points
	- The training data does not say what each individual layers should do
		- That is the training algorithms job
		- They are called *hidden layers*
	- The dimensionality of these hidden layers determines the *width* of the model

- The strategy of deep learning is the feature transform $\phi$
	- We have a model $=f(\pmb x; \pmb \theta, \pmb w) = \phi(\pmb x, \pmb \theta)^\top \pmb w$
	- We have the parameters $\pmb \theta$ that we use to learn $\phi$ from a broad class of functions
	- We have the parameters $\pmb w$ that map from $\phi(\pmb x)$ to the desired output

- Training a feedforward network requires making many of the same design decisions as are necessary for a linear model:
	- Choosing the optimizer
	- The cost function
	- The form of the output units. 

** Gradient-Based Learning 
*** General
- The non-linearity of a neural network causes most interesting loss functions to become non-convex
	- It means that NNs are usually are trained by using iterative, gradient-based optimizes that merely drive the cost function to a very low value
	- It is important to initialize all weights to random values becomes of the error function being non-convex, when using stochastic gradient descent 
		- The biases may be initialized to zero or a small positive values
	
*** Cost Functions 
**** General
- Cost functions for neural network are more or less the same as those for other models, such as linear models
	- The total cost function used to train a neural network will often combine one of the primary cost functions with a regularization term

**** Learning Conditional Distributions with Maximum Likelihood
- Most NNs are trained using maximum likelihood
	- The cost function is simply the NLL which is equivalently described as the cross-entropy between the training data and the model distribution 
	- This cost function is given by:
[[file:Deep Feedforward Networks/screenshot_2018-09-30_16-58-07.png]]
- The advantage of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model
	- Specifying a model $p(\pmb y \mid \pmb x)$ automatically determines a cost function $\log p(\pmb y \mid \pmb x)$

- Instead of learning a full probability distribution $p(\pmb y \mid \pmb x; \pmb \theta)$, we often want to learn just one conditional statistic of $\pmb y$ given $\pmb x$
	- Such as predicting the mean of $\pmb y$ given the predictor $f(\pmb x; \pmb \theta)$ 

**** Learning Conditional Statistics
- Instead of learning a full probability distribution $p(\pmb y \mid \pmb x; \pmb \theta)$ one often want to learn just one condition statistic of $\pmb y$ given $\pmb x$
	- Such as predicting the mean of $\pmb y$ 
	- The cost function can be viewed as being a *functional* rather than just a function
		- A mapping from functions to real numbers
	- The cost functional can be designed to have its minimum occur at some specific function we desire

- Solving the optimization problem
\begin{equation}
  f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||^2
\end{equation}
yields
\begin{equation}
  f^*(\pmb x) = \mathbb E_{\pmb y \sim p_{data}(\pmb y \mid x)[\pmb y]}
\end{equation}

- The following function yields a function that predicts the /median/ value of $\pmb y$ for each $\pmb x$ 
\begin{equation}
	f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||_1
\end{equation}
- This cost function is commonly call *mean absolute error*

*** Output Units
**** General 
- The choice of cost function is tightly coupled with the choice of output unit
	- Most of the time one simply uses the cross-entropy between the data distribution and the model distribution
	- The choice of how to represent the output determines the cross-entropy function
	- Any kind of neural network that may be used as output can also be used as a hidden unit

- The hidden features is defined by $\pmb h = f(\pmb x ; \pmb \theta)$
	- The role of the output layer is to provide some additional transformation from the features to complete the task that the network must perform

**** Linear Units for Gaussian Output Distributions
- Linear units is an output unit based on an affine transformation with no non-linearity
	- Given features $\pmb h$, a layer of linear output units produces a vector $\hat{\pmb y}= \pmb W^T \pmb h + b$
	- Linear output layers are often used to produce the mean of a conditional Gaussian distribution
\begin{equation}
  p(\pmb y \mid \pmb x) = \mathcal N(\pmb y; \hat{\pmb y}, \pmb I)
\end{equation}
- Maximizing the log-likelihood is the equivalent to minimizing the mean squared error 

** Hidden Units
*** General
- Some valid hidden units are not differentiable at all input points
	- Such as $g(z)=\max\{0,z\}$
	- Solved by using the right or left differential 

*** Rectiﬁed Linear Units and Their Generalizations
- Rectified linear units use the activation function $g(z) = \max\{0,z\}$
	- Easy to optimize because they are similar to linear units
		- Only difference is that a rectified unit outputs zero across half its domain

- Rectified linear units are typically used on top of an affine transformation
\begin{equation}
  \pmb h = g(\pmb W^\top \pmb x + \pmb b)
\end{equation}
- When initializing the parameters of the affine transformation it can be a good pratise to set all elements of $\pmb b$ to a small positive value
- A drawback to rectified linear units is that they cannot learn via gradient based methods on examples for which their activation is zero
	- Some generalizations of rectified linear units guarantee that they receive gradient everywhere

- *Maxout units* generalize rectified linear units further
	- Instead of applying an element-wise function $g(z)$ maxout units divide $\pmb z$ into groups of $k$ values
	- Each maxout unit the outputs the maximum element of one of these groups
\begin{equation}
  g(\pmb z)_i = \max_{j\in \mathbb G^{(u)}} z_j
\end{equation}
- where $\mathbb G^{(I)}$ is the set of indices into the inputs for group $i$ 

- A maxout unit can be seen as learning the activation function itself rather than just the relationship between units

*** Logistic Sigmoid and Hyperbolic Tangent
- The widespread saturation of sigmoid units can make gradient-based learning very difficult
	- Their use as hidden units in feedforward networks are discouraged
	- Training using the $tanh$ function for hidden layers are easier

** Architecture Design
*** General
- *Architecture* refers to the overall structure of the network
	- How many units it should have
	- How these units should be connected to each other

- Most NNs are organized into groups of units called layers
	- They are often arranged in a chain structure where each layer is a function of the layer that preceded it
	- The ith layer is given by 
\begin{equation}
  h^{(i)} = g^{(i)}(\pmb W ^{(i)T}\pmb h^{(i)} + \pmb b^{(i)})
\end{equation}
- where the first layer uses $x$ instead of $h^{(i)}$ 

- In chain based architectures, the main architectural considerations are to choose the the depth of the network and the width of each layer 
	- The ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error.

*** Universal Approximation Properties and Depth
- The universal approximation theorem means that regardless of what function we are trying to learn, we know that a large multi layered perceptron will be able to represent this function.
	- However, we are not guaranteed that the training algorithm will be able to learn that function
	- Learning it can fail for two different reasons
		1. The optimization algorithm used for training may not be able to find the value of the parameters corresponds to the desired function
		2. The training algorithm might choose the wrong function due to overfitting

- Feedforward networks provide a universal system for representing functions, in the sense that, given a function, there exists a feedforward network that approximates the function.
	- There is no universal procedure for examining a training set of specific examplesd and choosing a function that will generalize to point not in the training set

- The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be. 

- A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.
	- In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error 

- The number of linear regions carved out by a deep rectifier network with $d$ inputs, depth $l$, and $n$ units per hidden layer is 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-14-50.png]]

- Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions.

*** Other Architectural Considerations
- The layers need not be connected in a chain, but it is the most common practice.
	- Many architectures build a main chain but then add extra architectural features to it
		- Such as skip connections going from layer i to layer i + 2 or higher.
	- These skip connections make it easier for the gradient to ﬂow from output layers to layers nearer the input.

- A key consideration of architecture design is how to connect a pair of layers to each other
	- The default way is having every input unit connected to every output unit
	- Strategies for reducing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network,
		- They are often highly problem-dependent.

** Back-Propagation and Other Differentiation Algorithms
*** General
- *Forward propagation* is when the inputs $\pmb x$ provide initial information that then propagate up to the hidden units at each layer and finally produces an output $\hat{\pmb y}$
	- During training it can continue onward until it produces a scalar cost $J(\pmb \theta)$ 

- The *back-propagation* algorithm (*backprop*) allows the information from the cost to flow backwards through the network in order to compute the gradient
	- It is used to compute the gradient, another algorithm is used to do the learning e.g. stochastic gradient descent

*** Computational Graphs
#+NAME: computationalGraphs
#+CAPTION: Computational Graph Examples
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-49-44.png]]

- An *operation* is a simple function of one or more variables
	- Defined to return only a single output variable

- The graph language is accompanied by a set of allowable operations
	- If a variables $y$ is computed by applying an operation to a variable $x$, then there is drawn a directed edge from $x$ to $y$
	- The output node is sometimes annotated with the name of the operation applied

*** Chain Rule of Calculus
- Back-propagation computes the chain rule, with a specific order of operations that is highly efficient 
- The chain rule can generalize beyond the scalar case suppose that $\pmb x \in \mathbb R^m, \pmb y \in \mathbb R^n$, $g$ maps from $\mathbb R^m$ to $\mathbb R^n$, and $f$ maps from $\mathbb R^n$ to $R$. If $\pmb y = g(\pmb x)$ and $z = f(\pmb y)$, then 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-55-43.png]]
- In vector notation, this may equivalently be written as 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-56-15.png]]
- where $\frac{\partial \pmb y}{\partial \pmb x}$  is the $n \times m$ Jacobian matrix of $g$ 

- The back-propagation algorithm consists of performing Jacobian-gradient product given by the chain rule for each operation in the graph
- The back-propagation algorithm is typically applied to tensors of arbitrary dimensionality
	- Is exactly the same as back-propagation with vector conceptually
	- Denoting a gradient of a value $z$ with respect to a tensor $s.
- To denote the gradient of a value z with respect to a tensor $\pmb X$, we write $\nabla_{\pmb X} z$
	- The indices into $\pmb X$ have multiple coordinates

- The chain rule for tensors:
[[file:Deep Feedforward Networks/screenshot_2018-10-01_18-22-42.png]]

*** Recursively Applying the Chain Rule to Obtain Backprop
- Given a scalar $u^{(n)}$ which is the quantity whose gradient we want to obtain with respect to all the $n_i$ input nodes $u^{(1)}$ to $u^{(n_i)}$
	- We wish to compute $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ for $i \in \{1,2,\dots,n_i\}$
	- In backprop $u^{(n)}$ will be the cost associated with an example or a minibatch
	- $u^{(1)}$ to $u^{(n_i)}$ correspond to the parameters of the model
	- The nodes of the graph is assumed to be order in such a way that we can compute their output one after the other
	- Each node $u^{(i)}$ is associated with an operation $f^{(i)}$ and is computed by evaluating the function
\begin{equation}
  u^{(i)} = f(\mathbb A ^{(i)})
\end{equation}
- where $\mathbb A ^{(i)}$ is the set of all nodes that are parents of $u^{(i)}$ 
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-04-33.png]]

- The forward propagation computation is put in a graph $\mathcal G$
	- In order to perform back-propagation, one can constructs a computational graph that depends on $\mathcal G$ and add to it an extra set of nodes
		- These form a subgraph $\mathcal B$ with one node per node of $\mathcal G$
		- Computation in $\mathcal B$ proceeds in the reverse of the order of computation in $\mathcal G$
		- Each node of $\mathcal B$ computes the derivative $\frac{\partial u^{(n)}}{\partial u^{i}}$ associated with the forward graph node $u^{(i)}$ using the chain rule
	- The subgraph $\mathcal B$ contains one edge for each edge for each edge of $\mathcal G$
		- The edge from $u^{(j)}$ to $u^{(i)}$ is associated with the computation of $\frac{\partial u^{(i)}}{\partial u ^{(j)}}$
		- The dot product is performed for each node between the gradient already computed with respect to nodes $u^{(i)}$ that are children of $u^{(j)}$ and the vector containing the partial derivatives $\frac{\partial u^{(i)}}{\partial u ^{(j)}}$ for the same children nodes

- The amount of computation required for performing back-propagration scales linearly with the number of edges in $\mathcal G$  

[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-18-42.png]]

*** Back-Propagation Computation in Fully-Connected MLP
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-22-46.png]]

[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-23-03.png]]

*** Symbol-to-Symbol Derivatives
- *Symbolic Representations* is algebraic expressions and computational graphs that both operate on symbols, or variables that do not have speciﬁc values.

- Some approaches to back-propagation take a computational graph and a set of numerical values for the inputs to the graph, then return a set of numerical values describing the gradient at those input values.
	- This is called "symbol-to-number" diﬀerentiation

- Another approach for backprop is to take a computational graph and add additional nodes to the graph that provide a symbolic description of the desired derivatives

*** General Back-Propagation
- Each node in the graph $\mathcal G$ corresponds to a variable
	- This is described as being a tensor $\mathbf{\mathsf V}$
	- Tensor can in general have any number of dimensions
	- They subsume scalars, vectors, and matrices

- It is assumed that each variable $\mathbf{\mathsf V}$ is associated with the following subroutines:
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-51-44.png]] 

- Each operation ~op~ is also associated with a ~bprop~ operation
	- This ~bprop~ operation can compute a Jacobian vector product
	- Formally, ~op.bprop(inputs,X,G)~ must return:
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-58-55.png]]
- Inputs is a list of inputs that are supplied to the operation
- ~op.f~ is the mathematical function that the operation implements
- $\mathbf{\mathsf X}$ is the input whose gradient we which to compute
- $\mathbf{\mathsf G}$ is the gradient on the output of the operation

[[file:Deep Feedforward Networks/screenshot_2018-10-02_09-04-05.png]]

[[file:Deep Feedforward Networks/screenshot_2018-10-02_09-04-43.png]]

- The backprop algorithm uses dynamic programming to get a better running time

** Backpropagation equations 
- $w_{ji}^l$ is the weight from neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$
- $a_j^1 = x_j$ 
- $s_j^l=\sum_ia_i^{l-1}w_{j,i}^{l-1}+b_j^{l-1}$ 
- $a_k^l = \Phi(s_k^l)$
- $\delta_j^l=\frac{\partial e}{\partial s_j^l}$ 

[[file:Deep Feedforward Networks/Screenshot-20181009083230-1068x588_2018-10-09_08-33-23.png]]

[[file:Deep Feedforward Networks/Screenshot-20181009084054-933x797_2018-10-09_08-41-57.png]]

* Convolutional Networks
** General
- *Convolutional Networks* (CNNs) are a specialized kind of neural network for processing data that has a grid-like topology
	- *Convolution* is a specialized kind of linear operation
	- Convolutional networks are neural networks that use convolution in place of general matrix multiplication in at least one of their layers

** The Convolution Operation
- A convolution is in its most general form an operation on two functions of a real valued argument
	- Example of a convolution: $s(t) = \int x(a)w(t-a)da$
	- The convolution operation is typically denoted with an asterisk: $s(t) = (x*w)(t)$
	- The first argument to the convolution is often referred to as the *input*
	- The second argument is referred to as the *kernel*
	- The output is referred to as the *feature map*
	- In machine learning applications
		- The input is usually a multidimensional array of data
		- The kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm.
		- The multidimensional arrays are referred to as tensors
		- Because each element of the input and kernel must be explicitly stored separately, it is often assumed that these functions are zero everywhere but in the finite set of points for which we store the value
	- Convolutions are often used over more than one axis at a time
		- e.g. on a two-dimensional image $I$ as input one would probably use a two dimensional kernel $K$
	- Convolution is commutative which means that for a two dimensional kernel $K$ and a input $I$: $S(i,j) = (I*K)(i,j)=(K*I)(i,j)$
		- The last one is usually more straightforward to implement in a machine learning library, since there is less variation in the range of valid values for $m$ and $n$
	- Many neural network libraries implement are related function called *cross-correlation*, which is the same as convolution but without flipping the kernel
		- e.g. $S(i,j) = (K*I)(i,j)= \sum_m\sum_nI(i+m,j+n)K(m,n)$
		- Some also call this convolution

** Motivation 
- Convolution leverages three important ideas that can help improve a machine learning system:
	- *Sparse interactions* is accomplished by making the kernel smaller than the input
		- It is also referred to as *parse connectivity* *or *sparse weights*
		- e.g. one can detect small meaningful features in images such as edges with kernels
		- One needs to store fewer parameters which reduces the memory requirements of the model and improves its statistical efficiency
			- Computing the output requires fewer operations
			- The improvements in efficiency are usually quite large 

	- *Parameter sharing:* refers to using the same parameter for more than one function in the model
		- One can say that a network has *tied weights*, because the value of the weight applied to one input is tied to the value of a weight applied elsewhere
		- Each member of the kernel is used at every position of the input

	- *Equivariant representations*: If $g$ is any function that translates the input then that is shifts is, then the convolution function is equivalent to $g$ 
		- A function $f$ is equivalent to a function $g$ if $f(g(x))=g(f(x))$

- Some kinds of data cannot be processed by neural networks deﬁned bymatrix multiplication with a ﬁxed-shape matrix. Convolution enables processing of some of these kinds of data.

** Pooling 
[[file:Convolutional Networks/screenshot_2018-10-08_10-23-35.png]]

- A typical layer of a convolutional network consists of three stages
	1. In the first stage the layer performs several convolutions in parallel to produce a set of linear activations
	2. In the second stage each linear activation is run through a nonlinear activation, such as rectified linear activation function
		 - Is sometimes called the *detector stage*
	3. The third stage we use a *pooling function* to modify the output of the layer further
		 - It replaces the output of the net at a certain location with a summary statistic of the nearby output
			 - e.g. the *max pooling operation* which reports the maximum output within a rectangular neighborhood
		 - Pooling helps to make the representation approximately *invariant* to small translation of the input
			 - Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs
			 - Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.
		 - It is possible to use fewer pooling units than detector units by reporting summary statistics for pooling regions spaced $k$ pixels apart rather than $1$ pixels apart
			 - Improves computational efficiency of the network because the next layer has approximately $k$ times fewer inputs to process
			 - Can be used to handle images of variable size by changing how much it is space depending on the input size 

* Tree-Based Methods
** Background 
- Three based methods partition the feature space into a set of rectables and then fit a simple model in each one
	- They are simple yet powerful
	- One first split the space into two regions and models the response by the mean of $Y$ in each region, then one or both of the regions are split into two more regions, this process is continued until some stopping rule is applied

** Regression Trees
- The data consists of $p$ inputs and a response, for each of $N$ observations, that is $(x_i,y_i)$ for $i=1,2,\dots,N$, with $x_i=(x_{i1}, x_{i2}, \dots, x_{ip})$
- The algorithm needs to automatically decide on the splitting variables and split points and what topology (shape) the three should have
- If one have a partition into $M$ regions $R_1,R_2,\dots,R_M$, and model the response as a constant $c_m$ in each region:
\begin{equation}
  f(x) = \sum_{m=1}^Mc_mI(x \in R_m)
\end{equation}
- If the criterion is minimization of the sum of squares $\sum(y_i-f(x_i))^2$, he best $\hat c_m$ is just the average of $y_i$ in the region $R_m$: 
\begin{equation}
  \hat c_m = \text{ave}(y_i \mid x_i \in R_m ).
\end{equation}
- Since the binary partition in terms of minimum sum of squares is generally computationally infeasible one needs a greedy algorithm:
[[file:Tree-Based Methods/screenshot_2018-10-08_16-15-20.png]]
- For each splitting variable, the determination of the split point $s$ can be done very quickly by scanning through all the inputs
	- Having found the best split, one partition the data into the two resulting regions and repeat the splitting process on each of the two regions, which is the repeat on all the resulting regions
	- The optimal tree size should be adaptively chosen from the data
		- A preferred strategy is to grow a large $T_0$ stopping the splitting process only when some minimum node size is reached
		- The large tree is then pruned using cost-complexity pruning

- A *subtree* $T \subset T_0$ is defined to be any tree that can be obtained by *pruning* $T_0$
	- Pruning is collapsing any number of its internal (non-terminal nodes).
	- Terminal nodes is indexed by $m$, with node $m$ representing region $R_m$ 
	- $|T|$ denotes the number of terminal nodes in $T$ 
- Letting
[[file:Tree-Based Methods/screenshot_2018-10-08_16-27-26.png]]
- The idea is to find for each $\alpha$ the subtree $T_\alpha \subseteq T_0$ to minimize $C_\alpha(T)$
	- The tuning parameter $\alpha \geq 0$ governs the tradeof between tree size and its goodness of fit to the data
	- Larger values of $\alpha$ result in smaller trees $T_\alpha$ and the converse for smaller values of $\alpha$
	- With $\alpha = 0$ the solution is the full tree $T_0$
	- For each $\alpha$ there is a unique smallest subtree $T_\alpha$ that minimizes $C_\alpha(T)$ 

- If one want to find $T_\alpha$ one uses *weakest link pruning*:
	- One successively collapse the internal node that produces the smallest per-node increase in $\sum_m N_mQ_m(T)$ until we produce P and continue until we produce the single-node (root) tree.
	- This gives a finite sequence of subtrees and $T_\alpha$ must be one of these subtrees
	- Estimation of $\alpha$ is achieved by five-fold or ten-fold cross-validation:
		- The value $\hat \alpha$ is chosen to minimize the cross-validated sum of squares
		- The final tree is $T_{\hat \alpha}$ 

** Classification Trees 
[[file:Tree-Based Methods/screenshot_2018-10-08_16-49-37.png]]


- If the target is classification outcome taking values $1,2,\dots, K$ the only changes needed is the tree algorithm is the criteria for splitting nodes and pruning the tree
	- In a nodes $m$, representing a region $R_m$ with $N_m$ observations, let 
[[file:Tree-Based Methods/screenshot_2018-10-08_16-47-14.png]]
- The Gini index and cross-entropy are differentiable and are therefore more amenable to numerical optimization
	- They are often used for growing the three
	- To guide the cost-complexity pruning any of the three measures can be used but typically it is the misclassification rate 

** Other Issues
- *Categorial Predictors*
	- When splitting a predictor having $q$ possible unordered values, there are $2^{q-1}-1$ possible partitions of the $q$ values into two groups
		- The computations become prohibitive for large $q$
	- With a $0-1$  outcome the computation simplifies
		- One orders the predictor classes according to the proportion falling in outcome class $1$
		- This gives the optimal split in terms of cross-entropy or Gini index- among all possible $2^{q-1}-1$ splits

- *The Loss Matrix*: A $KxK$ loss matrix $\pmb L$, is defined with $L_{kk'}$ being the loss incurred for classifying a class $k$ observation as class $k'$
	- Typically no loss is incurred for correct classifications, that is $L_{kk}= 0 \ \forall k$
	- To incorporate P the losses into the modeling process, one could modify the Gini index to $\sum_{k\ne k'} L_{kk'} \hat p_{mk} \hat p_{mk'}$
	- This does not help in the two-class case and a better approch is to weight the observations in class $k$ by $L_{kk'}$ 

- *Missing Predictor Values:* If some of the data has some missing predictor values in some of the values
	- There are two better approaches than throwing the data away
		1. The first is applicable to variable predictors, where one simply makes a new category for /"missing"/
			 - This might make one discover that the observation with missing values for some measurement behave differently that those with nonmissing values
		2. The second is a more general approach which is the construction of surrogate variables
			 - When considering a predictor for a split, we use only the observations for which that predictor is not missing
			 - Having chosen the best (primary) predictor and split point, we form a list of surrogate predictors and split points.
				 - The first surrogate is the predictor and corresponding split point that best mimics the split of the training data achieved by the primary split.
				 - The second surrogate is the predictor and corresponding split point that does second best, and so on. 

- *Why Binary Splits?*
	- The problem with using multiway splits is that it fragment the data too quickly, leaving insufficient data at the next level down.
	- Since multiway splits can be achieved by a series of binary splits, the binary splits are preferred.

* Random forests 
** Introduction 
- *Bagging* or *bootstrap aggregation* is a technique for reducing the variance of an estimated prediction function
	- Bagging works seems to work especially well for high-variance, low-bias procedures such as trees
	- For regression we simply fit the same regression tree many time to bootstrap sampled versions of the training data and average the result
	- For classification, a *committee* of trees each cast a vote for the predicted class

- *Random forests* is a substantial modification of bagging that builds a large collection of de-correlated trees and averages them
	- On many problems its performance is very similar to boosting
	- They are simpler to train and tune than the boosting example 

** Bootstrap aggregating technique 
- Given a standard training set $D$ of size $n$
	- Bagging generates $m$ new training set $D_i$ each of size $n'$ by sampling from $D$ uniformly and with replacement
		- This is know as a *bootstrap* sample
	- The $m$ models are fitted using the bootstrap sample and combine by
		- Averaging the output (for regresion)
		- Voting (for classification)

** Definition of Random Forests 
[[file:Random forests/screenshot_2018-10-08_20-10-57.png]]

- The essential idea in bagging is to average many noisy but approximately unbiased models
	- This reduces the variance
	- Trees are ideal candidates for bagging since:
		- They can capture complex interaction structures in the data
		- If grown sufficiently deep, they have relative low bias
	- Since trees are very noisy they benefit greatly from averaging 
	- The bias of bagged trees is the same as that of the individual (bootstrap) trees
		- The only hope of improvement is through variance reduction
	
- An average of $B$ independent identically distributed random variables, each with variance $\sigma ^2$ has variance $\frac1B\sigma^2$
		- If the variables are simply identically distributed with positive correlation $\rho$ the variance of the average is 
\begin{equation}
  \rho \sigma^2 + \frac{1-\rho}B \sigma^2
\end{equation}
- As $B$ increases, the second terms disappears but the first remains
	- The size of the correlation of pairs of bagged trees limits the benefits of averaging
	- The idea in random forest is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance to much
		- This is achieved in the tree-growing process through random selection of the input variables
		- Typical values for $m$ are $\sqrt p$ or even as low as $1$
		- Reducing $m$ will reduce the correlation between any pair of trees in the ensemble and therefore reduce the variance of the average
	- After $B$ such trees $\{T(x;\Theta_b)\}^B_1$ are grown the random forest (regression) predictor is 
\begin{equation}
  \hat f_{rf}^B(x) = \frac1B\sum_{b=1}^BT(x;\Theta_b)
\end{equation}
- Where $\Theta_b$ characterizes the bth random forest tree in terms of split variables, cutpoints at each nodes, and terminal-node values

[[file:Random forests/screenshot_2018-10-09_08-08-09.png]]

* Boosting and Additive Trees
** Boosting Methods
- The motivation for *boosting* is to get a procedure that combines the outputs of many "weak" classifiers to produce a powerful "committee"

- One of the most popular algorithm called AdaBoost.M1 is as follows
	- Consider a two-class problem, with the output variable coded as $Y \in \{-1,1\}$
	- Given a vector of predictor variables $X$, a classifier $G(X)$ produces a prediction taking one of the two values $\{-1,1\}$
		- The error rate on the training sample is
\begin{equation}
  \bar{\text{err}} = \frac1N \sum_{i=1}^N I(y_i \ne G(x_i))
\end{equation}
- and the expected error on future predictions is $E_{XY}(I \ne G(X))$

- A *weak classifier* is one whose error rate is only slightly better than random guessing
	1. The purpose of boosting is to sequentially apply the weak classification algorithm to modified versions of the data and produces a sequence of weak classifiers $G_m(x),m=1,2,\dots,M$
	2. The predictions from all of them are then combined through a weighted majority vote to produce the final prediction
\begin{equation}
  G(x) = \text{sign} \bigg( \sum_{m=1}^M \alpha_mG_m(x) \bigg)
\end{equation}
- $\alpha_1, \alpha_2, \dots, \alpha_M$ are computed by the boosting algorithm and weight the contribution of each respective $G_m(x)$
	- This gives higher influence to the more accurate classifiers in the sequence 

[[file:Boosting and Additive Trees/screenshot_2018-10-21_20-24-38.png]]

** Boosting Fits an Additive Model 
- The key in boosting $G(x)$ expression
	- It is a way of fitting an additive expansion in a set of elementary "basis functions"
	- The basis functions are the individual classifiers $G_m(x) \in \{-1,1\}$
	- Basis function expansions take the form 
\begin{equation}
	f(x) = \sum_{m=1}^M\beta_mb(x; \gamma_m)
\end{equation}
- where $\beta_m$, $m=1,2,\dots,M$ are the expansion coefficients and $b(x;\gamma) \in \mathbb R$ are usually simple functions over the multivariate argument $x$ characterized by a set of parameters $\gamma$
	- Typically these models are fit by minimizing a loss function averaged over the training data
	- For many loss functions $L(y, f(x))$ and/or basis functions $b(x; \gamma)$ this requires computationally intensive numerical optimization techniques
	- A simple alternative can often be found when it is feasible to rapidly solve the subproblem of fitting just a single basis function
\begin{equation}
	\min_{\beta, \gamma} \sum_{i=1}^NL(y_i,\betab(x_i;\gamma))
\end{equation}

** Forward Stagewise Additive Modeling
[[file:Boosting and Additive Trees/screenshot_2018-10-21_20-44-17.png]]

- *Forward stagewise modeling* approximates a solution by sequentially adding new basis functions to the expansion without adjusting the parameters and coefficients of those that have already been added 
	- At each iteration $m$ one solves the optimal basis function $b(x; \gamma_m)$ and the corresponding coefficient $\beta_m$ to add to the current expansion $f_{m-1}(x)$
	- It produces $f_m(x)$ and the process is repeated

- For squared loss
\begin{equation}
	L(y,f(x)=(y-f(x))^2	
\end{equation}
- one has
\begin{equation}
  \begin{split} 
      L(y_i, f_{m-1}(x_1)+\beta b(x_i;\gamma)) &= (y_i-f_{m-1}(x_i)-\beta b(x_i;\gamma))^2
                                             &= (r_{im}-\beta b(x_i;\gamma))^2
  \end{split}
\end{equation}
- where $r_im = y_i - f_{m-1}(x_i)$ is simply the residual (difference) of the current modeland the ith observation
	- Therefore for squared-error loss, the term $\beta_m b(x;\gamma_m)$ that best fits the current residuals is added to the expansion at each step 

** Exponential Lost and AdaBoost  
- The *exponential loss function* is the following
\begin{equation}
	L(y,f(X))=\exp(-yf(x))
\end{equation}
- The exponential loss is more sensitive to changes in the estimated class probabilities 

- AdaBoost.M1 minimizes the exponential loss criterion via a forward-stagewise additive modeling approach

** Loss Functions and Robustness
*** Robust Loss Functions for Classification
#+NAME: lossFunctions
#+CAPTION: Loss functions for two-class classification
[[file:Boosting and Additive Trees/screenshot_2018-10-22_08-47-22.png]]
- Since the squared-error is not a decreasing function of the margin it is not good for classification
	- Due to it penalizing large positive correct margins

- For $K$ classification problems the logistic function generalizes nice 

*** Robust Loss Functions for Regression
[[file:Boosting and Additive Trees/screenshot_2018-10-22_09-19-17.png]]

- In the regression setting, analogous to the relationship between exponential loss and binomial log-likelihood is the relationship between *squared-error loss* $L(y,f(x))=(y-f(x))^2$ and *absolute loss* $L(y,f(x)) = | y - f(X)|$
	- The population solutions are
		- For squared-error loss: $f(x)=  E(Y \mid x)$
		- For absolute loss: $\text{median}(Y \mid x)$
	- For symmetric error distributions the two population solutions are the same
	- On finite samples squared-error loss places much more emphasis on observations with large absolutie residual $|y_i - f(x_i)|$ during the fitting process
		- It is far less robust and it performance severely degrades for the fitting process for long-tailed error distributions and especially for grossly mismeasured y-values ("outliers").
		- Other more robust criteria such as absolute loss perform much better in these situations 

- A criterion which is much more robust against outliers while being nearly as efficient as least squares is *Huber loss criterion* used for M-regression 
\begin{equation}
	L(y,f(x)) = 
		\begin{cases}
			\mbox{$[y-f(x)^2]$} & \mbox{for $|y-f(x)|\leq \delta$} \\
			\mbox{$2\delta| y - f(x) | - \delta^2$} & \mbox{otherwise} \\
		\end{cases}
\end{equation}

** Why Exponential Loss?
- The principal attraction of exponential loss in the context of additive modeling is computational
	- It leads to the simple modular reweighting AdaBoost algorithm
	- It has the same population minimizer has binomial log likelihood which is true probabilities 

** "Off-the-Shelf" Procedures for Data Mining
[[file:Boosting and Additive Trees/screenshot_2018-10-22_09-27-14.png]]

- Of all the well-known learning methods, decision trees come closest to meeting the requirements for serving as an off-the-shelf procedure for data mining
	- They are relatively fast to construct and they produce interpretable models (if the trees are small)
	- They have one aspect that prevents them from being the ideal tool forpredictive learning, namely inaccuracy.
		- They seldom provide predictive accuracy comparable to the best that can be achieved with the data at hand

** Boosting Trees
- A tree can be formally expressed as
\begin{equation}
  T(x; \Theta) = \sum_{j=1}^J\gamma_iI(x \in R_j)
\end{equation}
- with parameters $\Theta = \{R_j. \gama_j\}_1^J$
	- $J$ is usually treated as a meta parameters 
	- The parameters are found by minimizing the empirical risk
\begin{equation}
	\hat \Theta = \arg \min_\Theta \sum_{j=1}^J \sum_{x_i \in R_j} L(y_i, \gamma_j)
\end{equation}	
- The optimization problem is useful to divide into two parts
	- *Finding $\gamma_j$ given $R_j$*: $\hat \gamma = \bar y_j$ is often used and for misclassification $\hat \gamma$ is the modal class of the observations falling in region $R_j$
	- *Finding $R_j$*: It is the difficult part, for which approximate solutions are found
		- Finding $R_j$ typically involves estimating $\gamma _j$ as well
		- The typically strategy is to use a greedy, top-down recursive partitioning algorithm to find it
		- It is sometimes necessary to approximate the empirical risk by a smoother and more convenient criterion for optimizing the $R_j$:
\begin{equation}
	  \tilde \Theta = \arg \min_\Theta \sum_{j=1}^N \tilde L(y_i, T(x_i, \Theta))
\end{equation}	
- Then given the $\bar R_j = \tilde R_j$, the $\gamma$ can be estimated

- The classification trees described in 9.2 using the *Gini index* instead of misclassification loss in growing the tree is used in the *boosted tree model* as a sum
\begin{equation}
  f_M(x) = \sum_{m=1}^MT(x; \Theta_m)
\end{equation}
- induced in a forward stagewise manner
	- At each step in the forward stagewise procedure one must solve
\begin{equation}
	  \hat \Theta_m = \arg \min_{\Theta_m} \sum_{j=1}^N L(y_i, f_{m-1}(x_i)T(x_i, \Theta_m))
\end{equation}	
- for the region set and constants $\Theta_m = \{R_{jm}, \gamma_{jm}\}_1^{J_m}$ of the next tree given the current model $f_{m-1}(x)$
	- Given the regions $R_{jm}$ finding the optimal constants $\gamma_{jm}$ in each region is typically straightforward:
\begin{equation}
  \hat \gamma = \arg \min_{\gamma_{jm}} \sum_{x_i \in R_{jm}} L(y_i, f_{m-1}(x_i)+\gamma_{jm})
\end{equation}
- Finding the regions is difficult, and even more difficult than for a single tree, though for a few special cases it simplifies
	- For squared-error loss, the solution is no harder than for a single tree
		- It is simply the regression tree that best predicts the current residuals $y_i-f_{m-1}(x_i)$ and $\hat \gamma _{jm}$ is the mean of these residuals in each corresponding region
	- For two-class classification and exponential loss, this stagewise approach gives rise to the AdaBoost method for boosting classification trees
		- If the trees $T(x; \Theta_m)$ are restricted to be scaled classification trees the the solution is one that minimizes the weighted error rate $\sum_i=1^N w_i^{(i)} I(y_i \ne T(x_i;\Theta_m))$ with weights $w_i^{(m)}= e^{-y_if_{m-1}(x_i)}$
		- A scaled classification tree is $\beta_mT(x;\Theta_m)$ with the restriction that $\gamma_{jm} \in \{-1,1\}$
		- A non-scaled classification three the weighted exponential criterion for the new tree is
\begin{equation}
  \hat \theta_m = \arg \min_{\Theta_m} \sum_{i=1}^N w_i^{(m)} \exp[-y_iT(x_i;\Theta_m)]
\end{equation}
- It is straightforward to implement a greedy recursive partioning algorithm using this weighted exponential loss as a splitting criterion
	- Given the $R_{jm}$ one can show that solution is the weighted log-odds in each corresponding region
[[file:Boosting and Additive Trees/screenshot_2018-10-22_10-28-20.png]]

- Using loss criteria such as the absolute error or the Huber loss in place of squared-error loss for regression, and the deviance in place of exponential loss for classification, will serve to robustify boosting trees
	- Unfortunately, unlike their nonrobust counterparts, these robust criteria do not give rise to simple fast boosting algorithms

** Numerical Optimization via Gradient Boosting
*** General
- The loss in using $f(x)$ to predict $y$ on the training training data is 
\begin{equation}
  L(f) = \sum_{i=1}^N L(y_i,f(x_i))
\end{equation}
- The goal is to minimize $L(f)$ with respect to $f$
	- Where $f(x)$ is constrained to be a sum of trees.
	- Ignoring the constraint, minimizing it can be viewed as a numerical optimization
\begin{equation}
  \pmb{\hat f} = \arg \min_{\pmb{f}} L(\pmb f) 
\end{equation}
- where the "parameters $\pmb f \in \mathbb R ^n$ are the values of the approximating function $f(x_i)$ for each of the $N$ data points $x_i$
\begin{equation}
  \pmb f = \{f(x_1), f(x_2), \dots, f(x_N)\}^T
\end{equation}
- Numerical optimization procedures solve it as a sum of component vectors 
\begin{equation}
	\pmb f_M = \sum_{m=0}^M \pmb h_m, \ \pmb h_m \in \mathbb R ^ n
\end{equation}
- where $\pmb f_0 = \pmb h_0$ is the initial guess, and each successive $\pmb f_m$ is based on the current parameters vecor $\pmb f_{m-1}$ which is the sum of the previously induced updates induced updates.
	- Numerical optimization methods differ in their prescriptions for computing each increment vector $\pmb h_m$

*** Steepest Decent 
- Steepest decent chooses $\pmb h_m = - \rho_m \pmb g_m$ where $\rho_m$ is a scalar and $\pmb g_m \in \mathbb R^N$ is the gradient of $L(\pmb f)$ evaluated at $\pmb f = \pmb f_{m-1}$
	- The component of the gradient $\pmb g_m$ are
\begin{equation}
  g_{im} = \bigg[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\bigg]_{f(x_i)=f_{m-1}(x_i)}
\end{equation}
- The step length $\rho_m$ is the solution to 
\begin{equation}
  \rho_m = \arg \min_\rho L(\pmb f_{m-1}-\rho \pmb g_m)
\end{equation}
- The current solution is the updated
\begin{equation}
  \pmb f_m = \pmb f_{m-1}-\rho_m\pmb g_m
\end{equation}
- and the process repeated at the next iteration
	- It can be viewed as a very greedy strategy, since $\pmb g_m$ is the local direction in $\mathbb R^N$ for which $L(\pmb f)$ is most rapidly decreasing

*** Gradient Boosting 
[[file:Boosting and Additive Trees/screenshot_2018-10-22_21-24-34.png]]

- Forward stagewise boosting is a very greedy strategy
	- The tree prediction $T(x_i;\Theta_m)$ are analogous to the components of the negative gradient
		- The difference between them are that the tree components $\pmb t_m = \{T(x_1; \Theta_m), \dots T(x_N; \Theta_m)\}$ are not independent
		- They are constrained to the predictions of a $J_m$ terminal node whereas the negative gradient is the unconstrained maximal descent direction
		- A way to solve this is to induce a tree $T(x;\Theta_m)$ at the mth iteration whose predictions $\pmb t_m$ are as close as possible to the negative gradient which leads to
\begin{equation}
    \tilde \theta_m = \arg \min_\theta \sum_{i=1}^N (-g_{im}-T(x_i;\theta))^2
\end{equation}
- One fits the tree $T$ to the negative gradient values by least squares
	- Even though the solution regions $\tilde R_{jm}$ to this will not be the same as the solution regions to the general loss function, it will be similar enough to server the same purpose 

*** Implementations of Gradient Boosting
[[file:Boosting and Additive Trees/screenshot_2018-10-22_21-27-02.png]]
- The algorithm for classification is similar
	- Lines 2(a)-(d) are repeated $K$ times at each iteration $m$, once for each class using 10.38
	- The result at line 3 is $K$ different (coupled) tree expansions $f_{kM}(x), k=1,2,\dots,K£
	- The result produce probabilities via the softmax function

** Right-Sized Trees for Boosting 
- Using pruning to decide the size of the tree results in too large trees to begin with
	- The simplest strategy for avoiding this is to restrict all trees to be the same size $J_m = J \forall m$
	- At each iteration a $J$ terminal node regression tree is induced
	- $J$ becomes a meta-peter of the entire boosting procedure to be adjusted to maximize the estimate performance

- One can get an idea of useful values for $J$ by considering the properties of the target function
\begin{equation}
  \eta = \arg \min_f \E_{XY} L(Y,f(X))
\end{equation}
- The target function $\eta(x)$ is the one with minimum prediction risk on future data
	- This is the function we are trying to approximate
	- On important property of $\eta(X)$ is the degree to which the coordinate variables $X^T=(X_1,X_2, \dots, X_p)$ interact with each other
	- ANOVA expansions catches the about of interaction between coordinate variables
\begin{equation}
  \eta(X) = \sum_j \eta_j(X_j) + \sum_{jk} \eta_jk (X_j, X_k) + \sum_{jkl} \eta_jkl (X_j, X_k, X_l) + \dots
\end{equation}
- The sum is over functions of only a single predictor variable $X_j$
	- The particular functions $n_j(X_j)$ are those that jointly best approximate $\eta(X)$ under the loss criterion being use
		- They are called the "main effect" of X_j@
	- The second sum over two variable functions are called second-order interactions of each respectable variable pair
	- The third sum over three variable functions are called third-order interactions and so on
	- For many problems in practice are typically dominate by low-order interactions
		- When this is the case, models that produce strong higher-order interaction effect such as large decision trees suffer in accuracy

- The interaction level of tree based approximations is limited by the tree size $J$
	- No interactions effect of level greater than $J-1$ are possible
	- Since boosted model are additive in the trees this limits them as well
	- Setting $J=2$ produces models with on allow models with only main effects
	- Setting $J=2$ produces models where two variable interaction effects are also allowed and so on
	- Experience so far indicates that $4 \leq J \leq 8$ works well in the context of boosting, with results being fairly insensitive to particular choices in this range

** Regularization 
*** General 
- The other meta-parameter of gradient boosting is the number of boosting interactions $M$
	- Each iteration usually reduces training risk $L(f_M)$
	- For large enough $M$ the risk can be made arbitrarily small
		- This can lead to overfitting
	- The optimal number $M^*$ for minimizing future risk is application dependent
		- A way to estimate $M$ is to use a validation sample

*** Shrinkage
- The simplest implementation of shrinkage in the context of boosting is to scale the contribution of each tree by a factor $0 < \nu < 1$ when it is added to the current approximation
	- Line 2(d) of algorithm 10.3 is replaced by 
\begin{equation}
  f_m(x) = f_{m-1}(x) + \nu \cdot \sum_{j=1}^J \gamma_{j_m} I(x \in R_{jm})
\end{equation}
- The parameter $\nu$ can thought of ass the learning rate of the boosting procedure
	- Smaller values of $\nu$ results in larger training risk for the same number of iterations $M$
	- Both $\nu$ and $M$ control prediction risk on the training data
		- They are no independent
		- Smaller values of $\nu$ lead to larger values of $M$
	- It has been found that smaller values of $\nu$ favor better test error
		- The best strategy appears to be to set $\nu$ to be very small ($\nu < 0.1$) and then choose $M$ by early stopping
		- Yields dramatic improvements for regression and probability estimatio
	- The price paid for using $\nu$ is computation
		- Many iterations are generally computationally feasible
		- Even on larger data sets
		- Due to it operating on small trees with no pruning

*** Subsampling
- With /stochastic gradient boosting/, at each iteration we sample a fraction $\eta$ of the training observations (without replacement) and grow the next tree using that subsample
	- The rest of the algorithm is identical
	- A typically value for $\eta$ can be $\frac12$
		- For larger $N$ is can be substantially smaller than $\frac12$
	- It not only reduces computing time but in many cases it actually produces a more accurate model

- The downside is that we have four parameters to set: $J,M, \nu$ and $\eta$
	- Typically some early explorations determine suitable values for $J, \nu$ and $\eta$ leaving $M$ as the primary parameter

** Interpretation 
*** General
- Single decision trees are highly interpretable.
	- The model can be completely represented by a simple two-dimensional graphic (binary tree) that is easily visualized.
	- Linear combinations of trees (10.28) lose this important feature, and must therefore be interpreted in a different way.

*** Relative Importance of Predictor Variables
- In data mining applications the input predictor variables are seldom equally relevant.
	- Often only a few of them has substantial influence on the response
		- Vast majority are irrelevant
	- It is often useful to learn the relative importance or contribution of each variable in an given prediction
		- For a single decision tree $T$ the following is a measurement of relevance for ech predictor variable $X_\ell$ 
\begin{equation}
  \mathcal{T}^2(T) = \sum_{t=1}^{J-1}\hat i_t^2 I(v(t) = \ell)
\end{equation}  
- The sum is over the internal nodes of the tree.
	- At each node $t$ one of the input variables $X_{v(t)}$ is used to partition the  region associated with that node into two sub regions
		- Within each a separate constant is fit to the response values
		- The particular variable chosen is the once that gives maximal estimate improvement $\hat i_t^2$ in squared error risk over that for a constant fit over the entire region
		- The squared relative importance of variable $X_\mathcal L$ is the sum of sub squared improvements over all internal nodes
	- This importance is easily generalized to additive tree expansions
\begin{equation}
  \mathcal T^2_\ell = \frac1M \sum_{m=1}^M\mathcal T^2_\ell (T_m)
\end{equation}
- This measure is more reliable than for a single tree
	- Both measurements referees to the squared relevance  

*** Partial Dependence Plots
- The step after identifying the most relevant variable is to attempt to understand the nature of the dependence of the approximation $f(X)$ on their join values
	- Graphical renderings of the $f(X)$ as a functions of its arguments gives a comprehensive summary of this de dependence 
		- Limited to low-dimensional view
		- Can be viewed by fixing all but one or two producing a trellis of plots

- A general function $f(X)$ will in principle depend on all of the input variables: $f(x)=f(X_\mathcal{S}, X_\mathcal{C})$ 
	- One way to define the average or partial dependence of $f(X)$ on $X_\mathcal{S}$ is 
\begin{equation}
  f_{\mathcal S} (X_\mathcal S) = E_{X_c}f(X_\mathcal S,X_\mathcal C)
\end{equation}
- This can server as a useful description of the effect of the chosen subset on $f(X)$
	- e.g. when the variables in $X_\mathcal S$ do not have strong interactions with those in $X_\mathcal C$
	- 

- Partial dependence function can used to interpret the result of any "black box" learning method
	- One way to estimate them is
\begin{equation}
    \hat f_{\mathcal S} (X_\mathcal S) = \frac1N \sum_{i=1}^Nf(X_\mathcal S,X_{i'\mathcal C})
\end{equation}
- where $x_{1\mathcal C}, x_{2\mathcal C}, \dots, x_{N\mathcal C}$ are the values of $X_\mathcal C$ occurring in the training data
	- It can be rapidly computed from the tree itself without reference to the data

* Conditional probabilities and graphical models
** You have a joint probability — Now what?
- We can split a variables into three kinds
	- Those we have observed
		- Which are thus no longer random 
	- Those we don't care about
		- They are called nuisance parameter
		- They are just there to build the model
	- The parameters we are interested in
		- It can be underlying parameters
		- Future events we want to predict

- *Condition on observed parameters*: If we have the joint probability $p(X,Y,Z)$ but we have already observed $Z = z$ we are no longer interested in the outcome of $Z$ any longer
	- We will still be interested in $X$ and $Y$
	- When observing $Z$ we move our interest from $p(X,Y,Z)$ to $p(X,Y \mid Z)$
	- It can in general be done from marginalization e.g.
\begin{equation}
  p(X,Y \mid Z) = \frac{p(X,Y,Z)}{p(Z)}
\end{equation}
- and
\begin{equation}
  p(Z) = \sum_X \sum_Y p(X,Y,Z) 
\end{equation}

- *Marginalise away nuisance parameters:* If one is only interested in the outcome of $X$ and the $Y$ variable that was needed to connect the variable of interest to the variable $Z$ but in itself has no use
	- The interest does not lie in $p(X,Y \mid Z)$ but in $p(X \mid Z)$ which can be obtained from marginalization 
\begin{equation}
  p(X \mid Z) = \sum_Y $p(X,Y \mid Z)$
\end{equation}

- The summing over all possible values used in the marginalisation is potentially computational intractable, so one cannot always simply do this
	- Some times clever algorithms are necessary, but they will typically always just do one of those two things in a clever way

** Dependency graphs
#+NAME: dependencyGraphEksp
#+CAPTION: Dependency Graph Example for the distribution $p(X_1, X_2, X_3, X_4)$
[[file:Sequential Data/screenshot_2018-10-29_18-41-07.png]]

- A graphical notation for describing dependency relationships when specifying a joint distribution
	- Each random variable is represented as a node
	- Whenever the composition of the joint probability has a term $p(Y \mid X, \dots, X_k)$ we have directed edges from all $X_i$'s to $Y$

* Sequential Data 
** Markov Models
- Without the loss of generality one can use the product rule to express the joint distribution for a sequence of observations in the form
\begin{equation}
  p(\pmb x_1, \dots, \pmb x_N) = \prod_{n=1}^N p(\pmb x_n \mid \pmb x_1, \dots, \pmb x_{n-1})
\end{equation}
- If it is assumed that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent, we obtain the first-order Markov chain
	- The joint distribution for a sequence of $N$ observations under this models is given by 
\begin{equation}
  p(\pmb x_1, \dots, \pmb x_N) = p(\pmb x_1) \prod_{n=2}^N p(\pmb x_n \mid \pmb x_{n-1})
\end{equation}
- The conditional distribution for observation $\pmb x_n$, given all of the observations up to time $n$ is this given by
\begin{equation}
	p(\pmb x_n \mid \pmb x_1, \dots, \pmb x_{n-1}) = p(\pmb x_n \mid \pmb x_{n-1})
\end{equation}
- In most applications of these model, the conditional distributions $p(\pmb x_n \mid \pmb x_{n-1})$ that define the model will be constrained to be equal
	- Known as a *homogeneous Markov chain*
	- Still very constrained even though it is more general than the independent model

- If predictions also are allowed to depend on the previous-but-one value we obtained a second order Markov chain where the joint distribution know is given by
\begin{equation}
	p(\pmb x_1, \dots, \pmb x_N) = p(\pmb x_1) p(\pmb x_2 \mid \pmb x_1) \prod_{n=3}^N p(\pmb x_n \mid \pmb x_{n-1}, \pmb x_{n-2})  
\end{equation}  
- Each observation is now influenced by two previous observations

- One can consider extensions to an $M^\text{th}$  order Markov chain in which the condition distribution for a particular variable depends on the previous $M$ variables
	- The price paid for this increased flexibility is that the number of paramers in the models is now much larger
	- The number of parameters is $K^{M-1}(K-1)$ which grows exponential with $M$ and is therefore impractical for higher numbers of $M$ 

- For continuous variables we can use linear-Gaussian conditional distributions in which each node has a Gaussian distribution whose mean is a linear function of its parents
	- Known as an autoregressive or AR model

- An alternative approach is to use a parameter model for $p(x_n \mid x_{n-M}, \dots, x_{n-1})$ such as a neural network
	- Sometime called a tapped delay line
	- The number of parameters can be much smaller than in a completely general model
	- It is achieved at the expense of a restricted family of conditional distributions 


[[file:Sequential Data/screenshot_2018-10-29_11-21-18.png]]
- To build a model for sequences that is not limited by the Markov assumption in any order and that can be specified using a limited number of free parameters
	- It can be achieve by introducing additional latent variables to permit a rich class of models to be constructed out of simple components
	- For each observation $\pmb x_n$ there is introduced a corresponding latent variable $\pmb z_n$
		- May be a different type of dimensionality ti the observed variable
	- It is assumed that it is the latent variables that form a Markov chain
		- It gives rice to a graphical structure known as a state space model
		- It satisfies the key conditional independence property that $\pmb z_{n-1}$ and $\pmb z_{n+1}$ are independent given $\pmb z_{n}$ so that 
\begin{equation}
  \pmb z_{n+1} \perp\mkern-9.5mu\perp \pmb z_{n-1} \mid \pmb z_n
\end{equation}
- The joint distribution for this model is given by 
[[file:Sequential Data/screenshot_2018-10-29_11-18-12.png]]
- There is always a path connecting any two observed variables $\pmb x_n$ and $\pmb x_m$ via the latent variables
	- The observation for a given $\pmb x_{n+1}$ depends on all previous observations 

** Hidden Markov Models
*** General 
- The hidden Markov model can be viewed as a specific instance of the state space model where the latent variables are discrete 
	- If one examines a single time slice in the model it corresponds to a mixture distribution with component densities given by $p(\pmb x \mid \pmb z)$
	- In the case of a standard mixture model the latent variables are discrete multinomial variables $\pmb z_n$ describing which component of the mixture is responsible or generating the corresponding observation $\pmb x_n$
		- It is convenient to use a 1 of $K$ coding scheme
	- The probability distribution of $\pmb z_n$ is allowed to depend on the state of the previous latent variable $\pmb z_{n-1}$ through a conditional distribution $p(\pmb{z}_n | \pmb{z}_{n-1})$
		- Since they are $K$ dimensional binary variable, the conditional distribution corresponds to a table of numbers that we denote by $\pmb A$
			- Its elements are known as *transition probabilities*
			- They are given by $A_{jk} = p(z_{nk} = 1 \mid z_{n-1,j})$
			- Since they are probabilities they satisfy $0 \leq A_{jk}} \leq 1$ with $\sum_k A_{jk} = 1$
			- It has $K(K-1)$ independent parameters
			- The conditional distribution can be written explicitly in the form 
\begin{equation}
  p(\pmb z_n \mid \pmb z_{n-1, \pmb A}) = \prod_{k=1}^K \prod_{j=1}^K A_{jk}^{z_{n-1},jz_{nk}}
\end{equation}
- The initial latent node $z_1$ is special in that it does not have a parent node
	- It has a marginal distribution $p(\pmb z_1)$ represented by a vector of probabilities $\pmb \pi$ with elements $\pi \equiv p(z_{1k}=1)$ so that 
\begin{equation*}
  p(\pmb z_1 \mid \pmb \pi) = \prod_{k=1}^K \pi_k^{z_{1k}}
\end{equation*}
where $\sum_k \pi_k = 1$ 

- The transition matrix is sometimes illustrated diagrammatically by drawing the state as nodes in a state transition diagram e.g. 
[[file:Sequential Data/screenshot_2018-10-29_16-41-00.png]]

- It is sometimes to take a state diagram and unfold over time
	- This gives an alternative representation known as a /lattice/ or /trellis/ diagram
	- e.g. 
[[file:Sequential Data/screenshot_2018-10-29_16-43-52.png]]

- The specification of the probabilistic model is completed by defining the conditional distributions of the observed variables $p(\pmb x_n \mid \pmb z_n, \phi)$, where $\phi$ is a set of parameter governing the distribution
	- These are known as *emission probabilities*
	- They might e.g. be given by Gaussians if the elements of $\pmb x$ are continuous variables or by conditional probability tables if $\pmb x$ is discrete
	- Since $\pmb x_n$ is observed the distribution $p(\pmb x_n, \pmb z_n, \phi)$ consists for a given value of $\phi$ of a vector of $K$ numbers corresponding to the $K$ possible states of the binary vector $\pmb z_n$
	- The emission probabilities can be represented in the form 
\begin{equation}
  p(\pmb x_n \mid \pmb z_n, \phi) = \prod_{k=1}^K p(\pmb x_n \mid \phi_k)^{z_{nk}}
\end{equation}

- The joint probability distribution for a /homogeneous/ model over both latent and observed variables is given by 
[[file:Sequential Data/screenshot_2018-10-29_16-55-53.png]]
- where $\pmb X = \{\pmb x_1, \dots, \pmb x_n\}, \pmb Z = \{\pmb z_1, \dots, \pmb z_n\}$, and $\pmb \theta = \{\pmb \pi, \pmb A, \phi \}$ denoted the set of parameter governing the model 

- It is generated the following way
	1. We first choose the initial latent variable $\pmb z_1$ with probabilities govern by the parameters $\pi_k$
		 - We then sample the corresponding observation $\pmb x_1$
	2. We chose the state of the variable $\pmb z_2$ according to the transition probabilities $p(\pmb z_2 \mid \pmb z_1)$ using the already instantiated value of $\pmb z_1$
	3. If the sample for $\pmb z_1$ corresponds to state $j$, then we chose state $k$ of $\pmb z_2$ with probabilities $A_{jk}$ for $k=1,\dots K$
	4. Once we know $\pmb z_2$ we can draw a sample for $\pmb x_2$ and the next latent variable $\pmb z_3$ and so on 

- If the model has large transition elements $A_{kk}$ that are much larger than the off-diagonal elements a typical data sequence will have long runs of points generated from a single component

- There are many variants of the standard HMM model
	- It can e.g. be obtained for instance by putting constraints on the form of the transition matrix $\pmb A$ 
	- An example if the /right-to-left/ HMM, which is obtained by setting the elements $A_{jk}$ of $\pmb A$ to zero if $k<j$ 
		- Such models has typically their initial states probabilities for $p(\pmb z_1)$ modified such that $p(z_{11} = 1) and $p(z_{1j}) = 0$ for $j \ne 1$
		- The state is typically constrained to start in state 1
		- It can also be further constrained to ensure that large changes in the state index do not occur, so that $A_{jk} = 0$ if $k>k+ \Delta$

*** Decodings
- *Posterior decoding*: $\pmb z^*_n$ is the most likely state to be in the nth step
\begin{equation*}
  \pmb z^*_n = \arg \max_{\pmb z_n} p(\pmb z_n \mid \pmb x_1, \dots, \pmb x_N)
\end{equation*}

*** Problems 
- *Evaluation problem*
	- What is the probability that a particular sequence of symbols is produced by a particular model
	- Two algorithms are used  (DO NOT confuse them with the forward-backward algorithm).
		1. The forward algorithm
		2. The backwards algorithm

- *Decoding problem*
	- Given a sequence of symbols (your observations) and a model, what is the most likely sequence of states that produced the sequence.
	- For decoding we use the Viterbi algorithm.

- *Training problem*
	- Given a model structure and a set of sequences, find the model that best fits the data.
		- Can be solved by the following 3 algorithms:
			- MLE (maximum likelihood estimation)
			- Viterbi training(DO NOT confuse with Viterbi decoding)
			- Baum Welch = forward-backward algorithm

*** Maximum likelihood for the HMM
- If one have observed a data set $\pmb X = \{\pmb x_1, \dots, \pmb x_N\}$ one can determine the parameters of an HMM using maximum likelihood
	- It can be obtained from the joint distribution by marginalizing over the latent variables 
\begin{equation}
  p(\pmb X \mid \pmb \theta) = \sum_{\pmb Z} p(\pmb X, \pmb Z, \mid \pmb \theta)
\end{equation}
- Since it does not factorize over $n$ one cannot simply treat each of the summations over $\pmb z_n$ independently
	- The summations cannot be performed explicitly because there are $N$ variables to be summed over each of which has $K$ states resulting in $K^N$ terms
	- A further difficulty with this expression is that, because it corresponds to a generalization of a mixture distribution, it represents a summation over the emission models for different settings of the latent variables.
		- Directly maximizing over this will therefore lead to complex expressions with no closed form solutions

- The *EM algorithm* is used to find an efficient framework for maximizing the likelihood function in hidden Markov models
	- It starts with some initial selection for the model parameters which is denoted $\pmb \theta^\text{old}$
	- In the E step the parameter values are taken to find the posterior distribution of the latent variables $p(\pmb Z \mid \pmb X, \pmb \theta^\text{old})$
		- It then uses this to evaluate the expectation of the logarithm of the complete-data likelihood function, as a function of the parameters $\theta$, to give the function $Q(\pmb \theta, \pmb \theta^\text{old})$ defined by
\begin{equation}
  Q(\pmb \theta, \pmb \theta^\text{old}) = \sum_{\pmb Z} p(\pmb Z \mid \pmb X, \theta^\text{old}) \ln p(\pmb X, \pmb Z \mid \pmb \theta)
\end{equation}
- $\gamma(\pmb z_n)$ is used to denote the marginal posterior distribution of a latent variable $\pmb z_n$ and $\xi(\pmb z_{n-1}, \pmb z_n)$ to denote the joint posterior distributions of two successive latent variables so that 
\begin{equation}
  \begin{split} 
    \gamma (\pmb z_n) &= p(\pmb z_n \mid \pmb X, \pmb \theta^\text{old})\\
    \xi (\pmb z_{n-1},\pmb z_n) &= p(\pmb z_{n-1},\pmb z_n \mid \pmb X, \pmb \theta^\text{old})
  \end{split}
\end{equation}
- For each value of $n$
	- We can store $\gamma(\pmb z_n)$ using a set of $K$ non-negative number that sum to unity
	- We can store $\xi (\pmb z_{n-1},\pmb z_n)$ using a $K \times K$ matrix of non-negative numbers that sum to unit
- $\gamma (z_{nk})$ is used tot denote the condition probability of $z_{nk}=1$
	- A similar notions is used for $\xi (z_{n-1,j},z_{nk})$
- Since the expectation of binary random variables is just the probability that it takes the value $1$ we have 
\begin{equation}
  \begin{split} 
    \gamma (z_n) &= \mathbb E[z_{nk}] = \sum_{\pmb z} \gamma(\pmb z) z_{nk} \\
    $\xi (z_{n-1,j},z_{nk})$ &= \mathbb E[z_{n-1,j}z_{nk}] = \sum_{\pmb z} \gamma(\pmb z) z_{n-1,j}z_{nk} 
  \end{split}
\end{equation}
- If we make use of the definitions of $\gamma$ and $\xi$ we obtain
[[file:Sequential Data/screenshot_2018-11-04_09-38-18.png]]
- The goal of the E step is to evaluate the quantities $\gamma(\pmb z_n)$ and $\xi(\pmb z_{n-1}, \pmb z_n)$ efficiently 

- In the M step we maximize $Q(\pmb \theta, \pmb \theta^\text{old})$ with respect to the parameters $\pmb \theta = \{\pmb \pi, \pmb A, \pmb \phi\}$ the parameters $\gamma(\pmb z_n)$ and $\xi(\pmb z_{n-1}, \pmb z_n)$ are treated as constants
	- Maximization with respect to $\pmbm \pi$ and $\pmb A$ is easily achieved by using appropriate Lagrange multipliers with the results
[[file:Sequential Data/screenshot_2018-11-04_09-46-05.png]]

- The EM algorithm must be initialized by choosing starting values for $\pmb \pi$ and $\pmb A$
	- It should respect the summation constraints associated with their probabilistic interpretation
	- Any elements of $\pmb \pi$ or $\pmb A$ that are initially will remain zero in subsequent EM update
	- A typical initialization procedure would involve selecting random starting values of the parameters subject to the summation and non-negativity constraints
	- There are no particular modification to the EM results for the left-to-right models beyond choosing initial values for the elements $A_{jk}$ in which the appropriate elements are set to zero

- To maximize $Q(\pmb \theta, \pmb \theta^\text{old})$ with respect to $\phi_k$ it is only the final term in the extended form that depends on $\phi_k$
	- It has the same form as the data-dependent term in the corresponding function for a mixture distribution for i.i.d. data
	- We are simply maximizing the weighted log likelihood function for emission density $p(\pmb x \mid \pmb \phi_k)$ with weights $\gamma(z_{nk})$
	- In the case of Gaussian emission densities we have $p(\pmb x \mid \phi_k) = \mathcal N (\pmb x \mid \pmb \mu_k, \pmb \Sigma_k)$ and the maximization of the function $Q(\pmb \theta, \pmb \theta^\text{old})$ gives
[[file:Sequential Data/screenshot_2018-11-04_09-58-10.png]]

- For the case of discrete multinomial variables the conditional distribution takes the form
[[file:Sequential Data/screenshot_2018-11-04_09-59-08.png]]
- and the corresponding M-step equations are given by
[[file:Sequential Data/screenshot_2018-11-04_09-59-28.png]]
- The EM algorithm requires initial values for the parameters of the emission distribution.
	- A way to set these is first to treat the data initially as i.i.d. and fit the emission density by maximum likelihood, and then use the resulting values to initialize the parameters for EM.

*** The forward-backward algorithm 
- The *alpha-beta algorithm* is a variant of the *forward-backward algorithm*

- Since the posterior distributions of the latent variables is independent of the form $p(\pmb x \mid \pmb z)$ all we require is the values of the quantities $p(\pmb x_n \mid \pmb z_n)$ for each value of $\pmb z_n$ for every $n$
	- Therefore the following conditional independence properties does also hold
[[file:Sequential Data/screenshot_2018-10-30_09-50-41.png]]
- where $\pmb X = \{\pmb x_1, \dots, \pmb x_N\}$ 

- To evaluate $\gamma (z_{nk})$ we can we can use the following formula which uses the Bayers' theorem 
\begin{equation}
  \gamma (\pmb z_n \mid \pmb X) = \frac{p(\pmb X \mid \pmb z_n)p(\pmb z_n)}{p(\pmb X)}
\end{equation}
- The denominator $p(\pmb X)$ is implicitly conditions on the parameter $\pmb \theta^\text{old}$ of the HMM and there for represents the likelihood function
	- Based on property (13.24) together with the product rule of probability we obtain
\begin{equation}
  \gamma (\pmb z_n) = \frac{p(\pmb x_1, \dots, \pmb x_n,\pmb z_n)p(\pmb x_{n+1}, \dots, \pmb x_N | \pmb z_n)}{p(\pmb X)} = \frac{\alpha(\pmb z_n)\beta(\pmb z_n)}{p(\pmb X)} 
\end{equation}
- where the following is defined 
\begin{equation}
  \begin{split} 
    \alpha (\pmb z_n) &\equiv p(\pmb x_1, \dots, \pmb x_n,\pmb z_n) \\
    \beta (\pmb z_n) &\equiv p(\pmb x_{n+1}, \dots, \pmb x_N \mid \pmb z_n)
  \end{split}
\end{equation}
- The quantity $\alpha (\pmb z_n)$ represents the joint probability of observing all of the given data up to time $n$ and the value of $\pmb z_n$
	- $\alpha(z_{nk})$ is used to denote the value of $\alpha (\pmb z_n)$ when $z_{nk}=1$
- $\beta (\pmb z_n)$ represents the conditional probability of all future data from time $n+1$ up to $N$ given the value of $\pmb z_n$ 

- To more efficiently compute $\alpha(\pmb z_n)$ the following equation can be used 
\begin{equation}
  \alpha(\pmb z_n) = p(\pmb x_n \mid \pmb z_n) \sum_{\pmb z_{n-1}} \alpha (\pmb z_{n-1}) p(\pmb z_n \mid \pmb z_{n-1})
\end{equation}
- In order to start computing this recursively an initial condition is need which is given by 
[[file:Sequential Data/screenshot_2018-10-30_10-30-44.png]]
- this tells us that $\alpha (z_{1k})$, for $k=1,\dots, K$ takes the value $\pi_kp(\pmb x_1 \mid \pmb \phi_k)$
	- To evaluate $\alpha(\pmb z_n)$ along the chain for every latent node the cost is $O(K^2N)$ 

- Another way to evaluate $\beta(\pmb z_n)$ is using the following formula
[[file:Sequential Data/screenshot_2018-10-30_19-20-42.png]]
- The starting point for recursion is now $\beta(\pmb z_N)$ which is equal to
\begin{equation}
  p(\pmb z_n \mid \pmb X) = \frac{p(\pmb X, \pmb z_n)\beta(\pmb z_n)}{p(\pmb X)}
\end{equation}

- If we sum both sides over $\pmb z_n$ and just the fact that the left-hand side is a normalized distribution we obtain 
\begin{equation}
  p(\pmb X) = \sum_{\pmb z_n} \alpha ( \pmb z_n) \beta (\pmb z_b)
\end{equation}

$p(\pmb X)$ can be computed by the following formula
\begin{equation}
  p(\pmb X) = \sum_{\pmb z_n} \alpha (\pmb z_N)
\end{equation}
this is much better computing it by summing over all possible values of $\pmb Z$ 

- To implement the forward algorithm without numeric problems $\hat \alpha (\pmb z_n)$ is used 
\begin{equation}
	\hat \alpha (\pmb z_n) = \frac{\alpha (\pmb z_n)}{\prod_{m=1}^n c_m}
\end{equation}
- where $c_n = p (\pmb x_n \mid \pmb x_1, \dots, \pmb x_{n-1})$ 

- To compute the basis step the following formula is used
\begin{equation}
  \hat \alpha (\pmb z_1) = \frac{\alpha(\pmb z_1)}{c_1}
\end{equation}
- where $c_1 = \sum_{k=1}^{K} \pi_k p(\pmb x_1 \mid \phi_k)$

- To compute the recursion step the following formula is used
\begin{equation}
  \delta (\pmb z_{n}) = c_n \hat a (\pmb z_{n}) = p(\pmb x_n \mid \pmb z_n) \sum_{\pmb z_{n-1}} \hat \alpha (\pmb z_{n-1}) p(\pmb z_n \mid \pmb z_{n-1})
\end{equation}
- The $c_n$ is computed an stored as
\begin{equation}
	c_n = \sum_{k=1}^K \delta (z_{zk})
\end{equation}
- Then compute and store $\hat \alpha(z_{nk}) = \delta(z_{nk})/c_n$ 

- To implement the backward algorithm without numeric problems $\hat \beta (\pmb z_n)$ is used 
\begin{equation}
  \hat \beta = {\beta(\pmb z_b)}{\prod_{m=n+1}^N c_m}
\end{equation}
- The basis step is $\hat \beta (\pmb z_N) = 1$ as the unscaled version
- The recursion step $n$ is to compute the following $K$ values $\epsilon(z_{n1}), \dots, \epsilon(z_{nK})$ 
\begin{equation}
  \epsilon (\pmb z_n) = c_{n+1} \hat \beta(\pmb z_n) = \sum_{z_{n+1}} \hat \beta (\pmb z_{n+1}) p(\pmb x_{n+1} \mid \pmb z_{n+1}) p(\pmb z+1 \mid \pmb z_n)
\end{equation}
- Using the $c_{n+1}$ computed during forward recursion compute and store $\hat \beta (z_{nk}) = \epsilon (z_{nk}) / c_{n+1}$ 

- Using the scaled versions $\hat \alpha$ and $\hat \beta$ the following holds
\begin{equation}
  p(\pmb z_n \mid \pmb x_1, \dots, \pmb x_N) = \hat \alpha (\pmb z_n) \hat \beta (\pmb z_n)
\end{equation}

- Then the posterior decoding $\pmb z_n^*$ for a given $n$ becomes
\begin{equation}
  \pmb z_n^* = \arg \max_{\pmb z_n} \hat \alpha (\pmb z_n)  \hat \beta (\pmb z_n)
\end{equation}

*** The Viterbi decoding
- *Viterbi decoding*: $Z^*$ is the overall most likely explanation of $\pmb X$ :
\begin{equation}
  \pmb Z^* = \arg\max_{\pmb{z}} p(\pmb X, \pmb Z \mid \pmb \Theta)
\end{equation}

- The following is true for $p(\pmb X, \pmb Z^*)$ 
\begin{equation}
	p(\pmb X, \pmb Z^*) = \max_{\pmb z_N} \omega (\pmb z_N)
\end{equation}
- where $\omega (\pmb z_n) \equiv \max_{z_1, \dots, \pmb z_{n-1}} p(\pmb x_1, \dots, \pmb x_n, \pmb z_1 \dots, \pmb z_n)$ is the probabilitry of the most likely sequence of state $\pmb z_1, \dots, \pmb z_n$ ending in $\pmb z_n$ generating the observations $\pmb x_1, \dots, \pmb x_n$

- $\omega$ can be computed recursivly using the following which takes time $O(K^2N)$ and space $O(KN)$ using memorization
	- A table where $\omega[k][n] = \omega(\pmb z_n)$ if $\pmb z_n$ is state $k$
	- Basis: $\omega (\pmb z_1) = p(\pmb x_1, \pmb z_1) = p(\pmb z_1) p(\pmb x_1 \mid \pmb z_1)$
	- Recursion step: $\omega (\pmb z_n) = p(\pmb x_n \mid \pmb z_n) \max_{\pmb z_{z-1}} \omega (\pmb z_{n-1}) p(\pmb z_n \mid \pmb z_{n-1})$

- To find $\pmb Z^*$ from the $\omega$ table one can just do backtrack and find out which probability with the equation
	- It takes $O(KN)$ and space $O(KN)$ using $\omega$

*** Extension of the hidden Markov model
- The basic hidden Markov model, along with the standard training algorithm based on maximum likelihood, has been extended in numerous ways to meet the requirements of particular applications

- If the goal is sequence classification, there can be significant benefit in determining the parameters of hidden Markov models using discriminative rather than maximum likelihood techniques.
  - Suppose we have a training set of $R$ observation sequences $X_r$, where $r=1,\dots, R$ each of which is labelled according to its class $m$, where $m=1,\dots, M$
  - For each class, we have a separate hidden Markov model with its own parameters $\pmb \theta_m$
  - The problem of determining the parameter values is done as a standard classification problem where the cross-entropy is optimized
\begin{equation}
\sum_{r=1}^R \ln p(m_r \mid \pmb X_r)
\end{equation}
- Using Bayes’ theorem this can be expressed in terms of the sequence probabilities associated with the hidden Markov models
[[file:Sequential Data/screenshot_2018-11-11_20-10-55.png]]

- where $p(m)$ is the prior probability of class $m$
  - Optimization of this cost function is more complex than for maximum likelihood
  - It requires that every training sequence be evaluated under each of the models in order to compute the denominator

- A significant weakness of the hidden Markov model is the way in which it represents the distribution of times for which the system remains in a given state
  - The problem is that the probability that the Markov model will spend precisely $T$ steps in state $k$ and then make a transition is a exponentially decaying function of $T$
  - For many applications, this will be a very unrealistic model of state duration
  - The problem can be resolved by
    - Modelling state duration directly in which the diagonal coefficients $A_{kk}$ are all set to zero
    - Each state $k$ is explicitly associated with a probability distribution $p(T \mid k)$ of possible duration times.

- A limitation of the standard HMM is that it is poor at capturing longrange correlations between the observed variables
  - i.e. variables that are separated by many time steps
  - One way to address this is to generalize the HMM to give the autoregressive hidden Markov model
    - For discrete observations it corresponds to expanded tables of conditional probabilities for the emission distributions

* Johnson-Lindenstrauss Dimensionality Reduction
** Intro
- The goal of *dimensionality reduction* is to represent a high dimensional data set in a lower dimensional space while preserving much of the important structure

- *Principal Component Analysis* is a well known dimensionality reduction technique that finds (a few) directions of high variance in the data and projects the data onto these vectors of maximal variance
	- The idea is to maintain as much variance in the data as possible (the structure) while reducing the dimensionality
	- The mapping of data from a high dimensional space to a low dimensional space is called an *embedding*
		- i.e. we embed the higher dimensional data points in the lower dimensional space
	- The dimensionality reduction technique described approximately presevers all pairwise distances between the data points.

- A fundamental result of Johnson and Lindenstrauss says that any $m$ point subset of Euclidean space can be linearly embedded in $k=O(\lg m/\epsilon^2)$ dimensions without distorting the distances between any pair of points by more than a factor of $1 \pm \epsilon$ for any $0 < \epsilon < \frac12$
	- For Principal Component Analysis to be relevant the original data points $x_1, \dots, x_m$ must be inherently low dimensional 
	- The *Johnson-Lindenstraus theorem* requires no assumption on the original data and the target dimension is even independent of the input dimension
	- Another fundamental result og Larsen and Nelson, shows that that the Johnson-Lindenstrauss Lemma is tight even for non-linear embedding!
	- Besides the obvious application of using dimensionality for compression, the Johnson–Lindenstrauss theorem has found numerous applications in algorithms and machine learning for instance for Approximate
k-Means, Linear Regression, and Approximate Nearest Neighbors.

** Simple JL Lemma
- *Theorem 1* For any $0 < \epsilon < \frac12$ and any integer $m$, then for integer 
\begin{equation}
  k = O (\frac1{\epsilon^2} \lg m)
\end{equation}
- large enough and any points $x_1, \dots, x_m \subset \mathbb R^d$ there exists a linear map (matrix) $L: \mathbb R^d \to \mathbb R^k$ such that for any $1 \leq i, j \leq m$ 
\begin{equation}
	(1- \epsilon) || x_i - x_j ||_2^2 ||L x_i - Lx_j||^2_2 \leq (1+\epsilon) ||x_i - x_j||^2_2
\end{equation}

- The linear transformation $L$ in Theorem 1 is simply multiplication by a matrix whose entries are sampled independently from a standard Gaussian
	- To be precise define a random variable $A$ as a $k \times d$ matrix where each entry $A_{i,j} \sim \mathcal N(0,1)$
	- The final embedding matrix is sample of the random variable $L= \frac1{\sqrt k} A$ 

- *Lemma 1.* Fix any vector unit vector $x$. For any $0 < \epsilon, \delta < \frac12$. For $k = O(\epsilon ^{-2}\log \frac1\delta)$ large enough let $L = \frac1{\sqrt k}$ be a random variable where $A$ is a $k \times d$ random matrix whose entries are independent zero mean Gaussians ($\sim \mathcal N(0,1)$) Then:
\begin{equation}
	\Pr_L(|||Lx||^2 - 1| > \epsilon \leq delta
\end{equation}
- Stated differently, for any unit vector $x$ and values of $0 < \epsilon, \delta < \frac12$. If we pick a random matrix as described, then with probability at least $1- \delta$ the norm of $x$ is distorted by a vector at most $(1 \pm \epsilon)$ by $L$
	- This generalizes to any non-unit vector since any vector $v$ can be written as scribed, then with probability at least 1 − δ the norm of x is distorted by a factor at most (1 ± ε) by L.
	- Note that this generalizes to any non-unit vector since a vector $v$ can be written as $v = ||v|| \frac{v}{||v||}$
		- Since embedding is linear

- Result about the Normal Distribution and the $\chi^2$  
[[file:Johnson-Lindenstrauss Dimensionality Reduction/screenshot_2018-11-18_13-16-48.png]]
[[file:Johnson-Lindenstrauss Dimensionality Reduction/screenshot_2018-11-18_13-17-41.png]]

* Principal Component Analysis
** Intuition 
*** Problem statement 
- The dimensionality reduction is done linearly
	- The original data points should be possible to construct from the optimal subspace as well as possible
	- The mean squared distance between original and reconstructed data points is the *reconstruction error*
	- It is useful and common practice to remove the mean value from the data before doing the dimensionality reduction
	- Zero mean data is assumed throughout
	- Variance and 2nd moments are the same 

*** Projection and reconstruction error 
[[file:Principal Component Analysis/screenshot_2018-11-20_20-03-21.png]]

- The task of principal component analysis (PCA) is to reduce the dimensionality of some high-dimensional data points
	- This is done by linearly projecting them onto a lower dimensional space in a way that makes the reconstruction error minimal

*** Reconstruction error and variance
- Minimizing the construction error is equivalent to maximizing the variance of the projected data

*** Covariance matrix
- The data points are written as $\pmb x = (x_1, x_2)^T$
	- The variance of the first and second component can be written as $C_{11} := \langle x_1x_1 \rangle$ and $C_{22} := \langle x_2x_2 \rangle$
		- The angle brackets indicate averaging over all data points 
	- If $C_{11}$ is large compared to $C_{22}$ the direction of maximal variance is close to $(1,0)^T$
	- If $C_{11}$ is small the direction of maximal variance is close to $(0,1)^T$
	- If $C_{11}$ and $C_{22}$ are similar the covariance between the two components $C_{12} := \langle x_1x_1 \rangle$ can give additional information
		- A large positive value indicate a strong correlation and then $(1,1)^T$ should be used
		- A negative value would indicate anti-correlation and then $(-1,1)^T$ should be used
		- A small value would indicate no correlation, i.e. no prominent direction of maximal variance
	- The variances and covariances are arranged in a matrix with components $C_{ij} := \langle x_ix_j \rangle$
		- It is called *covariance matrix*, assuming zero mean data
		- The components obey the relation: $C_{ij} \leq C_{ii} C_{jj}$
		- Scaling the data by a factor $\alpha$ scales the matrix by a factor $\alpha^2$ 

*** Covariance matrix and higher order structure
[[file:Principal Component Analysis/screenshot_2018-11-20_20-38-39.png]]
- The covariance matrix only gives information about the general extent of the data
	- It does not give any information about the higher order structure of the data cloud

*** PCA by diagonalizing the covariance matrix
- If the covariance matrix is diagonal the direction of maximal variance is the axis belonging to the largest value of the covariance matrix
	- We can make a non-diagonal covariance matrix diagonal by rotating the coordinate system accordingly
	- Diagonalizing a matrix can be done by solving the corresponding eigenvalue equation
	- The eigenvectors of the covariance matrix point into the directions of maximal and minimal variance
	- The eigenvalue are equal to the variances along these direction
	- Projecting the data onto the eigenvectors with largest eigenvalues is therefore the optimal linear dimensionality reduction. 

** Formalism
*** Definition of the PCA-optimization problem
- The problem of principal component analysis (PCA) can be stated as follows:
[[file:Principal Component Analysis/screenshot_2018-11-20_20-50-02.png]]
- Remarks about the problem
	1. $\langle \pmb x^\mu \rangle_\mu$ indicate the mean over all $M$ data points indexed with $\mu$
	2. If one has non-zero-mean data, one typically removes the mean before applying PCA
	3. Matrix $\pmb U$ corresponds to a rotation of the data $\pmb x$
		 - The shape of the data cloud remains the same
		 - The perspective changes
	4. Projecting the data $\pmb x'$ onto the $P$ dimensional linear subspace is done by setting all components higher than $P$ to zero
		 - This can be done, because we have an orthonormal coordinate system
		 - If this was not the case the projection would become more mathematically complex
	5. The reconstruction error has to be minimal for any $P$

*** Matrix $V^T$: Mapping from high-dimensional old coordinate system to low-dimensional new coordinate system 
- Assume some data point $\pmb x$ are given in a $I$ dimensional space and a linear subspace is spanned by $P$ orthonormal vectors
[[file:Principal Component Analysis/screenshot_2018-11-20_21-09-02.png]]
- It is assumed that $P < I$ and spear of a high($I$) dimensional space and a low($P$) dimensional (sub)space
- Arraigning the vectors into a matrix yields
[[file:Principal Component Analysis/screenshot_2018-11-20_21-11-44.png]] 
- This matrix can be used to map the data points $\pmb x$ into the subspace spanned by the vectors $\pbm v_p$ yielding
\begin{equation}
  \pmb y := \pmb V^T \pmb x
\end{equation}
- To things are done here
	- The points are moved from the high-dimensional space onto the low-dimensional subspace
	- The points are represented in a new coordinate system
		- It is particular suitable for the low-dimensional subspace
		- The points in the high dimensional space cannot be represented accurately because we does not have enough dimensions 

*** Matrix $V$: Mapping from low-dimensional new coordinate system to subspace in old coordinate system 
- Since the vectors $\pmb v_p$ are orthonormal, matrix $V$ can also be used to transform the points back from the new to the old coordinate system
	- The lost dimensions cannot be recovered
	- The mapped $\pmb y$ in the new coordinate system become points $\pmb x_{||}$ in the old coordinate system and are given by 
\begin{equation}
  \pmb x_{||} := \pmb V \pmb y = \pmb V \pmb V^T \pmb x
\end{equation}
- $\pmb y$ and $\pmb x_{||}$ are equivalent representations
	- The contain the same information just in different coordinate systems 

*** Matrix $(\pmb V^T \pmb V)$: Identity mapping within new coordinate system 
- $\pmb V^T \pmb V$ is a $P \times P$ matrix and performs a transformation from the new coordinate system to and back again
	- Since the all the points in the old coordinate system come from the new coordinate system the mapping does not discard any information
	- $\pmb V^T \pmb V$ is the identity matrix 

*** Matrix ($VV^T$): Projection from high- to low-dimensional (sub)space within old coordinate system
- The matrix $VV^T$ maps the points $\pmb x$ onto the low-dimensional subspace
	- The mapped points are represented within the old coordinate system
	- This operation does not make a difference whether you apply more than once
	- $\pmb P = \pmb V \pmb V^T$
	- The smaller $P$ there more information is lost
		- The more does $\pmb P$ differ from the identity matrix 

*** Variance 
- The variance of a multi-dimensional data set is defined to be the sum over the variance of its components
[[file:Principal Component Analysis/screenshot_2018-11-20_21-34-46.png]]
- This also holds for the projected data $\text{var}(\pmb y) = \langle \pmb y^T \pmb y \rangle$ 

*** Reconstruction error 
- The reconstruction error $E$ is defined as the mean square sum over the distances between the orginal data points $\pmb x$ and the projected ones $\pmb x_{||}$
	- If we define the orthogonal vectors
\begin{equation}
  \pmb x_\bot = \pmb x - \pmb x_{||}
\end{equation}
- The following result can be found 
\begin{equation}
  E = \langle \pmb x^T \pmb x \rangle - \langle y^T y \rangle 
\end{equation}
- The reconstruction error equals the difference between the variance of the data minus the variance of the projected data.

*** Covariance matrix 
- The covariance matrix can be written as follows in vector notation
[[file:Principal Component Analysis/screenshot_2018-11-20_21-40-24.png]]

*** Eigenvalue equation of the covariance matrix 
- The eigenvalues for the covariance matrix are real and a set of orthogonal eigenvectors always exists
	- For a given covariance matrix $\pmb C_x$ we can always find a complete set of real eigenvalues $\lambda_i$ and corresponding eigenvectors $\pmb u_i$ such that 
[[file:Principal Component Analysis/screenshot_2018-11-20_21-47-41.png]]

- If the eigenvectors are combined into an orthogonal matrix $\pmb U$ and the eigenvalues into a diagonal matrix $\pmb \Lambda$ 
[[file:Principal Component Analysis/screenshot_2018-11-20_21-48-40.png]]

- The following holds
[[file:Principal Component Analysis/screenshot_2018-11-20_21-48-59.png]]

*** Total variance of data $\pmb x$ 
- Given a eigenvector matrix $\pmb U$ and the eigenvalue matrix $\pmb \Lambda$ it is easy to compute the total variance of the data
\begin{equation}
  \langle \pmb x^T \pmb x \rangle = \sum_i \lambda_i 
\end{equation}
- The total variance of the data is the sum of the eigenvalues of its covariance matrix

*** Diagonalizing the covariance matrix
- The matrix $\pmb U$ can be used to transform the data such that the covariance matrix becomes diagonal 
	- Define $X' := \pmb U^T \pmb x$ and denote the new covariance matrix by $\pmb C_x^'$
	- The we have $\pmb C_x^{'} := \pmb \Lambda$

*** Constraints of matrix $V^'$  
- Since the vectors $v_p^'$ are orthonormal $\pmb V'$ can always be completed to an orthogonal $I \times I$ matrix by adding $I-P$ additional orthonormal vectors
- The following holds
[[file:Principal Component Analysis/screenshot_2018-11-20_22-17-24.png]]

*** Finding the optimal subspace
- The following holds 
[[file:Principal Component Analysis/screenshot_2018-11-20_22-18-36.png]]

*** Interpretation of the result 
- $\pmb V ^'$ projects the data $\pmb x'$ onto the first $P$ axes
	- It is a projection onto the first $P$ eigenvectors of the covariance matrix $\pmb C_x$
	- The following is defined
[[file:Principal Component Analysis/screenshot_2018-11-20_22-25-48.png]]
- We would set $\pmb v_p := \pmb u_p$
- The variance of $y$ is $\sum_{i=1}^P \lambda_i$
- The reconstruction error is $E = \sum_{i=P+1}^I \lambda_i$ 

*** Whitening or sphering 
- It is sometimes desirable to transform a data set such that it has variance one in all directions
	- It is called *whitening* or *sphering*
	- Sphering requires to stretch and compress the data distribution along the axes of the principal components such they have variance one
	- One first rotates the data into a coordinate system where the covariance is diagonal, then performs stretching along the aces and then rotates the data back into the original coordinate system
	- The eigenvectors of the covariance matrix provide the aces of the new coordinate system
		- The eigenvalues indicate the variances and therefore how much one has to stretch the data
		- Sphering is achieved by multiplying the data with a sphering matrix
[[file:Principal Component Analysis/screenshot_2018-11-20_22-50-23.png]]
- If the final orientation does not matte this matrix is often defined without its first $\pmb U$
	- The sphering matrix is symmetrical 

*** Singular value decomposition 
- Sometimes one has fewer data points than dimensions
	- Then doing direct PCA is very inefficient and *singular value decomposition* (SVD) is the very helpful 

- Let $\pmb x^\mu$, $\mu=1,\dots, M$ be the $I$ dimensional data with $M < I$
	- All the data can be written in the one $I \times M$ matrix $\pmb X := (\pmb x^1, \dots, \pmb x^M)$
	- The second-moment matrix can then be written as 
\begin{equation}
  \pmb C_1 := \pmb X \pmb X^T /M
\end{equation}
- It eigenvalue equation and decomposition read 
[[file:Principal Component Analysis/screenshot_2018-11-20_22-56-59.png]]
- The data represented in the coordinate system of the eigenvectors is 
\begin{equation}
  \pmb Y_1 := \pmb U_1^T \pmb X 
\end{equation}

* Representative-based Clustering
** General 
- Given a dataset with $n$ points in a $d$ dimensional space, $\mathbf D = \{ \pmb x_1 \}_{i=1}^n$ and given the number of desired clusters $k$, the goal of *representative-based clustering* is to divide the dataset into $k$ groups or clusters
	- This is called a *clustering*
	- It is denoted as $\mathcal C = \{C_1, C_2, \dots, C_k\}$
	- For each cluster $C_i$ there exists a representative point that summarizes the cluster
		- A common choice being the mean (centroid) $\pmb \mu_i$ of all points in the clusters i.e. 
\begin{equation}
  \pmb \mu_i = \frac1{n_i} \sum _{x_j \in C_i} \pmb x_j
\end{equation}
- where $n_i = |C_i|$

- A brute-force algorithm for finding a good clustering is simply to generate all possible partitions of $n$ points into $k$ clusters
	- A optimization score is evaluated for each of them an the clustering with the best score is used
	- The number of ways of partitioning $n$ points into $k$ nonempty and disjoint parts is given by the Stirling number of the second kind given as 
\begin{equation}
    S(n,k) = \frac1{k!} \sum_{t=1}^k (-1)^t \binom{k}{t} (k-t)^n
\end{equation}

** K-Means Algorithm 
[[file:Representative-based Clustering/screenshot_2018-11-25_09-22-03.png]]

- Given a clustering $\mathcal C = \{C_1, C_2, \dots, C_k\}$ a the *sum of squared error* scoring function is defined as
\begin{equation}
	SSE(\mathcal C) = \sum_{i=1}^k \sum_{\pmb x_j \in C_i} ||\pmb x_j - \pmb \mu_i ||^2
\end{equation}
- The goal is to finding the clustering that minimizes the SSE score 
\begin{equation}
  C^* = \arg \min_{\mathcal C} \{SSE(\mathcal C)\}
\end{equation}

- K-means employs a greedy iterative approach to find a clustering that minimizes the SSE objective
	- It can converge to a local optima instead of a globally optimal clustering
	- K-means initializes the clusters by randomly generating $k$ points in the data space
		- Typically done by generating a value uniformly at random within the range for each dimension
	- Each iteration of K-means consists of two steps
		1. Cluster assignment: Each point $\pmb x_j \in \mathbf D$ is assigned to the closest mean
			 - This induces a clustering with each cluster $C_i$ compromising points that are closer to $\pmb \mu_i$
			 - Each point $\pmb x_j$ is assigned to cluster $C_{j^*}$ where $j^*=\arg\min_{i=1}^k \{||\pmb x_j - \pmb mu_i ||^2\}$
		2. Centroid update: Given a set of clusters $C_i, i=1, \dots, k$ new mean values are computed for each cluster from the points in $C_i$
	- The two steps are carried out iteratively until we reach a fixed point or local minima
		- One can assume that K-means has converged if the centroids do not change from one iteration to the next
	- Since the method starts with a random guess it is typically run several times
		- The run with the lowest SSE is chosen to report the final clustering
	- It generates convex shaped clusters
	- The total time for K-means is given as $O(tnkd)$
		- Where $t$ is the number of iteration
		- $d$ is the number of dimensions 

** Expectation-Maximization Clustering 
*** General
- The K-means approach is an example of a *hard assignment clustering*
	- Each point can belong to only one cluster

- *Soft assignment cluster* is where each point has a probability of belonging to each cluster 

- Let $\mathbf D$ consist of $n$ points $\pmb x_j$ in a d-dimensional space $\mathbb R^d$
	- Let $X_a$ denote the random variable corresponding to the $a$'th attribute
	- $X_a$ is also used to denote the $a$'th column vector corresponding to the $n$ data sample from $X_a$
	- Let $\mathbf X = (X_1, X_2, \dots, X_d)$ denote the vector random variable across the $d$ attributes, with $\pmb x_j$ being a data sample from $\mathbf X$

- EM does not always convex 

*** Gaussian Mixture Model 
- We assume that each cluster $C_i$ is characterized by the multivariate normal distribution i.e. 
[[file:Representative-based Clustering/screenshot_2018-11-27_19-30-49.png]]
- where the clusters mean $\pmb \mu_i \in \mathbb R^d$ and the covariance matrix $\mathbf \Sigma \in \mathbb R^{d \times d}$ are both unknown parameters
	- $f_i(\pmb x)$ is the probability density at $\pmb x$ attributable to cluster $C_i$
	- It is assumed that the probability density function of $\mathbf X$ is given as a Gaussian mixture model over all the $k$ cluster normals, defined as 
[[file:Representative-based Clustering/screenshot_2018-11-27_19-34-13.png]]
- Here the prior probabilities $P(C_i)$ are called the *mixture parameters* and must satisfy the condition 
\begin{equation}
  \sum_{i=1}^kP(C_i) = 1
\end{equation}

- The Gaussian mixture model is characterized by
	- The mean $\pmb \mu_i$
	- The covariance matrix $\mathbf \Sigma_i$
	- The mixture probability $P(C_i)$ for each of the $k$ normal distributions
	- The set of all model parameters are written compactly as 
[[file:Representative-based Clustering/screenshot_2018-11-27_19-37-51.png]]

*** Maximum Likelihood Estrimation 
- Given a data set $\mathbf D$ the likelihood of $\theta$ is defined as the conditional probability of the data $\mathbf D$ given the model parameters $\theta$
	- It is denoted as $P(\mathbf D \mid \mathbf \theta)$
	- Since each of the $n$ points $\pmb x_j$ is considered to be random sample from $\mathbf X$ the likelihood of $\mathbf \theta$ is given as 
\begin{equation}
  P(\mathbf D \mid \mathbf \theta) = \prod_{j=1}^n f(\pmb x_j)
\end{equation}
- The goal of MLE is to maximize the parameters $\mathbf \theta$
	- It typically done on the log-likelihood function
	- The log-likelihood function is given as 
[[file:Representative-based Clustering/screenshot_2018-11-27_19-55-44.png]]
- Since maximizing the log-likelihood over $\mathbf \theta$ directly is hard expectation maximization (EM) is used instead
	- It is an approach for finding maximum likelihood estimates for the parameters $\mathbf \theta$
	- It is a two step iterative approach that starts for an initial guess for the parameters $\pmb \theta$
	- Given the current estimates for $\mathbf \theta$ the expectation step EM computes the cluster posterior probabilities $P(C_i \mid \pmb x_j)$ via the Bayers theorem 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-03-11.png]]
- Since the cluster is modeled as a multivariate normal distribution the probability of $\pmb x_j$ given cluster $C_i$ can be obtained by considering a small interval $\epsilon > 0$  centered at $\pmb x_j$ as follows 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-05-54.png]]
The posterior probability of $C_i$ is thus given as 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-07-19.png]]
- and $P(C_i \mid \pmb x_j)$ can be considered as weight or contribution of the point $\pmb x_j$ to cluster $C_i$

- In the maximization step using the weight $P(C_i \mid \pmb x_j)$ EM re-estimates $\pmb \theta$
	- i.e. it re-estimates the parameters $\pmb \mu_i$, $\mathbf \Sigma_i$ and $P(C_i)$ for each cluster $C_i$
	- The re-estimated means is given as the weighted average of all the points
	- The re-estimated covariance matrix is given as the weighted covariance over all pairs of dimensions
	- The re-estimated prior probability for each is given as the fraction of the weights that contribute to that cluster 

*** EM in one Dimension
**** General
- A dataset $\mathbf D$ is considered which consists of a single attribute $X$
	- Each point $x_j \in \mathbb R$ for $j=1,\dots, n$ is a random sample from $X$
	- For the mixture model univariate normals it used for each cluster 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-20-22.png]]
- with the cluster parameters $\mu_i$, $\sigma_i^2$ and $P(C_i)$

- The EM approach consists of three steps initialization, expectation and maximization

**** Initialization
- For each cluster $C_i$ with $i=1,2,\dots,k$ we randomly initialize the cluster parameters $\mu_i$, $\sigma_i^2$ and $P(C_i)$
- The mean $\mu_i$ is selected uniformly at random from the range of possible values for $X$
- It is typical to assume that the initial variance is give as $\sigma_i^2$
- The cluster probabilities are initialized to $P(C_i) = \frac1k$ 

**** Expectation Step
- It is assumed that for each of the $k$ clusters that we have an estimate for the parameters
	- i.e. $\mu_i$, $\sigma_i^2$ and $P(C_i)$
- Given the estimated values the posterior probabilities are computed 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-26-56.png]]
- The notation $w_{ij} = P(C_i \mid x_k)$ is used
- Let 
\begin{equation}
  \pmb w_i = (w_{i1}, \dots, w_{in})^T
\end{equation}
- denote the weight vector for cluster $i$ across all the $n$ points 

**** Maximization Step
- It is assumed that all posterior probability values or weights $w_{ij} = P(C_i \mid x_k)$ are known
- It computes the ML estimates for the cluster parameters
- The re-estimated value for the cluster mean $\mu_i$ is computed as the weighted mean of all the points 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-34-42.png]]
- In terms of the weight vector $\pmb w_i$ and the attribute vector $X = (x_1, x_2, \dots, x_n)^T$ it can be written as 

[[file:Representative-based Clustering/screenshot_2018-11-27_20-35-45.png]]

- The re-estimated value of the cluster variance is computed as the weighted variance across all the points
[[file:Representative-based Clustering/screenshot_2018-11-27_20-38-34.png]]

- If we let $Z_i = X - \mu_i \mathbf 1=(x_1 - \mu_i, x_2 - \mu_i, \dots, x_n - \mu_i)^T = (z_{i1}, z_{i2}, \dots, z_{in})^T$ be the centered attribute vector for cluster $C_i$
	- Let $Z_i^s$ be the squared vector given as $Z_i^s=(z_{i1}^2, z_{i2}^2, \dots, z_{in}^2)^T$
	- Then the variances can be expressed as 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-42-51.png]]

- The prior probability of cluster $C_i$ is re-estimated as the fraction of total weight belonging to $C_i$ computed as 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-47-43.png]]
- This can be written in vector notation as 
\begin{equation}
  P(C_i) = \frac{\pmb w_i^T \pmb 1}n
\end{equation}

**** Iteration
- Starting for an initial set of values for the cluster parameters the EM algorithm applies the expectation step to compute the weights $w_{ij} = P(C_i \mid x_k)$
- The weights are used in the maximization step to compute the updated cluster parameters
- The expectation and maximization steps are iteratively applied until convergence
	- e.g. until there is very little change in the means 

*** EM in $d$ Dimensions
**** General
- The EM methods in now considered in $d$ dimensions
	- Each cluster is characterized by a multivariate normal distribution with parameters $\pmb \mu_i$, $\mathbf \Sigma$ and $P(C_i)$
	- For each cluster $C_i$ a $d$ dimension mean vector should be estimated 
\begin{equation}
  \pmb \mu_i = (\mu_{i1}, \mu_{i2}, \dots, \mu_{id})^T
\end{equation}
- and the $d \times d$ covariance matrix 
[[file:Representative-based Clustering/screenshot_2018-11-27_20-57-21.png]]
- Since the covariance matrix is symmetric, $\binom{d}2 = \frac{d(d-1)}2$ pairwise covariances and $d$ variances for a total of $\frac{d(d+1)}2$ parameters for $\mathbf \Sigma_i$
	- It may be too many parameters to estimate them reliably
	- A simplification is to assume that all dimensions are independent, which lead to a diagonal covariance matrix
[[file:Representative-based Clustering/screenshot_2018-11-27_21-00-50.png]]
- Under this assumption $d$ parameters only needs to be estimated for $\mathbf \Sigma_i$

**** Initialization
- For each cluster $C_i$ with $i=1,2,\dots,k$ we randomly initialize the mean $\mu_i$ by selecting a value $\mu_{ia}$ for each dimension $X_a$ uniformly at random from the range $X_a$
- The covariance matrix is initialized as the $d \times d$ identity matrix
- The cluster prior probabilities are initialized to $P(C_i) = \frac1k$ 

**** Expectation step 
- The posterior probability of cluster $C_i$ given point $\pmb x_j$ is computed using the formula
[[file:Representative-based Clustering/screenshot_2018-11-27_21-08-24.png]]
- with $i=1,\dots,k$ and $j=1,\dots,n$
	- The shorthand $w_{ij} = P(C_i \mid x_k)$ is used
	- The notation $\pmb w_i = (w_{i1}, \dots, w_{in})^T$ is used 

**** Maximization Step
- Given the weights the parameters $\pmb \mu_i$, $\mathbf \Sigma$ and $P(C_i)$ are re-estimated

- The mean $\mu_i$ for cluster $C_i$ can be estimated as 
[[file:Representative-based Clustering/screenshot_2018-11-27_21-15-09.png]]
- This can be expressed in matrix form as 
[[file:Representative-based Clustering/screenshot_2018-11-27_21-15-37.png]]

- Let $\mathbf Z_i = \mathbf D - \mathbf 1 \cdot \pmg \mu_i ^T$ be the centered data matrix for the cluster $C_i$
	- Let $z_{ji} = \pmb x_j - \pmb \mu_i \in \mathbb R^d$ denote the $j$'th centered point in $\mathbf Z_i$
	- $\mathbf \Sigma_i$ can be described compactly using the following equation 
[[file:Representative-based Clustering/screenshot_2018-11-27_21-18-43.png]]

- The covariance between dimension $X_a$ and $X_b$ is estimated as 
[[file:Representative-based Clustering/screenshot_2018-11-27_21-19-39.png]]

- The prior probability $P(C_i)$ for each cluster is the same as in the one-dimensional case given as 
[[file:Representative-based Clustering/screenshot_2018-11-27_21-21-47.png]]

**** EM Clustering Algorithm  
[[file:Representative-based Clustering/screenshot_2018-11-27_21-23-02.png]]
- The total time for the EM clustering algorithm is $O(t(kd^3 + nkd^2))$  
- The total time for the EM clustering algorithm with a diagonal covariance matrix is $O(tnkd)$
 
* Density-based Clustering
** The DBSCAN Algorithm
- Density-based clustering uses the local density of points to determine the clusters
	- A ball of radius $\epsilon$ is defined around a point $\pmb x \in \mathbb R^d$ called the $\epsilon$ neighborhood of $\pmb x$ as follows
\begin{equation}
  N_\epsilon (\pmb x) = B_d(\pmb x, \epsilon) = \{ \pmb y \mid \delta(\pmb x, \pmb y) \leq \epsilon \}
\end{equation}
- $\delta(\pmb x, \pmb y)$ represents the distance between points $\pmb x$ and $\pmb y$
	- It is usually assumed to be the Euclidean distance that it $\delta(\pmb x, \pmb y) = || \pmb x - \pmb y||_ 2$ 

- For any point $\pmb x \in \mathbf D$ we say that $\pmb x$ is a core point if there are at least $minpts$ point in its $\epsilon$ neighborhood
	- i.e. $\pmb x$ is a core point if $|N_\epsilon(\pmb x) | \geq minpts$ where $minpts$ is a user defined local density
	- A border point is defined as a point that does not meet the $minpts$ threshold but belongs to the $\epsilon$ neighborhood of some core point $\pmb z$
		- i.e. $|N_\epsilon(\pmb x) | < minpts$ and $x \in N_\epsilon(\pmb z)$
	- If a point is neither a core not a border point, then ti is called a noise point or an outlier
	- We say that a point $\pmb x$ is *directly density reachable* from another point $\pmb y$ if $\pmb x \in N_\epsilon(\pmb y)$ and $\pmb y$ is a core point
		- We say that $\pmb x$ is *density reachable* if their exists a chain of point $\pmb x_0, \pmb x_1, \dots, \pmb x_l$ such that $\pmb x = \pmb x_0$ and $\pmb y = \pmb x_i$ and $\pmb x_i$ is directly density reachable from $\pbm x_{i-1}$ for all $i=1, \dots,l$
		- Density reachability is an asymmetric relationship
		- Two points $\pmb x$ and $\pmb y$ is density connected if there exists a core point $\pmb z$ such that both $\pmb x$ and $\pmb y$ are density reachable from $\pmb z$
		- A *density-based cluster* is defined as the maximal set of density connected points 

[[file:The Dbscan Algorithm/screenshot_2018-11-27_21-50-17.png]]
- DBSCAN computes the $\epsilon$ neighborhood $N_\epsilon(\pmb x_i)$ for each point $\pmb x_i$ in the data set $\mathbf D$ and checks if it is a core point
	- It sets the cluster id $id(\pmb x_i) = \emptyset$ for all points
		- Indicates that they have not been assigned to any cluster
	- Starting from each unassigned core point, the method recursively finds all its density connected points, which are assigned to the same cluster
	- Some border point may be reachable from core points in more than one cluster
	- They may be assigned to one of the cluster or all of them - if overlapping clusters are allowed 

- A limitation of DBSCAN is that is sensitive to the choice of $\epsilon$
	- If $\epsilon$ is too small, sparser clusters will be categorized as noise
	- If $\epsilon$ is too large denser clusters may be merge together
	- If there are clusters with different local densities a single $\epsilon$ value may not suffice 

- If the dimensionality is not too high the DBSCAN takes $O(n \cdot \log(n))$ time
	- Worst case it takes $O(n^2)$ time
** Kernel Density Estimation 
*** General
- There is a close connection between density-based clustering and density estimation
	- The goal of density estimation is to find an unknown probability density function by finding the dense regions of points which in turn can be used for clustering
	- It is a nonparametric technique that does not assume any fixed probability model of the clusters
	- It tries to directly infer the underlying probability density at each point in the dataset 

*** Univariate Density Estimation 
- Assume that $X$ is a continuous random variable, and let $x_1, x_2, \dots, x_n$ be a random sample drawn from the underlying probability density function $f(x)$ which is assumed to be unknown
	- The cumulative distribution function can directly estimated by couting how many points are less that or equal to $x$ 
\begin{equation}
  \hat F (x) = \frac1n \sum_{i=1}^n I(x_i \leq x) 
\end{equation}
- $I$ is the indicator function that has value 1 only when its arguments is true and 0 otherwise
	- The density function can be estimated by taking the derivative of $\hat F(x)$ by considering a windows of small width $h$ centered at $x$ i.e.
[[file:Density-based Clustering/screenshot_2018-12-01_12-14-39.png]]
- where $k$ is the number of points that lie in the windows of width $h$ centered at $x$ i.e. within the closed interval $[x- \frac h2, x+ \frac h 2]$ 
	- The density estimate is the ration of the fraction of the points in the window $k/n$ to the volume of the window $h$
	- $h$ plays the role of influence
		- A large $h$ estimates the probability density over a large window which is a smoother estimate
		- If $h$ is small then only the proximity to $x$ are considered
		- $h$ should be small but not to small 

- *Kernel density* estimation relies on a kernel function $K$ that is non-negative, symmetric and integrates to 1
	- The following should be true for the kernel function $K(x) \geq 0$, $K(-x) = K(x)$ for all values $x$ and $\int K(x) dx = 1$
	- $K$ is essentially a probability density function 

- *Discrete Kernel* The density estimate $\hat f(x)$ can be rewritten in terms of the kernel function as follows 
[[file:Density-based Clustering/screenshot_2018-12-01_12-23-48.png]]
- where the *discrete kernel* function $k$ computes the number of points in a window of width $h$ defined as 
\begin{equation}
  	K(z) =
  		\begin{cases}
  			\mbox{$1$} & \mbox{If $|z| \leq \frac12$} \\
  			\mbox{$0$} & \mbox{otherwise} \\
  		\end{cases}
\end{equation}
- If $|z| = |\frac{x-x_i}h| \leq \frac12$ then the point is within a window of width $h$ centered at $x$ 

- *Gaussian Kernel* is a more smooth kernel than the discrete kernel where there is a more smooth transition of influence
[[file:Density-based Clustering/screenshot_2018-12-01_12-33-28.png]]
- Thus one have
[[file:Density-based Clustering/screenshot_2018-12-01_12-33-17.png]]
- $x$ plays the role of the mean and $h$ acts as the standard derivation 

*** Multivariate Density Estimation 
- To estimate the probability density at a $d$ dimensional point $\pmb x = (x_1, x_2, \dots, x_d)^T$ a $d$ dimensional window is defined
	- i.e. a hypercube centered at $\pmb x$ with edge length $h$
	- The volume of the hypercube is given as 
\begin{equation}
  \text{vol}(H_d(h)) = h^d
\end{equation}
- The density is estimated as the fraction of the point weight which lies within the $d$ dimensional window centered at $\pmb x$ divided by the volume of the hypercube
[[file:Density-based Clustering/screenshot_2018-12-01_12-43-21.png]]
- where the multivariate kernel function $K$ satisfies the condition $\int K(\pmb z) d \pmb z = 1$ 

- *Discrete Kernel* For any $d$ dimensional vector $\pmb z = (z_1, z_2, \dots z_d)^T$, the discrete kernel in a function in$d$ dimensions given as 
\begin{equation}
  	K(z) =
  		\begin{cases}
  			\mbox{$1$} & \mbox{If $|z| \leq \frac12$} \text{ for all dimensions } j=1,\dots,d\\
  			\mbox{$0$} & \mbox{otherwise} \\
  		\end{cases}
\end{equation}
- For $\pmb z = \frac{\pmb x - \pmb x_i}h$
	- Each point within the hypercube contributes a weight of $\frac1n$ to the density estimate 

- *Gaussian Kernel* The $d$ dimensional Gaussian kernel is given as 
[[file:Density-based Clustering/screenshot_2018-12-01_12-50-56.png]]
- It is assumed that the covariance matrix is the $d \times d$ identity matrix.
	- Setting $\pmb z = \frac{\pmb x- \pmb x_i}h$ we have
[[file:Density-based Clustering/screenshot_2018-12-01_12-53-05.png]]
- Each point contributes a weight to the density estimate inversely proportional to its distance from $\pmb x$ 

*** Nearest Neighbor Density Estimation 
- Another way to do density estimation is to fix a $k$ which is the number of points required to estimate the density and allow the volume of the enclosing region to vary to accommodate those $k$ points
	- This approach is called the $k$ neighbor approach to density estimation
	- It is a nonparametric approach
	- Given $k$, the number of neighbors we estimate the density at $\pmb x$ as follows
\begin{equation}
  \hat f (\pmb x) = \frac{k}{n \text{vol}(S_d(h_{\pmb x}))}
\end{equation}
- Where $h_{\pmb x}$  us the distance from $\pmb x$ to its kth nearest neighbor and $\text{vol}(S_d(h_{\pmb x}))$ is the volume of the $d$ dimensional hypersphere $S_d(h_{\pmb x})$ centered at $\pmb x$ with radius $h_{\pmb x}$

** Density-Based Clustering: DENCLUE
- A point $\pmb x^*$ is called a *density attractor* if it is a local maxima of the probability density function $f$
	- It can be found via a gradient ascent approach starting at some point $\pmb x$
	- The idea is to compute the density gradient, the direction of the largest increase in density and to most in the direction of the gradient in small steps until we reach a local maxima
	- The gradient at a point $\pmb x$ can be computed as the multivariate derivative of the probability density estimate given as 
[[file:Density-based Clustering/screenshot_2018-12-01_13-21-25.png]]

- For the Gaussian kernel we have 
[[file:Density-based Clustering/screenshot_2018-12-01_13-23-19.png]]

- We say that $\pmb x^*$ is a *density attractor* for $\pmb x$ or $\pmb x$ is density attracted to $\pmb x^*$ if the hill climing process started at $\pmb x$ converges to $\pmb x^*$
	- That is there exists a sequence of points $\pmb x= \pmb x_0 \to \pmb x_1 \to \dots \to \pmb x_m$ starting from $\pmb x$ and ending at $\pmb x_m$ such that $||\pmb x_m - \pmb x^* || \leq \epsilon$
	- The typically approach is to use the gradient-ascent method to compute $\pmb x^*$ starting from $\pmb$ by iteratively update it at each step $t$ via the update rule
\begin{equation}
  \pmb x_{t+1} = \pmb x_t + \delta \cdot \nabla \hat f (\pmb x_t)
\end{equation}
- where $\delta > 0$ is the step size 

- Instead of using gradient-ascent the direct update rule can be used
[[file:Density-based Clustering/screenshot_2018-12-01_13-30-26.png]]
- where $t$ denotes the current iteration and $\pmb x_{t+1}$ is updated value for the current vector $\pmb x_t$
	- It is much faster to converge than the hill-climbing process

- A cluster $C \subseteq \mathbf D$ is called a *center-defined cluster* if all the points $\pmb x \in C$ are density attracted to a unique density attractor $\pmb x^*$, such that $\hat f (\pmb x^*) \geq \xi$, where $\xi$ is a user defined minumum
	- In other words the following must hold 
[[file:Density-based Clustering/screenshot_2018-12-02_12-30-55.png]]

- An arbitrary-shaped cluster $C \subseteq \mathbf D$ is called a *density-based cluster* if there exists a set of attractors $\pmb x_1^*, \pmb x_2^*, \dots, \pmb x_m^*$, such that 
	1. Each points $\pmb x \in C$ is attracted to some attractor $\pmb x_i^*$
	2. Each density attractor has density above $\xi$ i.e. $\hat f (\pmb x_i^*) \geq \xi$
	3. Any two density attractors $\pmb x_i^*$ and $\pmb x_j^*$ are density reachable i.e. there exists a path from $\pmb x_i^*$ to $\pmb x_j^*$ such that for all points $\pmb y$ on the path, $\hat f(\pmb y) \geq \xi$ 

[[file:Density-based Clustering/screenshot_2018-12-02_12-37-20.png]]
- *DENCLUE Algorithm*
	- The first step is to compute the density attractor $\pmb x^*$ for each point $\pmb x$ in the dataset
		- If the density at $\pmb x^*$ is above the minimum density threshold $\xi$ the attractor is added to the set of attractors $\mathcal A$
		- The data point $\pmb x$ is also added to the set of points $R(\pmb x^*)$ attracted to $\pmb x^*$
	- In the second step DENCLUE finds all the maximal subsets of attractors $C \subseteq \mathcal A$ such that any pair of attractors in $C$ in density reachable from each other
		- These form the seed for each density-based clusters
	- Lastly for each attractor $\pmb x^* \in C$ we add the cluster all of the points $R(\pmb x^*)$ that are attracted to $\pmb x ^*$
		- This results in the final set of clusters $\mathcal C$
	- The ~FINDATTRACTOR~ method implements the hill-climbing process using the direct update rule
		- Results in fast convergence

* Clustering Validation
** General 
- Cluster validation and assessment encompasses three main tasks
	- *Clustering evaluation* seeks to assess the quality of the clustering
	- *Clustering stability* seeks to understand sensitive the clustering is to the algorithmic parameters
		- e.g. the number of cluster
	- *Clustering tendency* assess the suitability of applying clustering in the first placer
		- i.e. whether the data has any inherent grouping struictire 

- Validity measures can be divided into three main types:
	- *External:* External validation measure employ criteria the are not in the dataset
		- It can be in the from of prior or expert-specified knowledge
		- e.g. class labels for each point
	- *Internal:* Internal validation measure the employ criteria that are derived from the data itself
	- *Relative:* Relative validation compare different clusterings
		- It is usually other clusters obtained via different parameter settings for the same algorithm 

** External Measures 
*** General
- External measures assume that the correct clustering is known a priori
	- The true cluster labels play the role of external information which is used to evaluate the given clustering
	- In general one would not know the correct cluster
	- External measures can server as way to test and validate different measures

- Let $\mathbf D= \{\pmb x_i \}^n_{i=1}$ be the dataset consisting of $n$ points in a $d$ dimensional space which is partitioned into $k$ clusters
	- Let $y_i \in \{1,2,\dots,k\}$ denote the ground-truth cluster membership
	- The groud truth clustering is given as $\mathcal T= \{T_1, T_2, \dots, T_k\}$ where the cluster $T_j$ consists of all the points with label $j$
		- i.e. $T_j = \{\pmb x_i \in \mathbf D \mid y_i = j\}$
	- Let $\mathcal C = \{C_1, \dots, C_r\}$ denote a clustering of the same dataset into $r$ clusters which is obtained via some algorithm
	- Let $\hat y_i \in \{1,2,\dots,r\}$ denote the cluster label for $\pmb x_i$
	- $\mathcal T$ is refereed to as the ground-truth partitioning and each $T_i$ as a partition
	- $\mathcal C$ is called as clustering with each $C_i$ called a clustered
	- Since the ground truth is assumed to be known the algorithm is used with the right number of parameters $r=k$
		- But one would typically allow them to be different for generality 
	- External evaluation measure try to capture the extent to which points from same partition appear in the same cluters and different partition are grouped in different clusters
		- There are usually a trade of between the two goal, which is either explicitly captured by a measure or is implicit in its computation
	- All of the external measures rely on the $r \times k$ contingency table $\mathbf N$ that is induced by a clustering $\mathcal C$ and the ground-truth partitioning $\mathcal T$, defined as follows 
\begin{equation}
  \mathbf N(i,j) = n_{ij} = | C_i \cap T_j | 
\end{equation}
- The count $n_{ij}$ denotes the number of points that are common to cluster $C_i$ and group-truth partition $T_j$
	- Let $n_i = |C_i|$ denote the number of points in cluster $C_i$
	- Let $m_i = |T_j|$ denote the number of points in partition $T_j$
	- The contingency table can be computed from $\mathcal T$ and $\mathcal C$ in $O(n)$ time by examining the partition and cluster label $y_i$ and $\hat y_i$ for each point $\pmb x_i \in \mathbf D$ and incrementing the corresponding count $n_{y_i\hat y_i}

*** Matching Based Measures 
**** Purity
- *Purity* quantifies the extent to which a cluster $C_i$ contains entities from only one partition
	- How "pure" each cluster is
	- The purity of cluster $C_i$ is defined as
[[file:Clustering Validation/screenshot_2018-11-27_22-42-06.png]]

- The purity of clutering $\mathcal C$ is defined as the weighted sum of the clusterwise purity values 
[[file:Clustering Validation/screenshot_2018-11-27_22-43-11.png]]

- The maximum value of purity is $1$
	- When each clusters comprises points from only one partition
	- When $r=k$ a this indicates a perfect clustering 
	- When $r>k$ the each of the clusters is a subset of the ground-truth partitioning
	- When $r < k$ purity can never be one 

**** Maximum Matching 
- The maximum matching measure selects the mapping between clusters and partitions, such that the sum of common points $n_{ij}$ is maximized
	- This is provided that only one cluster can match with a given partitioning
	- The contingency table is treat as a complete weighted bipartite graph $G=(V,E)$
		- Each partion and cluster is a node i.e. $V= \mathcal C \cup \mathcal T$
		- There exists an edge $C_i, T_j \in E$ with weight $w(C_i, T_j)= n_{ij}$ for all $C_i \in \mathcal C$ and $T_j \in \mathcal T$
		- A matching $m$ in $G$ is a subset of $E$ such that the edges in $M$ are pairwise nonadjacent 
	- The maximum measure is defined as the maximum weight matching in $G$
[[file:Clustering Validation/screenshot_2018-11-27_22-48-16.png]]
- It can be computed in $O(k^4)$ time if $r=O(k)$ 

**** F-Measure
- Given cluster $C_i$, let $j_i$ denote the partition that contains the maximum number of points from $C_i$ i.e $j_i = \max_{j=1}^k{n_{ij}}$ 
	- The precision of a cluster $C_i$ is the same as its purity
[[file:Clustering Validation/screenshot_2018-11-27_22-53-22.png]]
- It measures the fractions of points in $C_i$ from the majority partition $T_{j_i}$
- The recall of cluster $C_i$ is defined as 
[[file:Clustering Validation/screenshot_2018-11-27_22-54-25.png]]
- It measure the fraction of point in partition $T_{j_i}$ shared in common $C_i$
- The F-measure is the harmonic mean of the precision and recall values for each cluster
- The F-measure for cluster $C_i$ is given as 
[[file:Clustering Validation/screenshot_2018-11-27_22-56-07.png]]
- The F-measure for the clustering $\mathcal C$ is the mean of the clusterwise F-measure values 
[[file:Clustering Validation/screenshot_2018-11-27_22-56-45.png]]
- F-measure tries to balance the precision and recall values across all the clusters
	- For a perfect clustering, when $r=k$ the maximum value of the F-measure is 1 

** Internal Measures 
*** General
- Internal evaluation measures not use the ground-truth partitioning
	- To evaluate the quality of the clustering internal measures utilize notions of intracluster similarity or compactness, contrasted with notions of intracluster separation
		- There are usually a trade-off in maximizing these two aims
	- The internal measures are based on the $n \times n$ distance matrix which is called the *proximity matrix*
[[file:Clustering Validation/screenshot_2018-11-27_23-03-15.png]]
- where 
[[file:Clustering Validation/screenshot_2018-11-27_23-03-35.png]]
- Since $\mathbf W$ is symmetric and $\delta(\pmb x_i, \pmb x_i) = 0$ only the upper triangular elements of $\mathbf W$ are used in calculations
	- It can also be considered as the adjacency matrix of the weighted complete graph $G$ over the $n$ point s with
		- nodes $V=\{\pmb x_i \mid \pmb x_i \in \mathbf D\}$
		- edges $E = \{(x_i,x_j) \mid x_i,x_j \in \mathbf D\}$
		- edge weights $w_{ij} = \mathbf W(i,j)$ for all $\pmb x_i, \pmb x_j \in \mathbf D$

- For internal measures it is assumed that there is not access to a ground-truth partioning
	- It is assumed that we are given a clustering $\mathcal C = C_1, \dots, C_k$ of $r=k$ clusters
	- Cluster $C_i$ contains $n_i = |C_i|$ points
	- Let $\hat y_i \in \{1,2,\dots, k\}$ denote the cluster label for point $\pmb x_i$
	- The clustering $\mathcal C$ can be considered as a k-way cut in $G$ since $C_i \ne \emptyset$ for all $i$, $C_i \cap C_j = \emptyset$ for all $i,j$ and $\union_i C_i = V$
		- Given any subsets $S,R \subset V$ define $W(S,R)$ as the sum of the weights on all the edges with one vertex in $S$ and the other in $R$ given as 
[[file:Clustering Validation/screenshot_2018-11-27_23-15-40.png]]

- The internal measures are based on various function over the intracluster and intracluster weights
	- The sum of all the intracluster weights over all clusters is given as 
[[file:Clustering Validation/screenshot_2018-11-27_23-17-14.png]]

- The sum of all intercluster weights is given as 
[[file:Clustering Validation/screenshot_2018-11-27_23-17-59.png]]

- The number of distinct intracluster edge, denoted $N_{in}$ and intercluster edges denoted $N_{out}$ are given as 
[[file:Clustering Validation/screenshot_2018-11-27_23-18-52.png]]

- The total number of distinct pair of points is
[[file:Clustering Validation/screenshot_2018-11-27_23-19-25.png]]

*** BetaCV Measure
- The BetaCV measure is the ratio of the mean intracluster distance to the mean intercluster distance 
[[file:Clustering Validation/screenshot_2018-11-27_23-20-50.png]]
- The smaller the BetaCV ratio, the better the clustering 

** Davies–Bouldin Index
- Let $\mu_i$ denote the cluster mean, given as 
\begin{equation}
  \mu_i = \frac1{n_i} \sum_{\pmb x_j \in C_i} \pmb x_j
\end{equation}

- Let $\sigma_{\mu_i}$ denote the spread of the points around the cluster mean, which is given 
[[file:Clustering Validation/screenshot_2018-11-25_09-28-28.png]] 
- where $var(C_i)$ is the total variance of cluster $C_i$ 

- The Davis-Bloudin measure for a pair of clusters $C_i$ and $C_j$ is defined as the ratio 
[[file:Clustering Validation/screenshot_2018-11-25_09-30-04.png]]
- It measures how compact the clustered are compared to the distance between the clusters means

- The Davies-Bloudin index is defined as 
[[file:Clustering Validation/screenshot_2018-11-25_09-50-46.png]]
- For each cluster $C_i$ we pick the cluster $C_j$ that yields the largest $DB_{ij}$ ration
	- The smaller the DB value the better the clustering
		- It means the clusters are well separated
		- Each cluster is well represented by its mean

** Silhouette Coefficient 
- The silhouette coefficient is a measure of both cohesion and separation of clusters
	- It is based on the difference between the average distance to points in the closest cluster and to points in the same cluster
	- For each point $\pmb x_i$ we calculate its silhouette coefficient $s_i$ as 
[[file:Clustering Validation/screenshot_2018-11-25_09-54-18.png]]
- where $\mu_{in} (\pmb x_i)$ is the mean distance from $\pmb x_i$ to points in its own cluster $\hat y_i$:
[[file:Clustering Validation/screenshot_2018-11-25_09-55-13.png]]
- and $\mu_{out}^{min}(\pmb x_i)$ is the mean of the distances from $\pmb x_i$ to points in the closest cluster 

[[file:Clustering Validation/screenshot_2018-11-25_09-55-59.png]]
- The $s_i$ value of a point lies in the interval $[-1, 1]$
	- A value close to $+1$ indicates that $\pmb x_i$ is much closer to points in its own cluster and is far from other clusters
	- A value close to zero indicates that $\pmb x_i$ is close to the boundary between two clusters
	- A value close to $-1$ indicates that $\pmb x_i$ is much closer to another cluster than its own cluster
		- The point may be mis-clustered

- The silhouette coefficient is defined as the mean $s_i$ value across all the points:
\begin{equation}
	SC = \frac1n \sum_{i=1}^n s_i
\end{equation}
- A value close to $+1$ indicates a good clustering 

** Relative Measures
- The silhouette coefficient for each point $s_j$ and the average SC value can be used to estimate the number of clusters in the data
	1. The approach consists of plotting the $s_j$ values in descending order for each cluster, and to note overall SC value for a particular value of $k$
	2. We can then pick the value $k$ that yields the best clustering with many points having high $s_j$ values within each cluster
		 - As well as high values for SC and $\text{SC}_i$ 
	
* Hierarchical Clustering
** General
- Given $n$ points $d$ dimensional space the goal of hierarchical clustering is to create a sequence of nested partitions
	- This can be visualized via a tree or hierarchy of clusters which is called the cluster dendrogram
	- The clusters in the hierarchy range from the fine-grained to the coarse grained
		- The lowest level of the tree (the leaves) consists of each point in its own cluster
		- The highest level (the root) consists of all points in one cluster
		- At some intermediate level one may find meaningful clusters
	- If the user supplies $k$, as the desired number of clusters it can be chosen at which level there are $k$ clusters

- There are two main algorithmic approaches to get hierarchical clusters
	- *Agglomerative* strategies work in a bottom-up manner
		- They start with each of the $n$ points in a separate cluster
		- They repeatedly merge the most similar pair of clusters until all points are members of the same cluster
	- *Divisive* strategies work in a top-down manner
		- They start with all the points in the same cluster
		- They recursively split the clusters until all points are in separate clusters

** Preliminaries 
- Given a dataset $\mathbf D = \{ \pmb x_1, \dots, \pmb x_n \}$, where $\pmb x_i \in \mathbb R^d$, a clustering $\mathcal C = \{C_1, \dots, C_k\}$ is a partition of $\mathbf D$
	- Each *cluster* is a set of points $C_i \subseteq \mathbf D$, such that the clusters are pairwise disjoint $C_i \cap C_j = \emptyset$ for all $i \ne j$ and $\cup_{i=1}^k C_i = \mathbf D$
	- A clustering $\mathcal A = \{ A_1, \dots, A_r\}$ is said to be *nested* in another clustering $\mathcal B = \{B_1, \dots, B_s\}$ if an only if $r>s$ and for each cluster $A_i \in \mathcal A$ there exists a cluster $B_j \in \mathcal B$ such that $A_i \subseteq B_j$
	- Hierarchical clustering yields a sequence of $n$ nested partitions $C_1, \dots, C_n$ ranging from the trivial clustering $C_1 = \{\{\pmb x_1\}, \dots, \{\pmb x_n\}\}$ where each point is in a separate cluster to the other trivial clustering $\mathcal C_n = \{\{\pmb x_1, \dots, \pmb x_n \}\}$ where all the points are in one cluster
		- In general the clustering $\mathcal C_{t-1}$ are nested in the clustering $\mathcal C_t$
	- The cluster dendrogram is a rooted binary tree that captures the nesting structure
		- There is edges between cluster $C_i \in \mathcal C_{t-1}$ and cluster $C_j \in \mathcal C_t$ if $C_i$ is nested in $C_j$ i.e. if $C_i \subset C_j$
	- The number of different clusterings is $(2n-3)!!$ 
		- This makes a naive approach of enumerating all possible hierarchical clusterings infeasible 

** Agglomerative Hierarchical Clustering 
*** General
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-29-20.png]]
- Several distance measures can be used in the algorithm to compute the distance between any two clusters 
	- The between cluster distances are ultimately based on the distance between two points which is typically computed using Euclidean distance defined as 

- Agglomerative hierarchical clustering begins with each of the $n$ points in a separate cluster
	- The two closest clusters are repeatedly merged until all points are members of the same cluster
	- Since the number of clusters decreases by one in each step, the process result in $n$ nested clusterings
	- If specified the merging process can be stopped when there are $k$ clusters remaining
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-36-31.png]]

- The computational complexity of hierarchical clustering is $O(n^2\log n)$ 

*** Distance between Clusters
- *Since Link* measure
	- Given two clusters $C_i$ and $C_j$ the distance between them denoted $\delta (C_i, C_j)$ is defined as the minimum distance between a point in $C_i$ and a point in $C_j$ 
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-38-39.png]]

- *Complete Link* measure
	- The distance between two clusters is defined as the maximum distance between a point in $C_i$ and a point in $C_j$ 
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-39-59.png]]

*** Updating Distance Matrix
- When two clusters $C_i$ and $C_j$ are merged into $C_{ij}$ the distance matrix should be updated by recomputing the distances from the newly creates clusters to other clusters $C_r$ hvor $r \ne i$ and $r \ne j$
	- The Lance-Williams provides a general equation to recompute the distances for all of the cluster proximity measures given as 
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-48-09.png]]
- The coefficient $\alpha_i$, $\alpha_j$, $\beta$, and $\gamma$ differ from one measure to another
	- Let $n_i = |C_i|$ denote the carnality of cluster $C_i$ the coefficients for the different distance meassures is the given as 
[[file:Hierarchical Clustering/screenshot_2018-12-01_11-51-06.png]]

* DBSCAN Revisted
** General
- The DBSCAN algorithm requires $O(n^2)$ for $d \geq 3$ not $O(n \log n)$ time as claimed in the original paper
	- This is independent of the parameters $\epsilon$ and $MinPts$ for DBSCAN
	- The running time can be dramatically brought down to $O(n)$ by allowing for slight inaccuracies (regardless of $d$)

- Let $B(p, \epsilon)$ be the $d$ dimensional ball centered at point $p$ with radius $\epsilon$
	- The distance metric is Euclidan distance
	- It is dense if it covers at least $MinPts$ points of $P$
	- This is used in DBSCAN to form clusters
		- i.e. the ball is the $\epsilon$ neighbor hood

- The DBSCAN problem is to find a unique set $C$ of clusters of $P$

- One can solve the DBSCAN problem in $O(n \log n)$ time dimensionality $d=2$ 

** Geometric results
- *Bichromatic Closest Pair (BCP)*
	- Let $P_1,P_2$ be to sets of points in $\mathbb R ^d$ for some constant $d$
	- Set $m_1 = | P_1 |$ and $m_2 = |P_2|$ 
	- The goal of the BCP problem is to find a pair of points $(p_1, p_2) \in P_1 \times P_2$ with the smalles distance
	- In 2D space it can be solved in $O(m_1 \log m_1 + m_2 \log m_2)$ time
[[file:DBSCAN Revisted/screenshot_2018-12-05_15-34-11.png]]

- *Spherical Emptiness and Hopcroft*
	- Let $S_{pt}$ be a set points and $S_{ball}$ be a set of balls with the same radius all in data space $\mathbb R^d$ where $d$ is a constant
	- The objective of USEC is to determine whether there is a point of $S_{pt}$ that is covered by some ball in $S_{ball}$
	- Set $n = |S_{pt}| + S_{ball}$
	- In 3D space the USEC problem can be solved in $O(n^{4/3} \cdot \log^{4/3} n)$ expected time
	- Finding a 3D USEC algorithm with running time $o(n^{4/3})$ is a big problem in computational geometry
		- It is widely believed to be impossible
	- Strong hardness results are known about USEC when the dimensionality $d$ is higher it has a connection between the problem to the *Hopcroft's problem*
		- Let $S_{pt}$ be a set of points and $S_{line}$ be a set of lines all in data space $\mathcal R^2$
		- The goal of the Hopcroft's problem is to determine whether there is a point in $S_{pt}$ that lies on some line $S_{line}$
	- Hopcroft's problem can be solved in time slightly higher than $O(n^{4/3})$ time
		- Where $n = |S_{pt}| + |S_{line}|$
		- It is believed that $\Omega (n^{4/3})$ is a lower bound on how fast the problem can be solved
	- A problem $X$ is *Hopcroft hard* if an algorithm solving $X$ in $o(n^{4/3})$ time implies an algorithm solving the Hopcrofts problem in $o(n^{4/3})$ time
	- *Lemma 3* The USEC problem in any dimensionality $d \geq 5$ is Hopcroft hard

** DBSCAN in $\geq$ 3 dimensions
[[file:DBSCAN Revisted/screenshot_2018-12-05_15-46-34.png]]


[[file:DBSCAN Revisted/screenshot_2018-12-05_15-47-41.png]]


[[file:DBSCAN Revisted/screenshot_2018-12-05_15-51-20.png]]

** $\rho$ Approximate DBSCAN
[[file:DBSCAN Revisted/screenshot_2018-12-05_16-02-06.png]]

- *Definition 5.* A $\rho$ approximate cluster $C$ is a non-empty subset of $P$ such that
	- *Maximality:* If a core point $p \in C$ then all points density-reachable from $p$ also belong to $C$
	- *$\rho$ Approximate Connectivity:* For any points $p_1, p_2 \in C$ there exists a point $p \in C$ such that both $p_1$ and $p_2$ are $\rho$ approximate density reachable from $p$ 

[[file:DBSCAN Revisted/screenshot_2018-12-05_16-05-59.png]]

- *The Sandwich Theorem:*
	- $\mathcal C_1$ is defined as the set of clusters of DBSCAN with parameters $(\epsilon, MinPts)$ 
	- $\mathcal C_2$ is defined as the set of clusters of DBSCAN with parameters $(\epsilon(1+p), MinPts)$ 
	- $\mathcal C$ is defied as an arpitary set of clusters of that is a legal result of $(\epsilon, MinPts, \rho)$ approx DBSCAN

[[file:DBSCAN Revisted/screenshot_2018-12-05_16-09-46.png]]

[[file:DBSCAN Revisted/screenshot_2018-12-05_16-18-07.png]]

* Exam
- Tag et billede med af hvordan tavlen skal se ud
