* TA Instructor Info
	- Frederik Hvilshøj
	- Email: fhvilshoj@gmail.com

* Important
** Dictionary 
- *RHS:* Right Hand Side

** Jacobian 
- If we have a function $f(z): \mathbb{R}^{a} \rightarrow \mathbb{R}^b$ such that $f(z) = [f_1(z),\dots, f_b(z)]$ then the Jacobian is the matrix 
\begin{equation}
  J_{i,j} = \frac{\partial f_i}{\partial z_j}
\end{equation} 
of size $b\times a$. 

* The Learning Problem (1)
** Components of Learning
- The main components for the learning problem
	- An input $x$
	- The unknown target function $f: \mathcal X \to \mathcal Y$
		- $\mathcal X$ is the input space
		- $\mathcal Y$ is the output space
	- There is a set of data $\mathcal D$ of input output examples $(x_1, y_1), \cdots, (x_N, y_N)$
		- where $y_n = f(x_n)$ for $n = 1,\dots,N$
		- often referred to as data points
	- The learning algorithm that uses dataset $\mathcal D$ to pick a formula $g: \mathcal X \to \mathcal Y$ that approximates $f$
		- choose $g$ from a set of candidate formulas under consideration which is called the hypothesis set $\mathcal H$
		- $\mathcal H$ could be the set of all linear formulas 
§
** A Simple Learning Model 
- The hypothesis set and learning model is referred informally to as the /learning model/

- A simple learning model (the /perceptron/)
	- Let $\mathcal X = \mathbb R ^d$ where $\mathcal X = \mathbb R ^d$ is the $d$-dimensional Euclidean space be the input space
	- Let $\mathcal Y = \{+1, -1\}$ be the output space
	- The hypothesis set $\mathcal H$ is specified through a functional form that all $h \in \mathcal H$ share
		- The functional form $h(x)$ chosen is to give weights to the different coordinates of $x$ which reflects their importance
	- The weighted score is compared to a threshold value which decides whether the output is $+1$ or $-1$
\begin{equation}
  \begin{split} 
    \text{Approve credit if} \ \sum_{i=1}^d w_ix_i &> \text{threshold} \\
    \text{Deny credit if} \ \sum_{i=1}^d w_ix_i &< \text{threshold}
  \end{split}
\end{equation}
This can be written mode compactly as 
\begin{equation}
  h(x) = \text{sign} \bigg( \bigg( \sum_{i=1}^d w_ix_i \bigg) + b \bigg)
\end{equation}
where $x_1, \dots, x_n$ are the components of the vector $\pmb x$ and $b$ is the threshold

- The bias can be added as the first weight $w_0 = b$ and adding a fixed input $x_0 = 1$ gives us the same result
	- With this convention the previous equation can be written as
\begin{equation}
  h(\pmb x) = \text{sign}(\pmb w^T \pmb x)
\end{equation}
where $\pmb w$ is the weight vector and $\pmb x$ is the input vector

- The /perceptron learning algorithm/ will determine $\pmb w$ based on the data
	- To use this the data should be linearly separable
		- Means that there is a $\pmb w$ which achieve the correct decision $h(\pmb x_n)=y_n$ on all data examples
	- The algorithm finds $\pmb w$ using the following simple iterative method
		- At iteration $t$ where $t = 0,1,2, \dots$ , there is a current value of the weight vector $\pmb w(t)$
		- The algorithm picks an example from $(x_1, y_1), \cdots, (x_N, y_N)$ $(x(t), y(t))$ and uses it to update $\pmb w(t)$ 
		- The update rule is  $\pmb w(t+1) = \pmb w(t) + y(t)\pmb x(t)$


- The learning algorithm is guaranteed to arrive at the right solution at the end

** Types of Learning 
*** Supervised Learning
- The training data contains explicit examples of the correct output for given inputs
- There are two variations of this protocol
	- *Active learning:* where the data set is acquired though queries that we make
		- We get to choose a point $\pmb x$ in the input space and the supervisor reports to us the target value for $\pmb x$
		- This opens up a strategic chosen $\pmb x$
	- *Online learning:* the data set is given to the algorithm one example at a time
		- This happens when we have streaming data that the algorithm has to process on the run
		- Useful for limitations of computing at storage
		- Can also be used in other paradigms of learning
		- e.g. a movie recommendation system

*** Reinforcement Learning
- When training data does not explicitly contain the correct output for each input we are no longer in the supervised setting
- The training example does not contain the target output
	- It contains so possible outputs of how good that output is
- The training examples in reinforcement learning are of the form
#+BEGIN_CENTER
	( input, some output, grade for this output )
#+END_CENTER 
- The examples does not say how good the inputs would have been for other settings
- Can e.g. be useful for learning to play a game
	
*** Unsupervised Learning
- In unsupervised learning the data does not contain any output information at all
	- We are just given input examples $\pmb x_1, \dots, \pmb x_N$

- The decision regions in unsupervised learning are the same as the one in supervised learning without label
- It can be viewed as a task of spontaneously finding patterns and structure in input data
- It can be a precursor to supervised learning 

** Linear Regression and Orthogonal Projections
- *Lemma 1.* Given a vector $y$ the closest point in $V$ to $y$, i.e. $\arg\min_v \in V : ||v-y||_2^2$  is the orthogonal projection of $y$ onto $V$.
- The optimal weight vector $w$ is found using the following formula 
\begin{equation}
  w = (X^TX)^{-1}X^Ty
\end{equation}
where $X$ is a $n \times d$ data matrix where each row is an input point and a $n \times 1$ vector y of targets

** Is Learning Feasible 
*** General
- To see the relationship between the data $\mathcal D$ and the data outside the *Hoeffding Inequality* is used
	- It states for a random variable $\nu$ in terms of the parameter $\mu$ and the sample size $N$ that 
\begin{equation}
      \mathbb P[|\nu - \mu > \epsilon] \geq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
- This shows that as one increase the sample size $\nu$ gets closer to $\mu$ for some small number $\epsilon$ 

- The error rate within the sample is called the *in-sample error*
\begin{equation}
  \begin{split} 
    E_\text{in}(h) &= \text{(fraction of $D$ where $f$ and $h$ disagree)} \\
      &= \frac1N \sum_{n=1}^N\llbracket h(\pmb x_n \ne f(\pmb x_n) \rrbracket
  \end{split}
\end{equation}
- where $\llbracket \text{statement} \rrbracket = 1$ if the statement is true and $0$ otherwise

- The *out-of-sample error* is defined as 
\begin{equation}
  E_\text{out}(h)=\mathbb P [h(\pmb x) \ne f(\pmb x)]
\end{equation}

- Using in-sample and out-of-sample error the *Hoeffding Inequality* can be written as 
\begin{equation}
        \mathbb [|E_\text{in}(h)-E_\text{out}(h) > \epsilon] \leq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
- where $N$ is the number of training examples

*** Feasibility of Learning 
 - $\mathcal D$ does not deterministic tell us something about $f$ outside of $\mathcal D$ but it gives us a probabilistic answer
 - Since the *Hoeffding Inequality* tells us that $E_\text{in}(g) \approx E_\text{out}(g)$ for a large enough $N$ $E_\text{in}(g)$ seems like a good proxy for $E_\text{out}(g)$ 

 - The feasibility of learning is split into two questions
	 1. Can we make sure that $E_\text{out}(g)$ is close enough to $E_\text{in}(g)$
	 2. Can we make $E_\text{in}(g)$ small enough?

 - *The complexity of* $\mathcal H$: If the number of hypothesis $M$ goes up we run more risk that $E_\text{in}(g)$ will be a poor estimator of $E_\text{out}(g)$
	 - $M$ can be though of as a measure of the complexity of the hypothesis set $\mathcal H$ that we use
	 - The bigger the $M$ the higher the change of finding a small enough $E_\text{in}(g)$ becomes  

 - *The complexity of* $f$: A more complex $f$ is harder to learn
	 - A more complex hypothesis makes the likelihood that the $E_\text{in}(g)$ and $E_\text{out}(g)$ are approximately the same smaller
	 - If the target function $f$ is two hard one may not be able to learn it at all
	 - Most target functions in real life are not too complex

** Error and Noise 
*** Error Measures 
- An *error measure* quantifies how well each hypothesis $h$ in the target function $f$ 
\begin{equation}
	\text{Error} = E(h,f)
\end{equation}

- While $E(h,f)$ is based on the entirety of $h$ and $f$ is almost universally defined based on the error of individual points $\pmb x$
	- If we define a pointwise error measure $e(h(\pmb x), f(\pmb x))$ the overall error will be the average of the pointwise error
	- The defined error should be defined on the use of the application 

*** Noisy target
- In a real world application the data that one learns from is not generated from a deterministic target function but in a noisy way
	- Formally we have a *target distribution* $P(y \mid \pmb x)$ instead of a taget function
	- One can think of a *noisy target* as a deterministic target, plus added noise

* Training versus Testing (2)
** Theory of Generalization
*** General 
- The *generalization error* is the discrepancy between $E_\text{in}$ and $E_\text{out}$
	- The Hoeffding Inequality provides a way to characterize it with a probabilistic bound
	- The Hoeffding Inequality can be rephrased as follows: pick a tolerance level $\delta$ e.g. 0.05 and assert with probability at least $1-\delta$ that
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac1{2N} \ln \frac{2M}\delta} 
\end{equation}
- It is referred to as the *generalization bound* 

*** Effective Number of Hypotheses
- The *growth function* is the quantity that will formalize the effective number of hypotheses
	- It will replace $M$ when $M=\infty$ in the generalization bound

- *Definition 2.1.* Let $x_1, \dots, x_N \in \mathcal X$. The *dichotomies* generated by $\mathcal H$ on these points are defined by 
\begin{equation}
  \mathcal H (\pmb x_1, \dots, \pmb x_N= \{ (h(\pmb x_1), \dots, h(\pmb x_N ) \mid h \in \mathcal H)
\end{equation}

- *Definition 2.2.* The *growth function* is defined for a hypothesis set $\mathcal H$ by 
\begin{equation}
  m_\mathcal{H}(N) = \underset{\pmb x_1, \dots, \pmb x_N \in \mathcal X}{\text{max}} | \mathcal H(\pmb x_1, \dots, \pmb x_N)|
\end{equation}
- where $|\cdot|$ denotes the number of elements of the set

- That $\mathcal H$ can *shatter* $\pmb x_1, \dots, \pmb x_N$ signifies that $\mathcal H$ is as diverse as can be on this particular sample

- *Definition 2.3.* If no data set of size $k$ can be shattered by $\mathcal H$, then $k$ is said to be a break point for $\mathcal H$ 

- If $k$ is a break point, then $m_\mathcal{H}(k) < 2^k$ 

*** Bounding the growth function
- *Definition 2.4.* $B(N,k)$ is the maximum number of dichotomies on $N$ points such that no subset of size $k$ of the $N$ points can be shattered by these dichotomies 

- *Lemma 2.3.* (Sauer's Lemma)
\begin{equation}
  B(N,k) \leq \sum_{i=0}^{k-1}\binom N i
\end{equation}

- *Theorem 2.4.* If $m_\mathcal H (k) < 2^k$ for some value of $k$, then
\begin{equation}
  m_\mathcal H (n) \leq \sum_{i=0}^{k-1} \binom N i
\end{equation}
- for all $N$. The RHS is polynomial in $N$ of degree $k-1$ 

*** The VC Dimension 
- *Definition 2.5.* The *Vapnik-Chervonekis dimension* of a hypothesis set $\mathcal H$, denoted by $d_{vc}(\mathcal H)$ or simply $d_{vc}$ is the largest value of $N$ for which $m_\mathcal H (N) = 2^N$. If $m_\mathcal H (N) = 2^N$ for all $N$, then $d_{vc}(\mathcal H) = \infty$.

- No smaller breakpoint than $k = d_{vc} +1$ exists

\begin{equation}
    d_{vc} \geq N \iff \text{there \textbf{exists} $\mathcal D$ of size $N$ such that $\mathcal H$ shatters $\mathcal D$}
\end{equation}

- The VC dimension of a $d$-dimensional perceptron is $d+1$. 

*** The VC Generalization Bound 
- *Theorem 2.5.* (VC generalization bound). For any tolerance $\delta > 0$, 
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}
\end{equation}
- with probability $\geq 1-\delta$ 

** Interpreting the Generalization Bound
*** General
- The VC generalization bound is a universal result
	- It applies to all hypotheses set, learning algorithms, input spaces, probability distributions and binary target functions
	- The bound is quite loose
	- It can be used as a guideline for generalization
	- Learning models with lower $d_\text{vc}$ tend to better than those with higher $d_\text{vc}$ 

*** Sample Complexity 
- The *sample complexity* denotes how many training examples $N$ are needed to achieve a certain generalization performance
	- The performance is specified using two parameters $\epsilon$ and $\delta$ 
		- The error tolerance $\epsilon$ determines the allowed generalization error
		- The confidence parameter $\delta$ determines how often the error tolerance $\epsilon$ is violated
	- How fast $N$ grows as $\epsilon$ and $\delta$ become smaller indicates the amount of data needed for a good generalization

- From the VC generalization bound it follows that
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4m_\mathcal H (2N)}\delta)
\end{equation}
- If $m_\mathcal H(2N)$ is replaced by its generalization polynomial upper bound we get that
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4((2N)^{d_\text{vc}} +1)} \delta)
\end{equation}
- The numerical value for $N$ can be obtained using simple iterative methods

*** Penalty for Model Complexity
- Often we have a fixed dataset, we can the use the Generalization bound to find out what performance we can expect to get
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}  
\end{equation}
- We can again use the polynomial bound based on $d_\text{vc}$ instead of $m_\mathcal H(2n)$ 
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

- We often denote the second as $\Omega(N,\mathcal H, \delta)$ and call it the penalty
\begin{equation}
  \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

- More complex models help $E_\text{in}$ and hurt $\Omega(N,\mathcal H, \delta)$
	- The optimal model is one that minimizes a combination of the two terms

[[file:Training versus Testing (2)/screenshot_2018-09-12_19-20-23.png]]

*** The Test Set
- One often estimates $E_\text{out}$ by using a *test set* that the learning algorithm has not seen before
	- Called $E_\text{test}$

** Approximation-Generalization Tradeoff 
*** Bias and Variance 
- The bias variance decomposition out-of-sample error is 
\begin{equation}
  E_\text{out}(g^{(\mathcal D)}) = \mathbb E_{\pmb x} \big[ (g^{(\mathcal D)}(\pmb x) - f(\pmb x))^2 \big]
\end{equation}

- The function $\bar g (\pmb x)$ can be interpreted in the following operational way
	1. Generate many data sets $\mathcal D_1, \dots, \mathcal D_K$
	2. Apply the learning algortihm to each data set obtaining final hypotheses $g_1,\dots,g_K$.
	3. The average function for any $\pmb x$ is then estimated by $\bar g(\pmb x) \approx \frac1K \sum_{k=1}^K g_k(\pmb x)$ 

- The *bias* is $\text{bias}(\pmb x) = (\bar g(\pmb x) - f(\pmb x))^2$
	- It measures how much the average function deviates from the target function 

- The *variance* is 
\begin{equation}
	\text{var}(\pmb x)   = E_\mathcal D [(g^{\mathcal D}(x) - \bar g(\pmb x))^2]
\end{equation}
- Says how much the different hypotheses varies

- Since bias and variance cannot be computed in a real model
	- They are purly a conceptual tool used when developing a model

*** The Learning Curve
[[file:Training versus Testing (2)/screenshot_2018-09-12_20-08-30.png]]

- For a simpler model the learning curves converge more quickly but to worse ultimate performance
	- The in-sample error learning curve is increasing in $N$
	- The out-of-sample error learning curve is decreasing in $N$

* The Linear Model (3)
** Linear Regression
*** The algorithm 
- The linear regression algorithm is based on minimizing the squared error between $h(x)$ and $y$
\begin{equation}
  E_\text{out}(h) = \mathbb E[(h(\pmb x) - y)^2]
\end{equation}
where the expected value is taken with respect to the joint probability distribution $P(x,y)$ 

- The goal is to find an hypothesis that achieves a small $E_\text{out}(h)$
	- Since the distribution $P(\pmb x, y)$ is unknown $E_\text{out}(h)$ cannot be computed the in-sample version is therefore used instead
\begin{equation}
    E_\text{in}(h) = \frac{1}{n}\sum_{n=1}^N(h(\pmb x_n) - y_n)^2
\end{equation}

- In linear regression $h$ takes the form of a linear combination of the components of $x$ that is
\begin{equation}
  h(\pmb x) = \sum_{i=0}^dw_ix_i = \pmb w^T\pmb x
\end{equation}
	where $x_0 = 1$ an $\pmb x \in \{1\} \times \mathbb R ^d$ as usual and $\pmb w \in \mathbb R^{d+1}$ 

- For the special case of linear $h$, it is very useful to have a matrix representation of $E_\text{in}(h)$
	- First we define the data matrix $X \in \mathbb R^{N \times (d+1)}$ to be the $N \times (d+1)$ matrix whose rows are the inputs $x_n$ as row vector
	- Define the target vector $\pmb y \in \mathbb R^N$ to be the column vector whose components are the target values $y_n$

[[file:The Linear Model/screenshot_2018-08-28_14-54-58.png]]
** Logistic Regression
*** Predicting a Probability
- To predict a probability we want something which restricts the output to the probability range $[0,1]$, one choice that accomplishes this goal is the logistic regression model
\begin{equation}
    h(\pmb x) = \theta(\pmb w^T \pmb x)
\end{equation}
- Where $\theta$ is the /logistic/ function $\theta(s) = \frac{e^s}{1+e^s}$ whose output is between 0 and 1
	- The output can be interpret as a probability for a binary event
	- The logistic function $\sigma$ is referred to as a *soft threshold* in contrast to the *hard threshold* in classification
	- It is also called a *sigmoid*

- When using Logistic Regression we are formally trying to learn the target function
\begin{equation}
  f(\pmb x) = \mathbb P [y=+1 \mid \pmb x]
\end{equation}

- The data given is generated by a noisy target $P(y \mid \pmb x)$ 
\begin{equation}
  \begin{equation*}
    P(y \mid \pmb x) = 
  		\begin{cases}
  			\mbox{$f(\pmb x)$} & \mbox{for $y=+1$} \\
  			\mbox{$1-f(\pmb x)$} & \mbox{for $y=-1$} 
  		\end{cases}
  \end{equation*}    
\end{equation}

- The standard *error measure* $e(h(\pmb x),y)$ used in logistic regression is based how likely it is that we would get this output $y$ from the input $\pmb x$. if the target distribution $P(y | \pmb x)$ was indeed captured by our hypothesis $h(x)$. 

- The *error measure* is $E_\text{in}(\pmb w) = \frac1N \sum_{n=1}^N\ln(1+e^{-y_n\pmb w^T x_n})$

*** Gradient Descent 
**** Batch Gradient Descent
- Gradient descent is a general technique for minimize a twice differentiable function
	- e.g. $E_\text{in}(\pmb w)$ in logistic regression
	- You start somewhere and go the steepest way down the surface
	- You may end up in a local minima
	- When using a convex function such as $E_i$ there is only one minima the global unique minimum

- When steeping in a direction you need that the step $\eta$ is not too small or too large
[[file:The Linear Model/screenshot_2018-09-03_17-19-24.png]]
- You typically want to choose $\eta_t = \eta || \bigtriangledown E_\text{IN} ||$ to obtain a good variable step size

[[file:The Linear Model/screenshot_2018-09-03_17-24-49.png]]
- A typical good choice for $\eta$ is a fixed learning rate is around $0.1$ the 

[[file:The Linear Model/screenshot_2018-09-03_17-28-37.png]]

- *Initialization*
	- Most of the time the initializing the initial weights as zeros works well
	- It is in general safer to initialize the weights randomly
	- Choosing each weight independently from a Normal distribution with zero mean and small variance usually works well

- *Termination*
	- A simple approach would be to set and upper limit on the number of iterations
		- Does not guarantee anything on the quality of the final weights
	- A natural terminal criterion would be to stop ones $||\pmb g_t||$ drops below a certain threshold
		- Eventually this must happen but we do not know but we will now know when
	- For logistic regression a combination of the two termination conditions is used

**** Stochastic Gradient Descent (SGD)
- A sequential version of Batch Gradient Decent
	- Often beats the batch version in practise

- Instead of considering the full batch gradient on all $N$ training points, we consider a stochastic version of the gradient
	1. Pick a training data point $(\pmb x_n, y_n)$ at uniformly random
	2. Consider only the error on that point (in case of logistic regression)
\begin{equation}
   e_n(w) = \ln ( 1+ e^{-y_n \pmb w^T \pmb x_n})
\end{equation}

- The gradient needed is
\begin{equation}
  \nabla e_n (\pmb w) = \frac{-y_n\pmb x_n}{1+e^{-y_n \pmb w^T \pmb x_n}}
\end{equation}
- The weight update is $\pmb w \leftarrow \pmb w \cdot \eta \nabla e_n(\pmb w)$

** Nonlinear Transformation
*** The $\mathcal Z$ space
#+NAME: transformExample
#+CAPTION: Example of nonlinear transform
[[file:The Linear Model/screenshot_2018-09-03_18-22-01.png]]
- Using a nonlinear transformation we can convert data which is not linear separable into data that is
	- The space $\mathcal Z$ generated is called the *feature space*
	- The transformation from the original space $\mathcal X$ to $\mathcal Z$ is called a *feature transform*

- Any linear hypothesis $\tilde h$ in $\pmb z$ corresponds to a (possible nonlinear) hypothesis of
 $\pmb x$ given by $h(\pmb x) = \tidle h (\theta(\pmb x))$ where $\theta$ is a non linear transform
	- The set of these hypothesis is denoted by $\mathcal H_\theta$

#+NAME: /tmp/screenshot.png @ 2018-09-03 18:28:08
#+CAPTION: The nonlinear transform for separating non separable data.
[[file:The Linear Model/screenshot_2018-09-03_18-28-08.png]]

- The feature transform $\theta_Q$ is defined for degree-$Q$ curves in $\mathcal X$
	- It is called the Qth order polynomial transform

- The power of the feature transform should be used with care, it may not be worth it to insist on linear separability and employ a highly complex surface
	- It is sometime better to tolerate a small $E_in$ than using a feature transform

*** Computation and generalization
- Computation is an issue because $\theta_Q$ maps a two dimensional vector $\pmb x$ to $\tilde d = \frac{Q(Q+3)}2$ dimensions, which increases the memory and computational cost
	- Things could get worse if $\pmb x$ is in a higher dimension to begin with

- The problem of generalization is another important issue
	- We will have a weaker guarantee that $E_\text{OUT}$ is small
	- It is sometime balanced  by the advantage we get in approximating the target better

[[file:The Linear Model (3)/screenshot_2018-09-04_07-51-06.png]]
* Multinomial/Softmax Regression
** Setup
- Multinomial/Softmax Regression generalizes logistic regression to handle $K$ classes instead of $2$
	- A target value $y$ is represented as a vector of length $K$ with all zeroes except one which is called a one-in-$K$ encoding
	- To store all the data points a matrix $Y$ of size $n \times K$ and the data matrix $X$ is unchanged
\begin{equation}
X=\begin{pmatrix} 
1&- & x_1^T & - \\
\vdots & \vdots & \vdots \\
1&- & x_n^T & - \\
\end{pmatrix}\in \mathbb{R}^{n \times d}\quad\quad 
y=\begin{pmatrix}
- & y_1^T & -\\
- & \vdots &- \\
- & y_n^T & -\end{pmatrix}\in\{0,1\}^{n\times K}
\end{equation}

- To generalize to $K$ classes we will use $K$ weight vectors $w_1,\dots,w_k$ each of length $d$, one for each class.
	- To classify data we can use the following algorith: Given data x, compute $w_i^\intercal x$ for $i=1,\dots, K$ and return the index of the largest value.
	- The list of weight vectors is packed into a matrix $W$ of size $d \times K$ by putting $w_1$ in column one and so on.
$$
W=\begin{pmatrix} 
(w_1)_1  & \dots & (w_K)_1 \\
\vdots & \vdots & \vdots \\
(w_1)_d  & \dots & (w_K)_d \\
\end{pmatrix}\in \mathbb{R}^{d \times n}
$$
- This way we can compute the weighed sum for each class by the vector matrix product $x^\intercal W$ and then pick argmax of that to do the classification. Pretty Neat!.

- Numpy example
#+BEGIN_SRC python
import numpy as np
# example with 3 classes and d = 10
W = np.random.rand(10, 3)
print('Shape w:', W.shape)
x = np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.0]).reshape(10, 1)
print('Shape x:', x.shape)
model_predictions = x.T @ W
print('model (unnormalzed log) predictions: - picke the larger one\n', model_predictions)
#+END_SRC

** Probabilistic Outputs
- Given a set of model parameters $W$ and a data point $x$ we want $P(y=i\mid x, W)$ for $i=1,\dots K$.

- *Softmax* is used in our probabilistic model
	- It takes as input a vector of length $K$ and outputs another vector of the same length $K$, that is a mapping from the $K$ input numbers into $K$ *probabilities*
\begin{equation}
\textrm{softmax}(x)_j =
\frac{e^{x_j}}
{\sum_{i=1}^K e^{x_i}}\quad
\textrm{ for }\quad j = 1, \dots, K.
\end{equation}
- where $\textrm{softmax}(x)_j$ denote the $j$'th entry in the vector.
	- The denominator acts as a normalization term that ensures that the probabilities sum to one 
	- The exponentiation ensures all numbers are positive.
	- We get the following derivatives:
\begin{equation}
\frac
{\partial \;\textrm{softmax}(x)_i}
{\partial x_j} =
(\delta_{i,j} - \textrm{softmax}(x)_j)
\textrm{softmax}(x)_i\quad\quad\text{where}\quad\quad
\delta_{ij}=\begin{cases}1 &\text{if }i=j\\
0 & \text{else}
\end{cases}
\end{equation}

- The following is our probabilistic model
\begin{equation}
p(y \mid x, W) =
\textrm{softmax}(W^\intercal x) =
 \left \{
\begin{array}{l l}
 \textrm{softmax}(W^\intercal x)_1 & \text{ if } y = e_1,  \\
 \vdots & \\
 \textrm{softmax}(W^\intercal x)_K & \text { if } y = e_K.
\end{array}
\right.
\end{equation}

- We compute the likelihood of the data given a fixed matrix of parameters.
	- The notation $[z]$ for the indicator function
$$
P(D \mid W) =
\prod_{(x,y)\in D}
\prod_{j=1}^K
\textrm{softmax}(W^\intercal x)_j^{[y_j=1]}
=
\prod_{(x,y)\in D}
y^\intercal
\textrm{softmax}(W^\intercal x)
.
$$

- This way of expressing is the same as we did for logistic regression.

** The Negative Log Likelihood
- The negative log likelihood of the data is minimized instead of maximizing the likelihood of the data and get a pointwise sum.
\begin{align}\textrm{NLL}(D\mid W) &=
-\sum_{(x,y)\in D}
\sum_{j=1}^K
[y_j=1]
\ln (\textrm{softmax}(W^\intercal x)_j)
\\
&=-\sum_{(x,y)\in D}
y^\intercal
\ln (\textrm{softmax}(W^\intercal x))
\end{align}


- In the last summation only one value will be nonzero:
\begin{equation}
  - \ln \textrm{softmax}(z)_j = \ln \left( \frac{e^{z_j}}{\sum_{i=1}^d e^{z_i}}\right) = - (z_j - \ln \sum_{i=1}^d e^{z_i})
\end{equation}

- The insample error is defined to be  $E_\textrm{in} = \frac{1}{|D|} \textrm{NLL}$
	- Cannot be solved for a 0 analytically
	- To apply stochastic mini-batch gradient descent as for Logistic Regression all you really need is the gradient of the negative log likelihood function.
		- The gradient is a *simple* generalization of the one used in logistic regression.
		- There is a set of parameters for each of $K$ classes, $W_j$ for $j=1,\ldots,K$
		- The gradient is 
$$
\nabla \textrm{NLL}(W) =
-X^\intercal
(Y - \textrm{softmax}(XW)),
$$

** Implementation Issues
*** Numerical Issues with Softmax
- There are some numerical issues with the softmax function

$$
\textrm{softmax}(x)_j = \frac{e^{x_j}}{\sum_{i=1}^K e^{x_i}} \textrm{ for } j=1,\ldots,K.
$$
- This is because this is a sum of exponentials and exponentiation of numbers tend to make them very large giving numerical problems.

- The problematic part is the logarithm of the sum of exponentials.
- We can move $e^c$ for any constant $c$ outside the sum easily, that is:
$$
\ln\left(\sum_i e^{x_i}\right) =
\ln\left(e^c \sum_i e^{x_i-c}\right) =
c + \ln\left(\sum_i e^{x_i -c}\right).
$$

- We need to find a good $c$, and we choose $c = \max_i x_i$
- Since $e^{x_i}$ is the dominant term in the sum. We are less concerned with values being inadvertently rounded to zero since that does not

*** One in k encoding
- Representing a number $k$ in $[1,\dots,k]$ as a vector of length may $K$ be quite cumbersome.
	- In general the input labels will just be a list/vector of numbers between 1 and k.
	- It is your job to transform it into a matrix if needed.
	- But this will be a very sparse matrix.
	- It may be worthwhile to consider whether it is possible to implement the operations with the matrix Y without actually creating the matrix. 

*** Always check your shapes
- If the shapes dont fit then
	- If trying to implement softmax it is very useful to ensure you have full control over the shapes of all matrices and vectors you use.
	- If there is a shape mismatch then clearly there is a larger issue.
	- Checking shapes is a very efficient heuristic for catching bugs.

*** Bias Variable
- If you need a Bias variable $b$ (remember $w^\intercal x + b$) for each class you need to add a columns of ones to $X$ and make $W$ a $d+1 \times K$ matrix.

* Overfitting (4)
** When Does Overfitting Occur?
*** General
#+NAME: overfittingExample
#+CAPTION: Example of overfitting
[[file:Overfitting (4)/screenshot_2018-09-17_10-02-37.png]]
- The main case of overfitting is when you pick the hypothesis with lower $E_{in}$ and it results in higher $E_{out}$
	- Means that $E_{in}$ alone is no longer a good guide for learning
	- A typical overfitting scenario is when a complex model uses its addition degress of freedom to "learn" the noise

*** Catalysts for Overfitting
[[file:Overfitting (4)/screenshot_2018-09-17_10-16-04.png]]
- On a finite data set the algorithm inadvertently uses some of the degrees of freedom to fit the noise
	- Can result in overfitting and a spurious final hypothesis

- There are two types of noise which that algorithm cannot differentiate
	- *Deterministic noise* will not change if the dataset was generated again
		- Is different depending on which model we use
		- Related to the bias
	- *Stochastic noise* will change if the dataset was generated again
		- Related to the variance

** Regularization
*** General
- *Regularization* is a way to combat overfitting
	- Constraints the learning algorithm to improve out-of-sample error
	- Especially when noise is present

- A view of regularization is thought the VC bound, which bounds $E_\text{out}$ using a model complexity penalty $\Omega(\mathcal H)$:
\begin{equation}
    E_\text{out}(h) \leq E_\text{in}(h) + \Omega(\mathcal H) \ \text{for all } h \in \mathcal H
\end{equation}
- We are better off fitting the data using a simple $\mathcal H$ 

- Instead on minimizing $E_\text{in}(h)$ alone one minimizes the combination of $E_\text{in}(h)$ and $\Omega(h)$
	- Avoids overfitting by constraining the learning algorithm to fit data well using a simple hypotheses

*** A Soft Order Constraint 
- A *Soft Order Constraint* can be defined as the hypotheses set 
\begin{equation}
    \mathcal{C}=\{h \mid h(\pmb x) = \pmb x ^T\pmb x, \pmb w^T \pmb w \leq C\}
\end{equation}
- Solving for $\pmb w_\text{reg}$:
	- If $\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C$ then $\pmb w_\text{reg} = \pmb w_\text{lin}$ since $\pmb w_\text{lin} \in \mathcal H(C)$ 
	- If $\pmb w_\text{lin} \notin \mathcal H(C)$ then not only is $\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C$ but $\pmb w_\text{lin}^T \pmb w_\text{lin} = C$
		- The weights $\pmb w$ must lie on the surface of thee $sphere $\pmb w^T \pmb w = C$ 

- If $\pmb w_\text{reg}$ is to be optimal then for some positive parameter $\lambda_C$
\begin{equation}
  \nabla E_\text{in}(\pmb w_\text{reg}) = - 2 \lambda_C\pmb w_\text{reg}
\end{equation}
- $\nabla E_\text{in}$ must be parallel to $\pmb w_\text{reg}$ 

For some $\lambda_C > 0$ $\pmb w_\text{reg}$ locally minimizes
\begin{equation}
    E_\text{in}(\pmb w) + \lambda_CW^TW
\end{equation}

*** Weight Decay and Augmented Error 
[[file:Overfitting (4)/screenshot_2018-09-18_08-20-44.png]]
- The *augmented error* is defined as
\begin{equation}
  E_\text{aug}(\pmb w) = E_\text{in}(\pmb w) + \lambda \pmb w^T \pmb w
\end{equation}
- where $\lambda \geq 0$ is now a free parameter

- The penalty term enforces a trade-off between making the in-sample error small and making the weights small
	- Is also known as the *weight decay* 
	- Minimizing the error together with the decay is known as *ridge regression*

- If we can find the optimal $\lambda^*$ we can minimize the out-of-sample error

- In general the *augmented error* for a hypothesis set $h \in \mathcal H$ is 
\begin{equation}
    E_\text{aug}(h,\lambda,\Omega) = E_\text{in}(h) + \frac\lambda N \Omega(h)
\end{equation}
- For weight decay $\Omega(h) = \pmb w^T\pmb w$
	- The need for regularization goes down as the number of data points goes up

*** Choosing a Regularizer
[[file:Overfitting (4)/screenshot_2018-09-18_08-32-17.png]]

- A uniform regularizer, is a penalizes the weights equally
	- encourages all weights to be small uniformly
	- Example $\Omega_\text{unif}(\pmb w)= \sum_{q=0}^{15}w_q^2$
- A lower-order regularizer
	- Pairs more attention to the higher order weights
	- Example $\Omega_\text{low}(\pmb w)= \sum_{q=0}^{15}qw_q^2$

- The price paid for overfitting is generally more severe than underfitting
- The optimal  value for the regularization parameter increases with noise
- No regularizer will be ideal for all settings
	- Not even specific settings
	- The entire burden rest on picking the right $\lambda$
- Some for of regularization is necessary as learning is quite sensitive to stochastic and deterministic noise 

** Validation
*** The Validation Set
- *Validation* tries to estimate the out-of-sample error directly

- The idea of a *validation set* is almost identical to that of a test set
	- A subset of the data is removed and not used in training
	- Will be used to make certain choice in the learning process
		- Therefore not a test set

- The *validation set* is created and used in the following way
	1. Partition the data set $\mathcal D$ using into a training set $\mathcal D_\text{train}$ of size $(N-K)$ and a validation set $\mathcal D_\text{val}$ of size $K$
		 - Any partitioning method which does not depend on the data will do 
	2. Run the learning algorithm using the training set $\mathcal D_\text{train}$ to obtain a final hypothesis $g^- \in \mathcal H$
	3. The validation error is then computed for $g$ using the validation set $\mathcal D_\text{val}$
\begin{equation}
  E_\text{val}(g^-)=\frac1K\sum_{\pmb x_n \in \mathcal D_\text{val}} e(g^-(\pmb x_n),y_n)
\end{equation}
- where $e(g(\pmb x),y)$ is the pointwise error measure

- The validation error is an /unbiased/ estimate of $E_\text{out}$ because the final hypothesis $g^-$ was created independently of the validation set
	- The expected error of $E_\text{val}$ is $E_\text{out}$
	- If $K$ is neither too small nor too large $E_\text{val}$ is a good estimate of $E_\text{out}$
	- A rule of thumb in practise is to set $K=\frac N5$ 

- We should not output $g^-$ we should output $g$ which is trained on the entire hypothesis set $D$
	- To estimate $E_\text{out}$ we use that $E_\text{out}(g) \leq E_\text{out}(g^-)$ because of the learning curve
		- Is not rigorously proved
		- It is just very likely 

*** Model Selection 
[[file:Overfitting (4)/screenshot_2018-09-18_09-48-43.png]]
- The most important use of validation is for *model selection*
	- Choosing linear or nonlinear, polynomial or not...
	- It could any choice that affects the learning process

- It can be used to estimate the out-of-sample error for more than one model, suppose we have $M$ models $\mathcal H_1, \dots, \mathcal H_M$ 
	- Validation can be used to select one of these models
	- Use the training set $\mathcal D_\text{train}$ to learn the final hypothesis $g^-_m$ for each model
	- Evaluate each model on the validation set to obtain the validation errors $E_1,\dots,E_M$ where
\begin{equation}
  E_m = E_\text{val}(g^-_m); \text{ for } m = 1,\dots,M
\end{equation}
- Then just select the model with the lowest validation error.
- For suitable $K$ even $g^-_{m*}$ is better than in-sample selection of the model
- The validation error can also be used to select a lambda by using $(\mathcal H, \lambda_1),(\mathcal H, \lambda_2),\dots,(\mathcal H \lambda_M)$ as our $M$ different models
- The more one uses the validation set to fine tune the model the more the it becomes like the training set

*** Cross Validation
[[file:Overfitting (4)/screenshot_2018-09-18_20-20-44.png]]

- There are $N$ ways to partition a set of size $N-1$ and a validation set of size $1$. Let
\begin{equation}
  \mathcal D_n = (\pmb x_1, y_1), \dots, (\pmb x_{n-1}, y_{n-1}), (\pmb x_{n+1}, y_{n+1}), \dots, (\pmb x_N, y_N)
\end{equation}
- The final hypothesis learned from $\mathcal D_n$ is denoted $g_n^-$
- Let $e_n$ be the error made by $g_n^-$ on its validation set which is just a single point $\{(\pmb x_n, y_n)\}$
- The cross validation estimate is the average value of the $e_n\text{'s}$
\begin{equation}
  E_\text{cv}=\frac1N \sum_{n=1}^Ne_n
\end{equation}

- *Theorem 4.4.* $E_\text{cv}$ is an unbiased estimate of $\bar E_\text{out}(N-1)$
	- The expectation of the model performance, $\mathbb E[E_\text{out}])$, over data set of size $N-1$

- The cross validation estimate will on average be an upper estimate for the out-of-sample error: $E_\text{out}(g) \leq E_\text{cv}$

- Cross validation can be for model selection for a given set of models $\mathcal H_1, \dots, \mathcal H_M$ in the same way as validation set

[[file:Overfitting (4)/screenshot_2018-09-18_20-42-49.png]]

- To get cross validation for $M$ models and a data set $D$ of size $N$ is requires $MN$ rounds of learning
- If one could analytically obtain $E_\text{cv}$ it would be a big bonus
	- Analytical results are hard to come
	- An analytical method exists for linear models 

- The cross validation estimate can be analytically computed as 
\begin{equation}
  E_\text{cv} = \frac1N\sum_{n=1}^N(\frac{\hat y_n-y_n}{1-H_nn(\lambda)})^2
\end{equation}
- where $H(\lambda)=Z(Z^TZ+ \lambda I)^{-1}Z^T$ 

* Support Vector Machines 
** Notation
- The classifier considered will be a linear classifier for a binary classification problem with labels $y$ and features $x$
	- $x \in \{-1,1\}$
	- The classifier with parameters $w$ and $b$  is written as
\begin{equation}
  h_{w,b}(x)=g(w^Tx+b)
\end{equation}
- where g = 1 if $z \geq 0$ and $g(z)= -1$ otherwise

** Functional and geometric margins
- Given a training example $(x^{(i)},y^{(i)})$, the *functional margin* of $(w,b)$ is defined with respect to the training example
\begin{equation}
  \hat \gamma^{(i)}=y^{(i)}(w^Tx+b)
\end{equation}
- A large functional margin represents a confident and a correct prediction
- Given a training set $S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}$ the functional margin of $(w,b)$ with respect to $S$ is
\begin{equation}
  \hat \gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \hat \gamma^{(i)}
\end{equation}

- The *geometric margin* of $(w,b)$ with respect to a training example $(x^{(i)},y^{(i)})$ to be 
\begin{equation}
	\gamma^{(i)} = y{(i)} \Bigg( \bigg( \frac w{||w||} \bigg)^T x^{(i)} + \frac b{||w||}  \Bigg)
\end{equation}
- If $||w|| == 1$ then the functional margin equals the geometric margin 
- Given a training set $S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}$ the *geometric margin* of $(w,b)$ with respect to $S$ is
\begin{equation}
	\gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \gamma^{(i)}
\end{equation}

** The optimal margin classifier
- The problem of finding a decision boundary which has the largest geometric margins is the following optimisation problem
\begin{equation}
  \begin{split} 
    \max_{\gamma,w,b} \ &\gamma\\
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq \gamma, \ i= 1, \dots, m \\
    &||w|| = 1
  \end{split}
\end{equation}
- this can be be turned into the following problem using functional margins and rescaling it 
\begin{equation}
  \begin{split} 
   \max_{\gamma,w,b} \ &\frac12 ||w||^2\\ 
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq 1, \ i= 1, \dots, m \\
  \end{split}
\end{equation}
- It is called the *optimal margin classifier*

** Lagrange duality 
- Consider a problem of the following form:
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}
- The *Lagrangian* is defined to be 
\begin{equation}
  \mathcal L(w,\beta) = f(w) + \sum_{i=1}^l\beta_ih_i(w)
\end{equation}
- The $\beta_i$'s are called the *Lagrange multipliers*
	- We would the find and set $\mathcal L$'s partial derivatives to zero
\begin{equation}
  \frac{\partial \mathcal L}{\partial w_i} = 0; \frac{\partial \mathcal L}{\partial \beta_i} = 0
\end{equation}
and solve for $w$ and $\beta$ 

- The *primal* optimization problem is the following 
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ g_i(w) \leq 0, \ i=1,\dots, k
								&\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}

- The primal optimization problem is solved by defining the *generalized Lagrangian* 
\begin{equation}
  \mathcal L (w,\alpha,\beta) = f(w) + \sum_{i=1}^k \alpha_ig_i(w) + \sum_{i=1}^k \beta_ih_i(w)
\end{equation}
- The $\alpha_i$'s and $\beta_i$'s are the Lagrange multipliers.
- Consider the quantity
\begin{equation}
    \theta_\mathcal{P}(w) = \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
- If $w$ violates any of the primal constraints then one should be able to verify that 
\begin{equation}
    \theta_\mathcal P (w) = \infty
\end{equation}
- If $w$ does not violate the constraints then $\theta_\mathcal P(w) = f(w)$ and therefore the minimization problem
\begin{equation}
  \min_w\theta_\mathcal P(w) = \min_w \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
- is the same as the original problem
	- the objective is called the *value* of the primal problem and is denoted $p* = \min_w \theta _ \mathcal P (w)$

- If one define 
\begin{equation}
    \theta_D(\alpha,\beta) = \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
- The "$\mathcal D$" subscript stands for dual
	- This can be used to pose the *dual* optimization problem
\begin{equation}
  \max_{\alpha,\beta: \alpha_i  \geq 0} \theta_\mathcal D (\alpha, \beta) = \max_{\alpha,\beta: \alpha_i  \geq 0} \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
- Which is exactly the same as the primal problem, except the order of the min and max has been exchanged
	- The solution to the dual problem is defined as $d^*$
	- It holds that $d^* \leq p^*$

- The KKT conditions on $w^*$, $\alpha^*$ and $\beta^*$ 
[[file:Support Vector Machines/screenshot_2018-09-24_17-52-15.png]]

- If some $w^*$, $\alpha^*$, $\beta^*$ satisfy the KKT conditions they are also a solution to the primal and dual problems

** Optimal margin classifiers 
- By using the KKT conditions obtain the following optimization problem, which gives us a decision boundary with the largest margins:
[[file:Support Vector Machines/screenshot_2018-09-25_07-46-53.png]]

- Where $w= \sum_{i=1}^m a_iy^{(i)}x^{(i)}$ 
	- The prediction $w^Tx+b$ can also be written as
[[file:Support Vector Machines/screenshot_2018-09-25_07-50-38.png]]
	- Where post of the inner products will be zero except for the support vectors

** Kernels
- Given a feature mapping $\phi$ we define the corresponding *Kernel* to be 
\begin{equation}
  K(x,z) = \phi(x)^T\phi(z)
\end{equation}
- Everywhere we previously had $\langle x, z \rangle$ we could simple replace it with $K(x,z)$ and the algorithm would now be learning using the features $\phi$  

- The *Gaussian kernel* which corresponds to an infinite dimensional feature mapping $\phi$ 
\begin{equation}
  K(x,z) = \exp(-\frac{||x-z||^2}{2\sigma^2})
\end{equation} 

- The matrix called the *Kernel matrix* is defined from some $m$ data points $\{x^{(1)}, \dots, x^{(m)}\}$ as the m-by-m matrix $K$ where the $(i,j)$ entry is given by $K_{ij}=K(x^{(i)},y^{(i)})$

- *Theorem (Mercer).* Let $K: \mathbb R^n \times \mathbb R^n \mapsto \mathbb R$ be given. Then for $K$ to be a valid (Mercer) kernel, it is necessary and sufficient that for any $\{x^{(1)}, \dots, x^{(m)}\}$, $m < \infty$, the corresponding kernel matrix is symmetric positive semi-definite 

** Regularization and the non-separable case
- To make the algorithm work for non-linearly separable datasets and being less sensitive to outliers, the optimization can be reformulated as follows using regularization:
[[file:Support Vector Machines/screenshot_2018-09-25_08-30-17.png]]
- Which means that examples are now permitted to have a functional margin less than 1

- By using some of the KKT conditions one can obtain the following dual form of problem 
[[file:Support Vector Machines/screenshot_2018-09-25_08-36-39.png]]

** The SMO algorithm
*** General
- The SMO algorithm gives an efficient way of solving the dual problem arising from the derivation of the SVM

*** Coordinate ascent 
#+NAME: coordinateAscent
#+CAPTION: Coordinate ascent example
[[file:Support Vector Machines/screenshot_2018-09-25_08-44-24.png]]

- If one is trying to solve the unconstrained optimization problem 
\begin{equation}
  \max_\alpha W(\alpha_1,\alpha_2, \dots,\alpha_m)
\end{equation}
- One can use the algorithm called *coordinate ascent:* 
[[file:Support Vector Machines/screenshot_2018-09-25_08-47-20.png]]
- Where one holds all the variables fixed except some $a_i$ 

*** SMO
- The SMO algorithm does the following 
[[file:Support Vector Machines/screenshot_2018-09-25_09-00-29.png]]
- To test for convergence, one can test whether the KKT conditions are satisfied within some /tol/
	- The tol is the convergence tolerance parameter normally set around $0.01$ to $0.001$ 
* Deep Feedforward Networks
** General 
- *Deep feedforward networks* are quintessential deep learning models 
	- The goal of a feedforward network is to approximate some function $f^*$
	- It defines a mapping $\pmb y = f(\pmb x; \pmb \theta)$ and learns the value of the parameters $\pmb \theta$ that result in the best function approximation

- It is called *feedforward* since information flows through the function being evaluated from $\pmb x$, through intermediate computations used to define $f$ and finally to the output $\pmb y$
	- When feedforward neural networks are extended to include feedback connections, they are called *recurrent neural networks*

- They are called *networks* because they typically are represented by composing together many different function
	- It is associated with a DAG describing ow the functions are composed together
	- The different functions are called *layers*
	- The overall length of the function chain gives the *depth* of the model
	- The final layer is called the *output layer*
	- During the training we drive $f(\pmb x)$ to match $f^*(\pmb x)$
	- The training data provides us with noisy, approximate examples of $f^*(\pmb x)$ evaluated at different training points
	- The training data does not say what each individual layers should do
		- That is the training algorithms job
		- They are called *hidden layers*
	- The dimensionality of these hidden layers determines the *width* of the model

- The strategy of deep learning is the feature transform $\phi$
	- We have a model $=f(\pmb x; \pmb \theta, \pmb w) = \phi(\pmb x, \pmb \theta)^\top \pmb w$
	- We have the parameters $\pmb \theta$ that we use to learn $\phi$ from a broad class of functions
	- We have the parameters $\pmb w$ that map from $\phi(\pmb x)$ to the desired output

- Training a feedforward network requires making many of the same design decisions as are necessary for a linear model:
	- Choosing the optimizer
	- The cost function
	- The form of the output units. 

** Gradient-Based Learning 
*** General
- The non-linearity of a neural network causes most interesting loss functions to become non-convex
	- It means that NNs are usually are trained by using iterative, gradient-based optimizes that merely drive the cost function to a very low value
	- It is important to initialize all weights to random values becomes of the error function being non-convex, when using stochastic gradient descent 
		- The biases may be initialized to zero or a small positive values
	
*** Cost Functions 
**** General
- Cost functions for neural network are more or less the same as those for other models, such as linear models
	- The total cost function used to train a neural network will often combine one of the primary cost functions with a regularization term

**** Learning Conditional Distributions with Maximum Likelihood
- Most NNs are trained using maximum likelihood
	- The cost function is simply the NLL which is equivalently described as the cross-entropy between the training data and the model distribution 
	- This cost function is given by:
[[file:Deep Feedforward Networks/screenshot_2018-09-30_16-58-07.png]]
- The advantage of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model
	- Specifying a model $p(\pmb y \mid \pmb x)$ automatically determines a cost function $\log p(\pmb y \mid \pmb x)$

- Instead of learning a full probability distribution $p(\pmb y \mid \pmb x; \pmb \theta)$, we often want to learn just one conditional statistic of $\pmb y$ given $\pmb x$
	- Such as predicting the mean of $\pmb y$ given the predictor $f(\pmb x; \pmb \theta)$ 

**** Learning Conditional Statistics
- Instead of learning a full probability distribution $p(\pmb y \mid \pmb x; \pmb \theta)$ one often want to learn just one condition statistic of $\pmb y$ given $\pmb x$
	- Such as predicting the mean of $\pmb y$ 
	- The cost function can be viewed as being a *functional* rather than just a function
		- A mapping from functions to real numbers
	- The cost functional can be designed to have its minimum occur at some specific function we desire

- Solving the optimization problem
\begin{equation}
  f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||^2
\end{equation}
yields
\begin{equation}
  f^*(\pmb x) = \mathbb E_{\pmb y \sim p_{data}(\pmb y \mid x)[\pmb y]}
\end{equation}

- The following function yields a function that predicts the /median/ value of $\pmb y$ for each $\pmb x$ 
\begin{equation}
	f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||_1
\end{equation}
- This cost function is commonly call *mean absolute error*

*** Output Units
**** General 
- The choice of cost function is tightly coupled with the choice of output unit
	- Most of the time one simply uses the cross-entropy between the data distribution and the model distribution
	- The choice of how to represent the output determines the cross-entropy function
	- Any kind of neural network that may be used as output can also be used as a hidden unit

- The hidden features is defined by $\pmb h = f(\pmb x ; \pmb \theta)$
	- The role of the output layer is to provide some additional transformation from the features to complete the task that the network must perform

**** Linear Units for Gaussian Output Distributions
- Linear units is an output unit based on an affine transformation with no non-linearity
	- Given features $\pmb h$, a layer of linear output units produces a vector $\hat{\pmb y}= \pmb W^T \pmb h + b$
	- Linear output layers are often used to produce the mean of a conditional Gaussian distribution
\begin{equation}
  p(\pmb y \mid \pmb x) = \mathcal N(\pmb y; \hat{\pmb y}, \pmb I)
\end{equation}
- Maximizing the log-likelihood is the equivalent to minimizing the mean squared error 

** Hidden Units
*** General
- Some valid hidden units are not differentiable at all input points
	- Such as $g(z)=\max\{0,z\}$
	- Solved by using the right or left differential 

*** Rectiﬁed Linear Units and Their Generalizations
- Rectified linear units use the activation function $g(z) = \max\{0,z\}$
	- Easy to optimize because they are similar to linear units
		- Only difference is that a rectified unit outputs zero across half its domain

- Rectified linear units are typically used on top of an affine transformation
\begin{equation}
  \pmb h = g(\pmb W^\top \pmb x + \pmb b)
\end{equation}
- When initializing the parameters of the affine transformation it can be a good pratise to set all elements of $\pmb b$ to a small positive value
- A drawback to rectified linear units is that they cannot learn via gradient based methods on examples for which their activation is zero
	- Some generalizations of rectified linear units guarantee that they receive gradient everywhere

- *Maxout units* generalize rectified linear units further
	- Instead of applying an element-wise function $g(z)$ maxout units divide $\pmb z$ into groups of $k$ values
	- Each maxout unit the outputs the maximum element of one of these groups
\begin{equation}
  g(\pmb z)_i = \max_{j\in \mathbb G^{(u)}} z_j
\end{equation}
- where $\mathbb G^{(I)}$ is the set of indices into the inputs for group $i$ 

- A maxout unit can be seen as learning the activation function itself rather than just the relationship between units

*** Logistic Sigmoid and Hyperbolic Tangent
- The widespread saturation of sigmoid units can make gradient-based learning very difficult
	- Their use as hidden units in feedforward networks are discouraged
	- Training using the $tanh$ function for hidden layers are easier

** Architecture Design
*** General
- *Architecture* refers to the overall structure of the network
	- How many units it should have
	- How these units should be connected to each other

- Most NNs are organized into groups of units called layers
	- They are often arranged in a chain structure where each layer is a function of the layer that preceded it
	- The ith layer is given by 
\begin{equation}
  h^{(i)} = g^{(i)}(\pmb W ^{(i)T}\pmb h^{(i)} + \pmb b^{(i)})
\end{equation}
- where the first layer uses $x$ instead of $h^{(i)}$ 

- In chain based architectures, the main architectural considerations are to choose the the depth of the network and the width of each layer 
	- The ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error.

*** Universal Approximation Properties and Depth
- The universal approximation theorem means that regardless of what function we are trying to learn, we know that a large multi layered perceptron will be able to represent this function.
	- However, we are not guaranteed that the training algorithm will be able to learn that function
	- Learning it can fail for two different reasons
		1. The optimization algorithm used for training may not be able to find the value of the parameters corresponds to the desired function
		2. The training algorithm might choose the wrong function due to overfitting

- Feedforward networks provide a universal system for representing functions, in the sense that, given a function, there exists a feedforward network that approximates the function.
	- There is no universal procedure for examining a training set of specific examplesd and choosing a function that will generalize to point not in the training set

- The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be. 

- A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.
	- In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error 

- The number of linear regions carved out by a deep rectifier network with $d$ inputs, depth $l$, and $n$ units per hidden layer is 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-14-50.png]]

- Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions.

*** Other Architectural Considerations
- The layers need not be connected in a chain, but it is the most common practice.
	- Many architectures build a main chain but then add extra architectural features to it
		- Such as skip connections going from layer i to layer i + 2 or higher.
	- These skip connections make it easier for the gradient to ﬂow from output layers to layers nearer the input.

- A key consideration of architecture design is how to connect a pair of layers to each other
	- The default way is having every input unit connected to every output unit
	- Strategies for reducing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network,
		- They are often highly problem-dependent.

** Back-Propagation and Other Differentiation Algorithms
*** General
- *Forward propagation* is when the inputs $\pmb x$ provide initial information that then propaget up to the hidden units at each layer and finally produces an output $\hat{\pmb y}$
	- During training it can continue onward until it produces a scalar cost $J(\pmb \theta)$ 

- The *back-propagation* algorithm (*backprop*) allows the information from the cost to flow backwards through the network in order to compute the gradient
	- It is used to compute the gradient, another algorithm is used to do the learning e.g. stochastic gradient descent

*** Computational Graphs
#+NAME: computationalGraphs
#+CAPTION: Computational Graph Examples
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-49-44.png]]

- An *operation* is a simple function of one or more variables
	- Defined to return only a single output variable

- The graph language is accompanied by a set of allowable operations
	- If a variables $y$ is computed by applying an operation to a variable $x$, then there is drawn a directed edge from $x$ to $y$
	- The output node is sometimes annotated with the name of the operation applied

*** Chain Rule of Calculus
- Back-propagation computes the chain rule, with a specific order of operations that is highly efficient 
- The chain rule can generalize beyond the scalar case suppose that $\pmb x \in \mathbb R^m, \pmb y \in \mathbb R^n$, $g$ maps from $\mathbb R^m$ to $\mathbb R^n$, and $f$ maps from $\mathbb R^n$ to $R$. If $\pmb y = g(\pmb x)$ and $z = f(\pmb y)$, then 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-55-43.png]]
- In vector notation, this may equivalently be written as 
[[file:Deep Feedforward Networks/screenshot_2018-10-01_17-56-15.png]]
- where $\frac{\partial \pmb y}{\partial \pmb x}$  is the $n \times m$ Jacobian matrix of $g$ 

- The back-propagation algorithm consists of performing Jacobian-gradient product given by the chain rule for each operation in the graph
- The back-propagation algorithm is typically applied to tensors of arbitrary dimensionality
	- Is exactly the same as back-propagation with vector conceptually
	- Denoting a gradient of a value $z$ with respoect to a tensor $s.
- To denote the gradient of a value z with respect to a tensor $\pmb X$, we write $\nabla_{\pmb X} z$
	- The indices into $\pmb X$ have multiple coordinates

- The chain rule for tensors:
[[file:Deep Feedforward Networks/screenshot_2018-10-01_18-22-42.png]]

*** Recursively Applying the Chain Rule to Obtain Backprop
- Given a scalar $u^{(n)}$ which is the quantity whose gradient we want to obtain with respect to all the $n_i$ input nodes $u^{(1)}$ to $u^{(n_i)}$
	- We wish to compute $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ for $i \in \{1,2,\dots,n_i\}$
	- In backprop $u^{(n)}$ will be the cost associated with an example or a minibatch
	- $u^{(1)}$ to $u^{(n_i)}$ correspond to the parameters of the model
	- The nodes of the graph is assumed to be order in such a way that we can compute their output one after the other
	- Each node $u^{(i)}$ is associated with an operation $f^{(i)}$ and is computed by evaluating the function
\begin{equation}
  u^{(i)} = f(\mathbb A ^{(i)})
\end{equation}
- where $\mathbb A ^{(i)}$ is the set of all nodes that are parents of $u^{(i)}$ 
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-04-33.png]]

- The forward propagation computation is put in a graph $\mathcal G$
	- In order to perform back-propagation, one can constructs a computational graph that depends on $\mathcal G$ and add to it an extra set of nodes
		- These form a subgraph $\mathcal B$ with one node per node of $\mathcal G$
		- Computation in $\mathcal B$ proceeds in the reverse of the order of computation in $\mathcal G$
		- Each node of $\mathcal B$ computes the derivative $\frac{\partial u^{(n)}}{\partial u^{i}}$ associated with the forward graph node $u^{(i)}$ using the chain rule
	- The subgraph $\mathcal B$ contains one edge for each edge for each edge of $\mathcal G$
		- The edge from $u^{(j)}$ to $u^{(i)}$ is associated with the computation of $\frac{\partial u^{(i)}}{\partial u ^{(j)}}$
		- The dot product is performed for each node between the gradient already computed with respect to nodes $u^{(i)}$ that are children of $u^{(j)}$ and the vector containing the partial derivatives $\frac{\partial u^{(i)}}{\partial u ^{(j)}}$ for the same children nodes

- The amount of computation required for performing back-propagration scales linearly with the number of edges in $\mathcal G$  

[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-18-42.png]]

*** Back-Propagation Computation in Fully-Connected MLP
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-22-46.png]]

[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-23-03.png]]

*** Symbol-to-Symbol Derivatives
- *Symbolic Representations* is algebraic expressions and computational graphs that both operate on symbols, or variables that do not have speciﬁc values.

- Some approaches to back-propagation take a computational graph and a set of numerical values for the inputs to the graph, then return a set of numerical values describing the gradient at those input values.
	- This is called "symbol-to-number" diﬀerentiation

- Another approach for backprop is to take a computational graph and add additional nodes to the graph that provide a symbolic description of the desired derivativess

*** General Back-Propagation
- Each node in the graph $\mathcal G$ corresponds to a variable
	- This is described as being a tensor $\mathbf{\mathsf V}$
	- Tensor can in general have any number of dimensions
	- They subsume scalars, vectors, and matrices

- It is assumed that each variable $\mathbf{\mathsf V}$ is associated with the following subroutines:
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-51-44.png]] 

- Each operation ~op~ is also associated with a ~bprop~ operation
	- This ~bprop~ operation can compute a Jacobian vector product
	- Formally, ~op.bprop(inputs,X,G)~ must return:
[[file:Deep Feedforward Networks/screenshot_2018-10-02_08-58-55.png]]
- Inputs is a list of inputs that are supplied to the operation
- ~op.f~ is the mathematical function that the operation implements
- $\mathbf{\mathsf X}$ is the input whose gradient we which to compute
- $\mathbf{\mathsf G}$ is the gradient on the output of the operation

[[file:Deep Feedforward Networks/screenshot_2018-10-02_09-04-05.png]]

[[file:Deep Feedforward Networks/screenshot_2018-10-02_09-04-43.png]]

- The backprop algorithm uses dynamic programming to get a better running time

* Convolutional Networks
** General
- *Convolutional Networks* (CNNs) are a specialized kind of neural network for processing data that has a grid-like topology
	- *Convolution* is a specialized kind of linear operation
	- Convolutional networks are neural networks that use convolution in place of general matrix multiplication in at least one of their layers

** The Convolution Operation
- A convolution is in its most general form an operation on two functions of a real valued argument
	- Example of a convolution: $s(t) \int x(a)w(t-a)da$
	- The convolution operation is typically denoted with an asterisk: $s(t) = (x*w)(t)$
	- The first argument to the convolution is often referred to as the *input*
	- The second argument is referred to as the *kernel*
	- The output is referred to as the *feature map*
	- In machine learning applications
		- The input is usually a multidimensional array of data
		- The kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm.
		- The multidimensional dimensional arrays are referred to as tensors
		- Because each element of the input and kernel must be explicitly stored separately, it is often assumed that these functions are zero everywhere but in the finite set of points for which we store the value
	- Convolutions are often used over more than one axis at a time
		- e.g. on a two-dimensional image $I$ as input one would probably use a two dimensional kernel $K$
	- Convolution is commutative which means that for a two dimensional kernel $K$ and a input $I$: $S(i,j) = (I*K)(i,j)=(K*I)(i,j)$
		- The last one is usually more straightforward to implement in a machine learning library, since there is less variation in the range of valid values for $m$ and $n$
	- Many neural network libraries implement are related function called *cross-correlation*, which is the same as convolution but without flipping the kernel
		- e.g. $S(i,j) = (K*I)(i,j)= \sum_m\sum_nI(i+m,j+n)K(m,n)$
		- Some also call this convolution

** Motivation 
- Convolution leverages three important ideas that can help improve a machine learning system:
	- *Sparse interactions* is accomplished by making the kernel smaller than the input
		- It is also referred to as *parse connectivity* *or *sparse weights*
		- e.g. one can detect small meaningful features in images such as edges with kernels
		- One needs to store fewer parameters which reduces the memory requirements of the model and improves its statistical efficiency
			- Computing the output requires fewer operations
			- The improvements in efficiency are usually quite large 

	- *Parameter sharing:* refers to using the same parameter for more than one function in the model
		- One can say that a network has *tied weights*, because the value of the weight applied to one input is tied to the value of a weight applied elsewhere
		- Each member of the kernel is used at every position of the input

	- *Equivariant representations*: If $g$ is any function that translates the input then that is shifts is, then the convolution function is equivalent to $g$ 
		- A function $f$ is equivalent to a function $g$ if $f(g(x))=g(f(x))$

- Some kinds of data cannot be processed by neural networks deﬁned bymatrix multiplication with a ﬁxed-shape matrix. Convolution enables processing of some of these kinds of data.

** Pooling 
[[file:Convolutional Networks/screenshot_2018-10-08_10-23-35.png]]

- A typical layer of a convolutional network consists of three stages
	1. In the first stage the layer performs several convolutions in parallel to produce a set of linear activations
	2. In the second stage each linear activation is run through a nonlinear activation, such as rectified linear activation function
		 - Is sometimes called the *detector stage*
	3. The third stage we use a *pooling function* to modify the output of the layer further
		 - It replaces the output of the net at a certain location with a summary statistic of the nearby output
			 - e.g. the *max pooling operation* which reports the maximum output within a rectangular neighborhood
		 - Pooling helps to make the representation approximately *invariant* to small translation of the input
			 - Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs
			 - Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.
		 - It is possible to use fewer pooling units than detector units by reporting summary statistics for pooling regions spaced $k$ pixels apart rather than $1$ pixels apart
			 - Improves computational efficiency of the network because the next layer has approximately $k$ times fewer inputs to process
			 - Can be used to handle images of variable size by changing how much it is space depending on the input size 

* Tree-Based Methods
** Background 
- Three based methods partition the feature space into a set of rectables and then fit a simple model in each one
	- They are simple yet powerful
	- One first split the space into two regions and models the response by the mean of $Y$ in each region, then one or both of the regions are split into two more regions, this process is continued until some stopping rule is applied

** Regression Trees
- The data consists of $p$ inputs and a response, for each of $N$ observations, that is $(x_i,y_i)$ for $i=1,2,\dots,N$, with $x_i=(x_{i1}, x_{i2}, \dots, x_{ip})$
- The algorithm needs to automatically decide on the splitting variables and split points and what topology (shape) the three should have
- If one have a partition into $M$ regions $R_1,R_2,\dots,R_M$, and model the response as a constant $c_m$ in each region:
\begin{equation}
  f(x) = \sum_{m=1}^Mc_mI(x \in R_m)
\end{equation}
- If the criterion is minimization of the sum of squares $\sum(y_i-f(x_i))^2$, he best $\hat c_m$ is just the average of $y_i$ in the region $R_m$: 
\begin{equation}
  \hat c_m = \text{ave}(y_i \mid x_i \in R_m ).
\end{equation}
- Since the binary partition in terms of minimum sum of squares is generally computationally infeasible one needs a greedy algorithm:
[[file:Tree-Based Methods/screenshot_2018-10-08_16-15-20.png]]
- For each splitting variable, the determination of the split point $s$ can be done very quickly by scanning through all the inputs
	- Having found the best split, one partition the data into the two resulting regions and repeat the splitting process on each of the two regions, which is the repeat on all the resulting regions
	- The optimal tree size should be adaptively chosen from the data
		- A preferred strategy is to grow a large $T_0$ stopping the splitting process only when some minimum node size is reached
		- The large tree is then pruned using cost-complexity pruning

- A *subtree* $T \subset T_0$ is defined to be any tree that can be obtained by *pruning* $T_0$
	- Pruning is collapsing any number of its internal (non-terminal nodes).
	- Terminal nodes is indexed by $m$, with node $m$ representing region $R_m$ 
	- $|T|$ denotes the number of terminal nodes in $T$ 
- Letting
[[file:Tree-Based Methods/screenshot_2018-10-08_16-27-26.png]]
- The idea is to find for each $\alpha$ the subtree $T_\alpha \subseteq T_0$ to minimize $C_\alpha(T)$
	- The tuning parameter $\alpha \geq 0$ governs the tradeof between tree size and its goodness of fit to the data
	- Larger values of $\alpha$ result in smaller trees $T_\alpha$ and the converse for smaller values of $\alpha$
	- With $\alpha = 0$ the solution is the full tree $T_0$
	- For each $\alpha$ there is a unique smallest subtree $T_\alpha$ that minimizes $C_\alpha(T)$ 

- If one want to find $T_\alpha$ one uses *weakest link pruning*:
	- One successively collapse the internal node that produces the smallest per-node increase in $\sum_m N_mQ_m(T)$ until we produce P and continue until we produce the single-node (root) tree.
	- This gives a finite sequence of subtrees and $T_\alpha$ must be one of these subtrees
	- Estimation of $\alpha$ is achieved by five-fold or ten-fold cross-validation:
		- The value $\hat \alpha$ is chosen to minimize the cross-validated sum of squares
		- The final tree is $T_{\hat \alpha}$ 

** Classification Trees 
[[file:Tree-Based Methods/screenshot_2018-10-08_16-49-37.png]]


- If the target is classification outcome taking values $1,2,\dots, K$ the only changes needed is the tree algorithm is the criteria for splitting nodes and pruning the tree
	- In a nodes $m$, representing a region $R_m$ with $N_m$ observations, let 
[[file:Tree-Based Methods/screenshot_2018-10-08_16-47-14.png]]
- The Gini index and cross-entropy are differentiable and are therefore more amenable to numerical optimization
	- They are often used for growing the three
	- To guide the cost-complexity pruning any of the three measures can be used but typically it is the misclassification rate 

** Other Issues
- *Categorial Predictors*
	- When splitting a predictor having $q$ possible unordered values, there are $2^{q-1}-1$ possible partitions of the $q$ values into two groups
		- The computations become prohibitive for large $q$
	- With a $0-1$  outcome the computation simplifies
		- One orders the predictor classes according to the proportion falling in outcome class $1$
		- This gives the optimal split in terms of cross-entropy or Gini index- among all possible $2^{q-1}-1$ splits

- *The Loss Matrix*: A $KxK$ loss matrix $\pmb L$, is defined with $L_{kk'}$ being the loss incurred for classifying a class $k$ observation as class $k'$
	- Typically no loss is incurred for correct classifications, that is $L_{kk}= 0 \ \forall k$
	- To incorporate P the losses into the modeling process, one could modify the Gini index to $\sum_{k\ne k'} L_{kk'} \hat p_{mk} \hat p_{mk'}$
	- This does not help in the two-class case and a better approch is to weight the observations in class $k$ by $L_{kk'}$ 

- *Missing Predictor Values:* If some of the data has some missing predictor values in some of the values
	- There are two better approaches than throwing the data away
		1. The first is applicable to variable predictors, where one simply makes a new category for /"missing"/
			 - This might make one discover that the observation with missing values for some measurement behave differently that those with nonmissing values
		2. The second is a more general approach which is the construction of surrogate variables
			 - When considering a predictor for a split, we use only the observations for which that predictor is not missing
			 - Having chosen the best (primary) predictor and split point, we form a list of surrogate predictors and split points.
				 - The first surrogate is the predictor and corresponding split point that best mimics the split of the training data achieved by the primary split.
				 - The second surrogate is the predictor and corresponding split point that does second best, and so on. 

- *Why Binary Splits?*
	- The problem with using multiway splits is that it fragment the data too quickly, leaving insufficient data at the next level down.
	- Since multiway splits can be achieved by a series of binary splits, the binary splits are preferred.

* Random forests 
** Introduction 
- *Bagging* or *bootstrap aggregation* is a technique for reducing the variance of an estimated prediction function
	- Bagging works seems to work especially well for high-variance, low-ias procedures such as trees
	- For regression we simply fit the same regression tree many time to bootstrapsampled versions of the training data and average the result
	- For classification, a *committee* of trees each cast a vote for the predicted class

- *Random forests* is a substantial modification of bagging that builds a large collection of de-correlated trees and averages them
	- On many problems its performace is very similar to boosting
	- They are simpler to train and tune than the boosting example 

** Definition of Random Forests 
[[file:Random forests/screenshot_2018-10-08_20-10-57.png]]

- The essential idea in bagging is to average many noisy but approximately unbiased models
	- This reduces the variance
	- Trees are ideal candidates for bagging since:
		- They can capture complex interaction structures in the data
		- If grown sufficiently deep, they have relative low bias
	- Since trees are very noisy they benefit greatly from averaging 
	- The bias of bagged trees is the same as that of the individual (bootstrap) trees
		- The only hope of improvement is through variance reduction
		- 
