\BOOKMARK [1][-]{section.1}{TA Instructor Info}{}% 1
\BOOKMARK [1][-]{section.2}{Important}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Dictionary}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Jacobian}{section.2}% 4
\BOOKMARK [1][-]{section.3}{The Learning Problem \(1\)}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Components of Learning}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.2}{A Simple Learning Model}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.3}{Types of Learning}{section.3}% 8
\BOOKMARK [3][-]{subsubsection.3.3.1}{Supervised Learning}{subsection.3.3}% 9
\BOOKMARK [3][-]{subsubsection.3.3.2}{Reinforcement Learning}{subsection.3.3}% 10
\BOOKMARK [3][-]{subsubsection.3.3.3}{Unsupervised Learning}{subsection.3.3}% 11
\BOOKMARK [2][-]{subsection.3.4}{Linear Regression and Orthogonal Projections}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.5}{Is Learning Feasible}{section.3}% 13
\BOOKMARK [3][-]{subsubsection.3.5.1}{General}{subsection.3.5}% 14
\BOOKMARK [3][-]{subsubsection.3.5.2}{Feasibility of Learning}{subsection.3.5}% 15
\BOOKMARK [2][-]{subsection.3.6}{Error and Noise}{section.3}% 16
\BOOKMARK [3][-]{subsubsection.3.6.1}{Error Measures}{subsection.3.6}% 17
\BOOKMARK [3][-]{subsubsection.3.6.2}{Noisy target}{subsection.3.6}% 18
\BOOKMARK [1][-]{section.4}{Training versus Testing \(2\)}{}% 19
\BOOKMARK [2][-]{subsection.4.1}{Theory of Generalization}{section.4}% 20
\BOOKMARK [3][-]{subsubsection.4.1.1}{General}{subsection.4.1}% 21
\BOOKMARK [3][-]{subsubsection.4.1.2}{Effective Number of Hypotheses}{subsection.4.1}% 22
\BOOKMARK [3][-]{subsubsection.4.1.3}{Bounding the growth function}{subsection.4.1}% 23
\BOOKMARK [3][-]{subsubsection.4.1.4}{The VC Dimension}{subsection.4.1}% 24
\BOOKMARK [3][-]{subsubsection.4.1.5}{The VC Generalization Bound}{subsection.4.1}% 25
\BOOKMARK [2][-]{subsection.4.2}{Interpreting the Generalization Bound}{section.4}% 26
\BOOKMARK [3][-]{subsubsection.4.2.1}{General}{subsection.4.2}% 27
\BOOKMARK [3][-]{subsubsection.4.2.2}{Sample Complexity}{subsection.4.2}% 28
\BOOKMARK [3][-]{subsubsection.4.2.3}{Penalty for Model Complexity}{subsection.4.2}% 29
\BOOKMARK [3][-]{subsubsection.4.2.4}{The Test Set}{subsection.4.2}% 30
\BOOKMARK [2][-]{subsection.4.3}{Approximation-Generalization Tradeoff}{section.4}% 31
\BOOKMARK [3][-]{subsubsection.4.3.1}{Bias and Variance}{subsection.4.3}% 32
\BOOKMARK [3][-]{subsubsection.4.3.2}{The Learning Curve}{subsection.4.3}% 33
\BOOKMARK [1][-]{section.5}{The Linear Model \(3\)}{}% 34
\BOOKMARK [2][-]{subsection.5.1}{Linear Regression}{section.5}% 35
\BOOKMARK [3][-]{subsubsection.5.1.1}{The algorithm}{subsection.5.1}% 36
\BOOKMARK [2][-]{subsection.5.2}{Logistic Regression}{section.5}% 37
\BOOKMARK [3][-]{subsubsection.5.2.1}{Predicting a Probability}{subsection.5.2}% 38
\BOOKMARK [3][-]{subsubsection.5.2.2}{Gradient Descent}{subsection.5.2}% 39
\BOOKMARK [2][-]{subsection.5.3}{Nonlinear Transformation}{section.5}% 40
\BOOKMARK [3][-]{subsubsection.5.3.1}{The Z space}{subsection.5.3}% 41
\BOOKMARK [3][-]{subsubsection.5.3.2}{Computation and generalization}{subsection.5.3}% 42
\BOOKMARK [1][-]{section.6}{Multinomial/Softmax Regression}{}% 43
\BOOKMARK [2][-]{subsection.6.1}{Setup}{section.6}% 44
\BOOKMARK [2][-]{subsection.6.2}{Probabilistic Outputs}{section.6}% 45
\BOOKMARK [2][-]{subsection.6.3}{The Negative Log Likelihood}{section.6}% 46
\BOOKMARK [2][-]{subsection.6.4}{Implementation Issues}{section.6}% 47
\BOOKMARK [3][-]{subsubsection.6.4.1}{Numerical Issues with Softmax}{subsection.6.4}% 48
\BOOKMARK [3][-]{subsubsection.6.4.2}{One in k encoding}{subsection.6.4}% 49
\BOOKMARK [3][-]{subsubsection.6.4.3}{Always check your shapes}{subsection.6.4}% 50
\BOOKMARK [3][-]{subsubsection.6.4.4}{Bias Variable}{subsection.6.4}% 51
\BOOKMARK [1][-]{section.7}{Overfitting \(4\)}{}% 52
\BOOKMARK [2][-]{subsection.7.1}{When Does Overfitting Occur?}{section.7}% 53
\BOOKMARK [3][-]{subsubsection.7.1.1}{General}{subsection.7.1}% 54
\BOOKMARK [3][-]{subsubsection.7.1.2}{Catalysts for Overfitting}{subsection.7.1}% 55
\BOOKMARK [2][-]{subsection.7.2}{Regularization}{section.7}% 56
\BOOKMARK [3][-]{subsubsection.7.2.1}{General}{subsection.7.2}% 57
\BOOKMARK [3][-]{subsubsection.7.2.2}{A Soft Order Constraint}{subsection.7.2}% 58
\BOOKMARK [3][-]{subsubsection.7.2.3}{Weight Decay and Augmented Error}{subsection.7.2}% 59
\BOOKMARK [3][-]{subsubsection.7.2.4}{Choosing a Regularizer}{subsection.7.2}% 60
\BOOKMARK [2][-]{subsection.7.3}{Validation}{section.7}% 61
\BOOKMARK [3][-]{subsubsection.7.3.1}{The Validation Set}{subsection.7.3}% 62
\BOOKMARK [3][-]{subsubsection.7.3.2}{Model Selection}{subsection.7.3}% 63
\BOOKMARK [3][-]{subsubsection.7.3.3}{Cross Validation}{subsection.7.3}% 64
\BOOKMARK [1][-]{section.8}{Support Vector Machines}{}% 65
\BOOKMARK [2][-]{subsection.8.1}{Notation}{section.8}% 66
\BOOKMARK [2][-]{subsection.8.2}{Functional and geometric margins}{section.8}% 67
\BOOKMARK [2][-]{subsection.8.3}{The optimal margin classifier}{section.8}% 68
\BOOKMARK [2][-]{subsection.8.4}{Lagrange duality}{section.8}% 69
\BOOKMARK [2][-]{subsection.8.5}{Optimal margin classifiers}{section.8}% 70
\BOOKMARK [2][-]{subsection.8.6}{Kernels}{section.8}% 71
\BOOKMARK [2][-]{subsection.8.7}{Regularization and the non-separable case}{section.8}% 72
\BOOKMARK [2][-]{subsection.8.8}{The SMO algorithm}{section.8}% 73
\BOOKMARK [3][-]{subsubsection.8.8.1}{General}{subsection.8.8}% 74
\BOOKMARK [3][-]{subsubsection.8.8.2}{Coordinate ascent}{subsection.8.8}% 75
\BOOKMARK [3][-]{subsubsection.8.8.3}{SMO}{subsection.8.8}% 76
\BOOKMARK [1][-]{section.9}{Deep Feedforward Networks}{}% 77
\BOOKMARK [2][-]{subsection.9.1}{General}{section.9}% 78
\BOOKMARK [2][-]{subsection.9.2}{Gradient-Based Learning}{section.9}% 79
\BOOKMARK [3][-]{subsubsection.9.2.1}{General}{subsection.9.2}% 80
\BOOKMARK [3][-]{subsubsection.9.2.2}{Cost Functions}{subsection.9.2}% 81
\BOOKMARK [3][-]{subsubsection.9.2.3}{Output Units}{subsection.9.2}% 82
\BOOKMARK [2][-]{subsection.9.3}{Hidden Units}{section.9}% 83
\BOOKMARK [3][-]{subsubsection.9.3.1}{General}{subsection.9.3}% 84
\BOOKMARK [3][-]{subsubsection.9.3.2}{Rectied Linear Units and Their Generalizations}{subsection.9.3}% 85
\BOOKMARK [3][-]{subsubsection.9.3.3}{Logistic Sigmoid and Hyperbolic Tangent}{subsection.9.3}% 86
\BOOKMARK [2][-]{subsection.9.4}{Architecture Design}{section.9}% 87
\BOOKMARK [3][-]{subsubsection.9.4.1}{General}{subsection.9.4}% 88
\BOOKMARK [3][-]{subsubsection.9.4.2}{Universal Approximation Properties and Depth}{subsection.9.4}% 89
\BOOKMARK [3][-]{subsubsection.9.4.3}{Other Architectural Considerations}{subsection.9.4}% 90
\BOOKMARK [2][-]{subsection.9.5}{Back-Propagation and Other Differentiation Algorithms}{section.9}% 91
\BOOKMARK [3][-]{subsubsection.9.5.1}{General}{subsection.9.5}% 92
\BOOKMARK [3][-]{subsubsection.9.5.2}{Computational Graphs}{subsection.9.5}% 93
\BOOKMARK [3][-]{subsubsection.9.5.3}{Chain Rule of Calculus}{subsection.9.5}% 94
\BOOKMARK [3][-]{subsubsection.9.5.4}{Recursively Applying the Chain Rule to Obtain Backprop}{subsection.9.5}% 95
\BOOKMARK [3][-]{subsubsection.9.5.5}{Back-Propagation Computation in Fully-Connected MLP}{subsection.9.5}% 96
\BOOKMARK [3][-]{subsubsection.9.5.6}{Symbol-to-Symbol Derivatives}{subsection.9.5}% 97
\BOOKMARK [3][-]{subsubsection.9.5.7}{General Back-Propagation}{subsection.9.5}% 98
\BOOKMARK [2][-]{subsection.9.6}{Backpropagation equations}{section.9}% 99
\BOOKMARK [1][-]{section.10}{Convolutional Networks}{}% 100
\BOOKMARK [2][-]{subsection.10.1}{General}{section.10}% 101
\BOOKMARK [2][-]{subsection.10.2}{The Convolution Operation}{section.10}% 102
\BOOKMARK [2][-]{subsection.10.3}{Motivation}{section.10}% 103
\BOOKMARK [2][-]{subsection.10.4}{Pooling}{section.10}% 104
\BOOKMARK [1][-]{section.11}{Tree-Based Methods}{}% 105
\BOOKMARK [2][-]{subsection.11.1}{Background}{section.11}% 106
\BOOKMARK [2][-]{subsection.11.2}{Regression Trees}{section.11}% 107
\BOOKMARK [2][-]{subsection.11.3}{Classification Trees}{section.11}% 108
\BOOKMARK [2][-]{subsection.11.4}{Other Issues}{section.11}% 109
\BOOKMARK [1][-]{section.12}{Random forests}{}% 110
\BOOKMARK [2][-]{subsection.12.1}{Introduction}{section.12}% 111
\BOOKMARK [2][-]{subsection.12.2}{Bootstrap aggregating technique}{section.12}% 112
\BOOKMARK [2][-]{subsection.12.3}{Definition of Random Forests}{section.12}% 113
\BOOKMARK [1][-]{section.13}{Boosting and Additive Trees}{}% 114
\BOOKMARK [2][-]{subsection.13.1}{Boosting Methods}{section.13}% 115
\BOOKMARK [2][-]{subsection.13.2}{Boosting Fits an Additive Model}{section.13}% 116
\BOOKMARK [2][-]{subsection.13.3}{Forward Stagewise Additive Modeling}{section.13}% 117
\BOOKMARK [2][-]{subsection.13.4}{Exponential Lost and AdaBoost}{section.13}% 118
\BOOKMARK [2][-]{subsection.13.5}{Why Exponential Loss?}{section.13}% 119
\BOOKMARK [2][-]{subsection.13.6}{Loss Functions and Robustness}{section.13}% 120
\BOOKMARK [3][-]{subsubsection.13.6.1}{Robust Loss Functions for Classification}{subsection.13.6}% 121
\BOOKMARK [3][-]{subsubsection.13.6.2}{Robust Loss Functions for Regression}{subsection.13.6}% 122
\BOOKMARK [2][-]{subsection.13.7}{"Off-the-Shelf" Procedures for Data Mining}{section.13}% 123
\BOOKMARK [2][-]{subsection.13.8}{Boosting Trees}{section.13}% 124
\BOOKMARK [2][-]{subsection.13.9}{Numerical Optimization via Gradient Boosting}{section.13}% 125
\BOOKMARK [3][-]{subsubsection.13.9.1}{General}{subsection.13.9}% 126
\BOOKMARK [3][-]{subsubsection.13.9.2}{Steepest Decent}{subsection.13.9}% 127
\BOOKMARK [3][-]{subsubsection.13.9.3}{Gradient Boosting}{subsection.13.9}% 128
\BOOKMARK [3][-]{subsubsection.13.9.4}{Implementations of Gradient Boosting}{subsection.13.9}% 129
\BOOKMARK [2][-]{subsection.13.10}{Right-Sized Trees for Boosting}{section.13}% 130
\BOOKMARK [2][-]{subsection.13.11}{Regularization}{section.13}% 131
\BOOKMARK [3][-]{subsubsection.13.11.1}{General}{subsection.13.11}% 132
\BOOKMARK [3][-]{subsubsection.13.11.2}{Shrinkage}{subsection.13.11}% 133
\BOOKMARK [3][-]{subsubsection.13.11.3}{Subsampling}{subsection.13.11}% 134
\BOOKMARK [2][-]{subsection.13.12}{Interpretation}{section.13}% 135
\BOOKMARK [3][-]{subsubsection.13.12.1}{General}{subsection.13.12}% 136
\BOOKMARK [3][-]{subsubsection.13.12.2}{Relative Importance of Predictor Variables}{subsection.13.12}% 137
\BOOKMARK [3][-]{subsubsection.13.12.3}{Partial Dependence Plots}{subsection.13.12}% 138
\BOOKMARK [1][-]{section.14}{Sequential Data}{}% 139
\BOOKMARK [2][-]{subsection.14.1}{Markov Models}{section.14}% 140
\BOOKMARK [2][-]{subsection.14.2}{Hidden Markov Models}{section.14}% 141
\BOOKMARK [3][-]{subsubsection.14.2.1}{General}{subsection.14.2}% 142
\BOOKMARK [3][-]{subsubsection.14.2.2}{Decodings}{subsection.14.2}% 143
\BOOKMARK [3][-]{subsubsection.14.2.3}{Problems}{subsection.14.2}% 144
\BOOKMARK [3][-]{subsubsection.14.2.4}{Maximum likelihood for the HMM}{subsection.14.2}% 145
\BOOKMARK [3][-]{subsubsection.14.2.5}{The forward-backward algorithm}{subsection.14.2}% 146
\BOOKMARK [3][-]{subsubsection.14.2.6}{The Viterbi decoding}{subsection.14.2}% 147
\BOOKMARK [3][-]{subsubsection.14.2.7}{Extension of the hidden Markov model}{subsection.14.2}% 148
\BOOKMARK [1][-]{section.15}{Conditional probabilities and graphical models}{}% 149
\BOOKMARK [2][-]{subsection.15.1}{You have a joint probability \204 Now what?}{section.15}% 150
\BOOKMARK [2][-]{subsection.15.2}{Dependency graphs}{section.15}% 151
\BOOKMARK [1][-]{section.16}{Johnson-Lindenstrauss Dimensionality Reduction}{}% 152
\BOOKMARK [2][-]{subsection.16.1}{Intro}{section.16}% 153
\BOOKMARK [2][-]{subsection.16.2}{Simple JL Lemma}{section.16}% 154
\BOOKMARK [1][-]{section.17}{Principal Component Analysis}{}% 155
\BOOKMARK [2][-]{subsection.17.1}{Intuition}{section.17}% 156
\BOOKMARK [3][-]{subsubsection.17.1.1}{Problem statement}{subsection.17.1}% 157
\BOOKMARK [3][-]{subsubsection.17.1.2}{Projection and reconstruction error}{subsection.17.1}% 158
\BOOKMARK [3][-]{subsubsection.17.1.3}{Reconstruction error and variance}{subsection.17.1}% 159
\BOOKMARK [3][-]{subsubsection.17.1.4}{Covariance matrix}{subsection.17.1}% 160
\BOOKMARK [3][-]{subsubsection.17.1.5}{Covariance matrix and higher order structure}{subsection.17.1}% 161
\BOOKMARK [3][-]{subsubsection.17.1.6}{PCA by diagonalizing the covariance matrix}{subsection.17.1}% 162
\BOOKMARK [2][-]{subsection.17.2}{Formalism}{section.17}% 163
\BOOKMARK [3][-]{subsubsection.17.2.1}{Definition of the PCA-optimization problem}{subsection.17.2}% 164
\BOOKMARK [3][-]{subsubsection.17.2.2}{Matrix VT: Mapping from high-dimensional old coordinate system to low-dimensional new coordinate system}{subsection.17.2}% 165
\BOOKMARK [3][-]{subsubsection.17.2.3}{Matrix V: Mapping from low-dimensional new coordinate system to subspace in old coordinate system}{subsection.17.2}% 166
\BOOKMARK [3][-]{subsubsection.17.2.4}{Matrix \(V- .4 T V- .4 \): Identity mapping within new coordinate system}{subsection.17.2}% 167
\BOOKMARK [3][-]{subsubsection.17.2.5}{Matrix \(VVT\): Projection from high- to low-dimensional \(sub\)space within old coordinate system}{subsection.17.2}% 168
\BOOKMARK [3][-]{subsubsection.17.2.6}{Variance}{subsection.17.2}% 169
\BOOKMARK [3][-]{subsubsection.17.2.7}{Reconstruction error}{subsection.17.2}% 170
\BOOKMARK [3][-]{subsubsection.17.2.8}{Covariance matrix}{subsection.17.2}% 171
\BOOKMARK [3][-]{subsubsection.17.2.9}{Eigenvalue equation of the covariance matrix}{subsection.17.2}% 172
\BOOKMARK [3][-]{subsubsection.17.2.10}{Total variance of data x- .4 }{subsection.17.2}% 173
\BOOKMARK [3][-]{subsubsection.17.2.11}{Diagonalizing the covariance matrix}{subsection.17.2}% 174
\BOOKMARK [3][-]{subsubsection.17.2.12}{Constraints of matrix V'}{subsection.17.2}% 175
\BOOKMARK [3][-]{subsubsection.17.2.13}{Finding the optimal subspace}{subsection.17.2}% 176
\BOOKMARK [3][-]{subsubsection.17.2.14}{Interpretation of the result}{subsection.17.2}% 177
\BOOKMARK [3][-]{subsubsection.17.2.15}{Whitening or sphering}{subsection.17.2}% 178
\BOOKMARK [3][-]{subsubsection.17.2.16}{Singular value decomposition}{subsection.17.2}% 179
\BOOKMARK [1][-]{section.18}{Representative-based Clustering}{}% 180
\BOOKMARK [2][-]{subsection.18.1}{General}{section.18}% 181
\BOOKMARK [2][-]{subsection.18.2}{K-Means Algorithm}{section.18}% 182
\BOOKMARK [2][-]{subsection.18.3}{Expectation-Maximization Clustering}{section.18}% 183
\BOOKMARK [3][-]{subsubsection.18.3.1}{General}{subsection.18.3}% 184
\BOOKMARK [3][-]{subsubsection.18.3.2}{Gaussian Mixture Model}{subsection.18.3}% 185
\BOOKMARK [3][-]{subsubsection.18.3.3}{Maximum Likelihood Estrimation}{subsection.18.3}% 186
\BOOKMARK [3][-]{subsubsection.18.3.4}{EM in one Dimension}{subsection.18.3}% 187
\BOOKMARK [3][-]{subsubsection.18.3.5}{EM in d Dimensions}{subsection.18.3}% 188
\BOOKMARK [1][-]{section.19}{Density-based Clustering}{}% 189
\BOOKMARK [2][-]{subsection.19.1}{The DBSCAN Algorithm}{section.19}% 190
\BOOKMARK [2][-]{subsection.19.2}{Kernel Density Estimation}{section.19}% 191
\BOOKMARK [3][-]{subsubsection.19.2.1}{General}{subsection.19.2}% 192
\BOOKMARK [3][-]{subsubsection.19.2.2}{Univariate Density Estimation}{subsection.19.2}% 193
\BOOKMARK [3][-]{subsubsection.19.2.3}{Multivariate Density Estimation}{subsection.19.2}% 194
\BOOKMARK [3][-]{subsubsection.19.2.4}{Nearest Neighbor Density Estimation}{subsection.19.2}% 195
\BOOKMARK [2][-]{subsection.19.3}{Density-Based Clustering: DENCLUE}{section.19}% 196
\BOOKMARK [1][-]{section.20}{Clustering Validation}{}% 197
\BOOKMARK [2][-]{subsection.20.1}{General}{section.20}% 198
\BOOKMARK [2][-]{subsection.20.2}{External Measures}{section.20}% 199
\BOOKMARK [3][-]{subsubsection.20.2.1}{General}{subsection.20.2}% 200
\BOOKMARK [3][-]{subsubsection.20.2.2}{Matching Based Measures}{subsection.20.2}% 201
\BOOKMARK [2][-]{subsection.20.3}{Internal Measures}{section.20}% 202
\BOOKMARK [3][-]{subsubsection.20.3.1}{General}{subsection.20.3}% 203
\BOOKMARK [3][-]{subsubsection.20.3.2}{BetaCV Measure}{subsection.20.3}% 204
\BOOKMARK [2][-]{subsection.20.4}{Davies\205Bouldin Index}{section.20}% 205
\BOOKMARK [2][-]{subsection.20.5}{Silhouette Coefficient}{section.20}% 206
\BOOKMARK [2][-]{subsection.20.6}{Relative Measures}{section.20}% 207
\BOOKMARK [1][-]{section.21}{Hierarchical Clustering}{}% 208
\BOOKMARK [2][-]{subsection.21.1}{General}{section.21}% 209
\BOOKMARK [2][-]{subsection.21.2}{Preliminaries}{section.21}% 210
\BOOKMARK [2][-]{subsection.21.3}{Agglomerative Hierarchical Clustering}{section.21}% 211
\BOOKMARK [3][-]{subsubsection.21.3.1}{General}{subsection.21.3}% 212
\BOOKMARK [3][-]{subsubsection.21.3.2}{Distance between Clusters}{subsection.21.3}% 213
\BOOKMARK [3][-]{subsubsection.21.3.3}{Updating Distance Matrix}{subsection.21.3}% 214
\BOOKMARK [1][-]{section.22}{DBSCAN Revisted}{}% 215
\BOOKMARK [2][-]{subsection.22.1}{General}{section.22}% 216
\BOOKMARK [2][-]{subsection.22.2}{Geometric results}{section.22}% 217
\BOOKMARK [2][-]{subsection.22.3}{DBSCAN in \0403 dimensions}{section.22}% 218
\BOOKMARK [2][-]{subsection.22.4}{ Approximate DBSCAN}{section.22}% 219
\BOOKMARK [1][-]{section.23}{Exam}{}% 220
