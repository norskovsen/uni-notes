% Created 2018-10-09 Tue 08:57
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Martin}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Martin},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.2 (Org mode 9.1.5)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\section{TA Instructor Info}
\label{sec:org7fd986a}
\begin{itemize}
\item Frederik Hvilshøj
\item Email: fhvilshoj@gmail.com
\end{itemize}

\section{Important}
\label{sec:org9a63e6a}
\subsection{Dictionary}
\label{sec:orgef3fb87}
\begin{itemize}
\item \textbf{RHS:} Right Hand Side
\end{itemize}

\subsection{Jacobian}
\label{sec:orgf37e437}
\begin{itemize}
\item If we have a function \(f(z): \mathbb{R}^{a} \rightarrow \mathbb{R}^b\) such that \(f(z) = [f_1(z),\dots, f_b(z)]\) then the Jacobian is the matrix
\end{itemize}
\begin{equation}
  J_{i,j} = \frac{\partial f_i}{\partial z_j}
\end{equation} 
of size \(b\times a\). 

\section{The Learning Problem (1)}
\label{sec:orgf4ca0f8}
\subsection{Components of Learning}
\label{sec:org1dda13e}
\begin{itemize}
\item The main components for the learning problem
\begin{itemize}
\item An input \(x\)
\item The unknown target function \(f: \mathcal X \to \mathcal Y\)
\begin{itemize}
\item \(\mathcal X\) is the input space
\item \(\mathcal Y\) is the output space
\end{itemize}
\item There is a set of data \(\mathcal D\) of input output examples \((x_1, y_1), \cdots, (x_N, y_N)\)
\begin{itemize}
\item where \(y_n = f(x_n)\) for \(n = 1,\dots,N\)
\item often referred to as data points
\end{itemize}
\item The learning algorithm that uses dataset \(\mathcal D\) to pick a formula \(g: \mathcal X \to \mathcal Y\) that approximates \(f\)
\begin{itemize}
\item choose \(g\) from a set of candidate formulas under consideration which is called the hypothesis set \(\mathcal H\)
\item \(\mathcal H\) could be the set of all linear formulas
\end{itemize}
\end{itemize}
\end{itemize}
§
\subsection{A Simple Learning Model}
\label{sec:org5461c1f}
\begin{itemize}
\item The hypothesis set and learning model is referred informally to as the \emph{learning model}

\item A simple learning model (the \emph{perceptron})
\begin{itemize}
\item Let \(\mathcal X = \mathbb R ^d\) where \(\mathcal X = \mathbb R ^d\) is the \$d\$-dimensional Euclidean space be the input space
\item Let \(\mathcal Y = \{+1, -1\}\) be the output space
\item The hypothesis set \(\mathcal H\) is specified through a functional form that all \(h \in \mathcal H\) share
\begin{itemize}
\item The functional form \(h(x)\) chosen is to give weights to the different coordinates of \(x\) which reflects their importance
\end{itemize}
\item The weighted score is compared to a threshold value which decides whether the output is \(+1\) or \(-1\)
\end{itemize}
\end{itemize}
\begin{equation}
  \begin{split} 
    \text{Approve credit if} \ \sum_{i=1}^d w_ix_i &> \text{threshold} \\
    \text{Deny credit if} \ \sum_{i=1}^d w_ix_i &< \text{threshold}
  \end{split}
\end{equation}
This can be written mode compactly as 
\begin{equation}
  h(x) = \text{sign} \bigg( \bigg( \sum_{i=1}^d w_ix_i \bigg) + b \bigg)
\end{equation}
where \(x_1, \dots, x_n\) are the components of the vector \(\pmb x\) and \(b\) is the threshold

\begin{itemize}
\item The bias can be added as the first weight \(w_0 = b\) and adding a fixed input \(x_0 = 1\) gives us the same result
\begin{itemize}
\item With this convention the previous equation can be written as
\end{itemize}
\end{itemize}
\begin{equation}
  h(\pmb x) = \text{sign}(\pmb w^T \pmb x)
\end{equation}
where \(\pmb w\) is the weight vector and \(\pmb x\) is the input vector

\begin{itemize}
\item The \emph{perceptron learning algorithm} will determine \(\pmb w\) based on the data
\begin{itemize}
\item To use this the data should be linearly separable
\begin{itemize}
\item Means that there is a \(\pmb w\) which achieve the correct decision \(h(\pmb x_n)=y_n\) on all data examples
\end{itemize}
\item The algorithm finds \(\pmb w\) using the following simple iterative method
\begin{itemize}
\item At iteration \(t\) where \(t = 0,1,2, \dots\) , there is a current value of the weight vector \(\pmb w(t)\)
\item The algorithm picks an example from \((x_1, y_1), \cdots, (x_N, y_N)\) \((x(t), y(t))\) and uses it to update \(\pmb w(t)\)
\item The update rule is  \(\pmb w(t+1) = \pmb w(t) + y(t)\pmb x(t)\)
\end{itemize}
\end{itemize}
\end{itemize}


\begin{itemize}
\item The learning algorithm is guaranteed to arrive at the right solution at the end
\end{itemize}

\subsection{Types of Learning}
\label{sec:org94d6340}
\subsubsection{Supervised Learning}
\label{sec:orgf2b31ec}
\begin{itemize}
\item The training data contains explicit examples of the correct output for given inputs
\item There are two variations of this protocol
\begin{itemize}
\item \textbf{Active learning:} where the data set is acquired though queries that we make
\begin{itemize}
\item We get to choose a point \(\pmb x\) in the input space and the supervisor reports to us the target value for \(\pmb x\)
\item This opens up a strategic chosen \(\pmb x\)
\end{itemize}
\item \textbf{Online learning:} the data set is given to the algorithm one example at a time
\begin{itemize}
\item This happens when we have streaming data that the algorithm has to process on the run
\item Useful for limitations of computing at storage
\item Can also be used in other paradigms of learning
\item e.g. a movie recommendation system
\end{itemize}
\end{itemize}
\end{itemize}

\subsubsection{Reinforcement Learning}
\label{sec:orgec759eb}
\begin{itemize}
\item When training data does not explicitly contain the correct output for each input we are no longer in the supervised setting
\item The training example does not contain the target output
\begin{itemize}
\item It contains so possible outputs of how good that output is
\end{itemize}
\item The training examples in reinforcement learning are of the form
\end{itemize}
\begin{center}
( input, some output, grade for this output )
\end{center}
\begin{itemize}
\item The examples does not say how good the inputs would have been for other settings
\item Can e.g. be useful for learning to play a game
\end{itemize}

\subsubsection{Unsupervised Learning}
\label{sec:orgc15c7b1}
\begin{itemize}
\item In unsupervised learning the data does not contain any output information at all
\begin{itemize}
\item We are just given input examples \(\pmb x_1, \dots, \pmb x_N\)
\end{itemize}

\item The decision regions in unsupervised learning are the same as the one in supervised learning without label
\item It can be viewed as a task of spontaneously finding patterns and structure in input data
\item It can be a precursor to supervised learning
\end{itemize}

\subsection{Linear Regression and Orthogonal Projections}
\label{sec:org902bd91}
\begin{itemize}
\item \textbf{Lemma 1.} Given a vector \(y\) the closest point in \(V\) to \(y\), i.e. \(\arg\min_v \in V : ||v-y||_2^2\)  is the orthogonal projection of \(y\) onto \(V\).
\item The optimal weight vector \(w\) is found using the following formula
\end{itemize}
\begin{equation}
  w = (X^TX)^{-1}X^Ty
\end{equation}
where \(X\) is a \(n \times d\) data matrix where each row is an input point and a \(n \times 1\) vector y of targets

\subsection{Is Learning Feasible}
\label{sec:org1a38f3d}
\subsubsection{General}
\label{sec:org44dd029}
\begin{itemize}
\item To see the relationship between the data \(\mathcal D\) and the data outside the \textbf{Hoeffding Inequality} is used
\begin{itemize}
\item It states for a random variable \(\nu\) in terms of the parameter \(\mu\) and the sample size \(N\) that
\end{itemize}
\end{itemize}
\begin{equation}
      \mathbb P[|\nu - \mu > \epsilon] \geq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
\begin{itemize}
\item This shows that as one increase the sample size \(\nu\) gets closer to \(\mu\) for some small number \(\epsilon\)

\item The error rate within the sample is called the \textbf{in-sample error}
\end{itemize}
\begin{equation}
  \begin{split} 
    E_\text{in}(h) &= \text{(fraction of $D$ where $f$ and $h$ disagree)} \\
      &= \frac1N \sum_{n=1}^N\llbracket h(\pmb x_n \ne f(\pmb x_n) \rrbracket
  \end{split}
\end{equation}
\begin{itemize}
\item where \(\llbracket \text{statement} \rrbracket = 1\) if the statement is true and \(0\) otherwise

\item The \textbf{out-of-sample error} is defined as
\end{itemize}
\begin{equation}
  E_\text{out}(h)=\mathbb P [h(\pmb x) \ne f(\pmb x)]
\end{equation}

\begin{itemize}
\item Using in-sample and out-of-sample error the \textbf{Hoeffding Inequality} can be written as
\end{itemize}
\begin{equation}
        \mathbb [|E_\text{in}(h)-E_\text{out}(h) > \epsilon] \leq 2e^{-2\epsilon^2N} \ \text{for any} \ \epsilon > 0
\end{equation}
\begin{itemize}
\item where \(N\) is the number of training examples
\end{itemize}

\subsubsection{Feasibility of Learning}
\label{sec:org03ea66c}
\begin{itemize}
\item \(\mathcal D\) does not deterministic tell us something about \(f\) outside of \(\mathcal D\) but it gives us a probabilistic answer
\item Since the \textbf{Hoeffding Inequality} tells us that \(E_\text{in}(g) \approx E_\text{out}(g)\) for a large enough \(N\) \(E_\text{in}(g)\) seems like a good proxy for \(E_\text{out}(g)\)

\item The feasibility of learning is split into two questions
\begin{enumerate}
\item Can we make sure that \(E_\text{out}(g)\) is close enough to \(E_\text{in}(g)\)
\item Can we make \(E_\text{in}(g)\) small enough?
\end{enumerate}

\item \textbf{The complexity of} \(\mathcal H\): If the number of hypothesis \(M\) goes up we run more risk that \(E_\text{in}(g)\) will be a poor estimator of \(E_\text{out}(g)\)
\begin{itemize}
\item \(M\) can be though of as a measure of the complexity of the hypothesis set \(\mathcal H\) that we use
\item The bigger the \(M\) the higher the change of finding a small enough \(E_\text{in}(g)\) becomes
\end{itemize}

\item \textbf{The complexity of} \(f\): A more complex \(f\) is harder to learn
\begin{itemize}
\item A more complex hypothesis makes the likelihood that the \(E_\text{in}(g)\) and \(E_\text{out}(g)\) are approximately the same smaller
\item If the target function \(f\) is two hard one may not be able to learn it at all
\item Most target functions in real life are not too complex
\end{itemize}
\end{itemize}

\subsection{Error and Noise}
\label{sec:org17e7f3e}
\subsubsection{Error Measures}
\label{sec:orga075769}
\begin{itemize}
\item An \textbf{error measure} quantifies how well each hypothesis \(h\) in the target function \(f\)
\end{itemize}
\begin{equation}
	\text{Error} = E(h,f)
\end{equation}

\begin{itemize}
\item While \(E(h,f)\) is based on the entirety of \(h\) and \(f\) is almost universally defined based on the error of individual points \(\pmb x\)
\begin{itemize}
\item If we define a pointwise error measure \(e(h(\pmb x), f(\pmb x))\) the overall error will be the average of the pointwise error
\item The defined error should be defined on the use of the application
\end{itemize}
\end{itemize}

\subsubsection{Noisy target}
\label{sec:orgbf983b9}
\begin{itemize}
\item In a real world application the data that one learns from is not generated from a deterministic target function but in a noisy way
\begin{itemize}
\item Formally we have a \textbf{target distribution} \(P(y \mid \pmb x)\) instead of a taget function
\item One can think of a \textbf{noisy target} as a deterministic target, plus added noise
\end{itemize}
\end{itemize}

\section{Training versus Testing (2)}
\label{sec:org8a3dd75}
\subsection{Theory of Generalization}
\label{sec:org5a408f2}
\subsubsection{General}
\label{sec:orgbfa1730}
\begin{itemize}
\item The \textbf{generalization error} is the discrepancy between \(E_\text{in}\) and \(E_\text{out}\)
\begin{itemize}
\item The Hoeffding Inequality provides a way to characterize it with a probabilistic bound
\item The Hoeffding Inequality can be rephrased as follows: pick a tolerance level \(\delta\) e.g. 0.05 and assert with probability at least \(1-\delta\) that
\end{itemize}
\end{itemize}
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac1{2N} \ln \frac{2M}\delta} 
\end{equation}
\begin{itemize}
\item It is referred to as the \textbf{generalization bound}
\end{itemize}

\subsubsection{Effective Number of Hypotheses}
\label{sec:orgfc14dfb}
\begin{itemize}
\item The \textbf{growth function} is the quantity that will formalize the effective number of hypotheses
\begin{itemize}
\item It will replace \(M\) when \(M=\infty\) in the generalization bound
\end{itemize}

\item \textbf{Definition 2.1.} Let \(x_1, \dots, x_N \in \mathcal X\). The \textbf{dichotomies} generated by \(\mathcal H\) on these points are defined by
\end{itemize}
\begin{equation}
  \mathcal H (\pmb x_1, \dots, \pmb x_N= \{ (h(\pmb x_1), \dots, h(\pmb x_N ) \mid h \in \mathcal H)
\end{equation}

\begin{itemize}
\item \textbf{Definition 2.2.} The \textbf{growth function} is defined for a hypothesis set \(\mathcal H\) by
\end{itemize}
\begin{equation}
  m_\mathcal{H}(N) = \underset{\pmb x_1, \dots, \pmb x_N \in \mathcal X}{\text{max}} | \mathcal H(\pmb x_1, \dots, \pmb x_N)|
\end{equation}
\begin{itemize}
\item where \(|\cdot|\) denotes the number of elements of the set

\item That \(\mathcal H\) can \textbf{shatter} \(\pmb x_1, \dots, \pmb x_N\) signifies that \(\mathcal H\) is as diverse as can be on this particular sample

\item \textbf{Definition 2.3.} If no data set of size \(k\) can be shattered by \(\mathcal H\), then \(k\) is said to be a break point for \(\mathcal H\)

\item If \(k\) is a break point, then \(m_\mathcal{H}(k) < 2^k\)
\end{itemize}

\subsubsection{Bounding the growth function}
\label{sec:org955b113}
\begin{itemize}
\item \textbf{Definition 2.4.} \(B(N,k)\) is the maximum number of dichotomies on \(N\) points such that no subset of size \(k\) of the \(N\) points can be shattered by these dichotomies

\item \textbf{Lemma 2.3.} (Sauer's Lemma)
\end{itemize}
\begin{equation}
  B(N,k) \leq \sum_{i=0}^{k-1}\binom N i
\end{equation}

\begin{itemize}
\item \textbf{Theorem 2.4.} If \(m_\mathcal H (k) < 2^k\) for some value of \(k\), then
\end{itemize}
\begin{equation}
  m_\mathcal H (n) \leq \sum_{i=0}^{k-1} \binom N i
\end{equation}
\begin{itemize}
\item for all \(N\). The RHS is polynomial in \(N\) of degree \(k-1\)
\end{itemize}

\subsubsection{The VC Dimension}
\label{sec:orgfc53e64}
\begin{itemize}
\item \textbf{Definition 2.5.} The \textbf{Vapnik-Chervonekis dimension} of a hypothesis set \(\mathcal H\), denoted by \(d_{vc}(\mathcal H)\) or simply \(d_{vc}\) is the largest value of \(N\) for which \(m_\mathcal H (N) = 2^N\). If \(m_\mathcal H (N) = 2^N\) for all \(N\), then \(d_{vc}(\mathcal H) = \infty\).

\item No smaller breakpoint than \(k = d_{vc} +1\) exists
\end{itemize}

\begin{equation}
    d_{vc} \geq N \iff \text{there \textbf{exists} $\mathcal D$ of size $N$ such that $\mathcal H$ shatters $\mathcal D$}
\end{equation}

\begin{itemize}
\item The VC dimension of a \$d\$-dimensional perceptron is \(d+1\).
\end{itemize}

\subsubsection{The VC Generalization Bound}
\label{sec:orgf1bd280}
\begin{itemize}
\item \textbf{Theorem 2.5.} (VC generalization bound). For any tolerance \(\delta > 0\),
\end{itemize}
\begin{equation}
  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}
\end{equation}
\begin{itemize}
\item with probability \(\geq 1-\delta\)
\end{itemize}

\subsection{Interpreting the Generalization Bound}
\label{sec:org3ebb967}
\subsubsection{General}
\label{sec:orgbfa2683}
\begin{itemize}
\item The VC generalization bound is a universal result
\begin{itemize}
\item It applies to all hypotheses set, learning algorithms, input spaces, probability distributions and binary target functions
\item The bound is quite loose
\item It can be used as a guideline for generalization
\item Learning models with lower \(d_\text{vc}\) tend to better than those with higher \(d_\text{vc}\)
\end{itemize}
\end{itemize}

\subsubsection{Sample Complexity}
\label{sec:org3270caf}
\begin{itemize}
\item The \textbf{sample complexity} denotes how many training examples \(N\) are needed to achieve a certain generalization performance
\begin{itemize}
\item The performance is specified using two parameters \(\epsilon\) and \(\delta\) 
\begin{itemize}
\item The error tolerance \(\epsilon\) determines the allowed generalization error
\item The confidence parameter \(\delta\) determines how often the error tolerance \(\epsilon\) is violated
\end{itemize}
\item How fast \(N\) grows as \(\epsilon\) and \(\delta\) become smaller indicates the amount of data needed for a good generalization
\end{itemize}

\item From the VC generalization bound it follows that
\end{itemize}
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4m_\mathcal H (2N)}\delta)
\end{equation}
\begin{itemize}
\item If \(m_\mathcal H(2N)\) is replaced by its generalization polynomial upper bound we get that
\end{itemize}
\begin{equation}
    N \geq \frac8{\epsilon^2} \ln (\frac{4((2N)^{d_\text{vc}} +1)} \delta)
\end{equation}
\begin{itemize}
\item The numerical value for \(N\) can be obtained using simple iterative methods
\end{itemize}

\subsubsection{Penalty for Model Complexity}
\label{sec:orgf71319d}
\begin{itemize}
\item Often we have a fixed dataset, we can the use the Generalization bound to find out what performance we can expect to get
\end{itemize}
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4m_\mathcal{H}(2N)}\delta}  
\end{equation}
\begin{itemize}
\item We can again use the polynomial bound based on \(d_\text{vc}\) instead of \(m_\mathcal H(2n)\)
\end{itemize}
\begin{equation}
	  E_\text{out}(g) \leq E_\text{in}(g) + \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

\begin{itemize}
\item We often denote the second as \(\Omega(N,\mathcal H, \delta)\) and call it the penalty
\end{itemize}
\begin{equation}
  \sqrt{\frac8N\ln\frac{4((2N)^{d_\text{vc}} + 1)}\delta}  
\end{equation}

\begin{itemize}
\item More complex models help \(E_\text{in}\) and hurt \(\Omega(N,\mathcal H, \delta)\)
\begin{itemize}
\item The optimal model is one that minimizes a combination of the two terms
\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Training versus Testing (2)/screenshot_2018-09-12_19-20-23.png}
\end{center}

\subsubsection{The Test Set}
\label{sec:org432e581}
\begin{itemize}
\item One often estimates \(E_\text{out}\) by using a \textbf{test set} that the learning algorithm has not seen before
\begin{itemize}
\item Called \(E_\text{test}\)
\end{itemize}
\end{itemize}

\subsection{Approximation-Generalization Tradeoff}
\label{sec:org7d6b293}
\subsubsection{Bias and Variance}
\label{sec:orge6bc181}
\begin{itemize}
\item The bias variance decomposition out-of-sample error is
\end{itemize}
\begin{equation}
  E_\text{out}(g^{(\mathcal D)}) = \mathbb E_{\pmb x} \big[ (g^{(\mathcal D)}(\pmb x) - f(\pmb x))^2 \big]
\end{equation}

\begin{itemize}
\item The function \(\bar g (\pmb x)\) can be interpreted in the following operational way
\begin{enumerate}
\item Generate many data sets \(\mathcal D_1, \dots, \mathcal D_K\)
\item Apply the learning algortihm to each data set obtaining final hypotheses \(g_1,\dots,g_K\).
\item The average function for any \(\pmb x\) is then estimated by \(\bar g(\pmb x) \approx \frac1K \sum_{k=1}^K g_k(\pmb x)\)
\end{enumerate}

\item The \textbf{bias} is \(\text{bias}(\pmb x) = (\bar g(\pmb x) - f(\pmb x))^2\)
\begin{itemize}
\item It measures how much the average function deviates from the target function
\end{itemize}

\item The \textbf{variance} is
\end{itemize}
\begin{equation}
	\text{var}(\pmb x)   = E_\mathcal D [(g^{\mathcal D}(x) - \bar g(\pmb x))^2]
\end{equation}
\begin{itemize}
\item Says how much the different hypotheses varies

\item Since bias and variance cannot be computed in a real model
\begin{itemize}
\item They are purly a conceptual tool used when developing a model
\end{itemize}
\end{itemize}

\subsubsection{The Learning Curve}
\label{sec:org6148675}
\begin{center}
\includegraphics[width=.9\linewidth]{Training versus Testing (2)/screenshot_2018-09-12_20-08-30.png}
\end{center}

\begin{itemize}
\item For a simpler model the learning curves converge more quickly but to worse ultimate performance
\begin{itemize}
\item The in-sample error learning curve is increasing in \(N\)
\item The out-of-sample error learning curve is decreasing in \(N\)
\end{itemize}
\end{itemize}

\section{The Linear Model (3)}
\label{sec:orgba8d4f8}
\subsection{Linear Regression}
\label{sec:org7d5b958}
\subsubsection{The algorithm}
\label{sec:org51c53d0}
\begin{itemize}
\item The linear regression algorithm is based on minimizing the squared error between \(h(x)\) and \(y\)
\end{itemize}
\begin{equation}
  E_\text{out}(h) = \mathbb E[(h(\pmb x) - y)^2]
\end{equation}
where the expected value is taken with respect to the joint probability distribution \(P(x,y)\) 

\begin{itemize}
\item The goal is to find an hypothesis that achieves a small \(E_\text{out}(h)\)
\begin{itemize}
\item Since the distribution \(P(\pmb x, y)\) is unknown \(E_\text{out}(h)\) cannot be computed the in-sample version is therefore used instead
\end{itemize}
\end{itemize}
\begin{equation}
    E_\text{in}(h) = \frac{1}{n}\sum_{n=1}^N(h(\pmb x_n) - y_n)^2
\end{equation}

\begin{itemize}
\item In linear regression \(h\) takes the form of a linear combination of the components of \(x\) that is
\end{itemize}
\begin{equation}
  h(\pmb x) = \sum_{i=0}^dw_ix_i = \pmb w^T\pmb x
\end{equation}
where \(x_0 = 1\) an \(\pmb x \in \{1\} \times \mathbb R ^d\) as usual and \(\pmb w \in \mathbb R^{d+1}\) 

\begin{itemize}
\item For the special case of linear \(h\), it is very useful to have a matrix representation of \(E_\text{in}(h)\)
\begin{itemize}
\item First we define the data matrix \(X \in \mathbb R^{N \times (d+1)}\) to be the \(N \times (d+1)\) matrix whose rows are the inputs \(x_n\) as row vector
\item Define the target vector \(\pmb y \in \mathbb R^N\) to be the column vector whose components are the target values \(y_n\)
\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-08-28_14-54-58.png}
\end{center}
\subsection{Logistic Regression}
\label{sec:orge603390}
\subsubsection{Predicting a Probability}
\label{sec:orga8aa0aa}
\begin{itemize}
\item To predict a probability we want something which restricts the output to the probability range \([0,1]\), one choice that accomplishes this goal is the logistic regression model
\end{itemize}
\begin{equation}
    h(\pmb x) = \theta(\pmb w^T \pmb x)
\end{equation}
\begin{itemize}
\item Where \(\theta\) is the \emph{logistic} function \(\theta(s) = \frac{e^s}{1+e^s}\) whose output is between 0 and 1
\begin{itemize}
\item The output can be interpret as a probability for a binary event
\item The logistic function \(\sigma\) is referred to as a \textbf{soft threshold} in contrast to the \textbf{hard threshold} in classification
\item It is also called a \textbf{sigmoid}
\end{itemize}

\item When using Logistic Regression we are formally trying to learn the target function
\end{itemize}
\begin{equation}
  f(\pmb x) = \mathbb P [y=+1 \mid \pmb x]
\end{equation}

\begin{itemize}
\item The data given is generated by a noisy target \(P(y \mid \pmb x)\)
\end{itemize}
\begin{equation}
  \begin{equation*}
    P(y \mid \pmb x) = 
  		\begin{cases}
  			\mbox{$f(\pmb x)$} & \mbox{for $y=+1$} \\
  			\mbox{$1-f(\pmb x)$} & \mbox{for $y=-1$} 
  		\end{cases}
  \end{equation*}    
\end{equation}

\begin{itemize}
\item The standard \textbf{error measure} \(e(h(\pmb x),y)\) used in logistic regression is based how likely it is that we would get this output \(y\) from the input \(\pmb x\). if the target distribution \(P(y | \pmb x)\) was indeed captured by our hypothesis \(h(x)\).

\item The \textbf{error measure} is \(E_\text{in}(\pmb w) = \frac1N \sum_{n=1}^N\ln(1+e^{-y_n\pmb w^T x_n})\)
\end{itemize}

\subsubsection{Gradient Descent}
\label{sec:org5b1924d}
\begin{enumerate}
\item Batch Gradient Descent
\label{sec:org5edd127}
\begin{itemize}
\item Gradient descent is a general technique for minimize a twice differentiable function
\begin{itemize}
\item e.g. \(E_\text{in}(\pmb w)\) in logistic regression
\item You start somewhere and go the steepest way down the surface
\item You may end up in a local minima
\item When using a convex function such as \(E_i\) there is only one minima the global unique minimum
\end{itemize}

\item When steeping in a direction you need that the step \(\eta\) is not too small or too large
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-09-03_17-19-24.png}
\end{center}
\begin{itemize}
\item You typically want to choose \(\eta_t = \eta || \bigtriangledown E_\text{IN} ||\) to obtain a good variable step size
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-09-03_17-24-49.png}
\end{center}
\begin{itemize}
\item A typical good choice for \(\eta\) is a fixed learning rate is around \(0.1\) the
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-09-03_17-28-37.png}
\end{center}

\begin{itemize}
\item \textbf{Initialization}
\begin{itemize}
\item Most of the time the initializing the initial weights as zeros works well
\item It is in general safer to initialize the weights randomly
\item Choosing each weight independently from a Normal distribution with zero mean and small variance usually works well
\end{itemize}

\item \textbf{Termination}
\begin{itemize}
\item A simple approach would be to set and upper limit on the number of iterations
\begin{itemize}
\item Does not guarantee anything on the quality of the final weights
\end{itemize}
\item A natural terminal criterion would be to stop ones \(||\pmb g_t||\) drops below a certain threshold
\begin{itemize}
\item Eventually this must happen but we do not know but we will now know when
\end{itemize}
\item For logistic regression a combination of the two termination conditions is used
\end{itemize}
\end{itemize}

\item Stochastic Gradient Descent (SGD)
\label{sec:org5011bd2}
\begin{itemize}
\item A sequential version of Batch Gradient Decent
\begin{itemize}
\item Often beats the batch version in practise
\end{itemize}

\item Instead of considering the full batch gradient on all \(N\) training points, we consider a stochastic version of the gradient
\begin{enumerate}
\item Pick a training data point \((\pmb x_n, y_n)\) at uniformly random
\item Consider only the error on that point (in case of logistic regression)
\end{enumerate}
\end{itemize}
\begin{equation}
   e_n(w) = \ln ( 1+ e^{-y_n \pmb w^T \pmb x_n})
\end{equation}

\begin{itemize}
\item The gradient needed is
\end{itemize}
\begin{equation}
  \nabla e_n (\pmb w) = \frac{-y_n\pmb x_n}{1+e^{-y_n \pmb w^T \pmb x_n}}
\end{equation}
\begin{itemize}
\item The weight update is \(\pmb w \leftarrow \pmb w \cdot \eta \nabla e_n(\pmb w)\)
\end{itemize}
\end{enumerate}

\subsection{Nonlinear Transformation}
\label{sec:orgf40914e}
\subsubsection{The \(\mathcal Z\) space}
\label{sec:orgc789d65}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-09-03_18-22-01.png}
\caption{\label{fig:org5b6defd}
Example of nonlinear transform}
\end{figure}
\begin{itemize}
\item Using a nonlinear transformation we can convert data which is not linear separable into data that is
\begin{itemize}
\item The space \(\mathcal Z\) generated is called the \textbf{feature space}
\item The transformation from the original space \(\mathcal X\) to \(\mathcal Z\) is called a \textbf{feature transform}
\end{itemize}

\item Any linear hypothesis \(\tilde h\) in \(\pmb z\) corresponds to a (possible nonlinear) hypothesis of
\(\pmb x\) given by \(h(\pmb x) = \tidle h (\theta(\pmb x))\) where \(\theta\) is a non linear transform
\begin{itemize}
\item The set of these hypothesis is denoted by \(\mathcal H_\theta\)
\end{itemize}
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{The Linear Model/screenshot_2018-09-03_18-28-08.png}
\caption{\label{fig:org7c131f6}
The nonlinear transform for separating non separable data.}
\end{figure}

\begin{itemize}
\item The feature transform \(\theta_Q\) is defined for degree-\(Q\) curves in \(\mathcal X\)
\begin{itemize}
\item It is called the Qth order polynomial transform
\end{itemize}

\item The power of the feature transform should be used with care, it may not be worth it to insist on linear separability and employ a highly complex surface
\begin{itemize}
\item It is sometime better to tolerate a small \(E_in\) than using a feature transform
\end{itemize}
\end{itemize}

\subsubsection{Computation and generalization}
\label{sec:org16abb81}
\begin{itemize}
\item Computation is an issue because \(\theta_Q\) maps a two dimensional vector \(\pmb x\) to \(\tilde d = \frac{Q(Q+3)}2\) dimensions, which increases the memory and computational cost
\begin{itemize}
\item Things could get worse if \(\pmb x\) is in a higher dimension to begin with
\end{itemize}

\item The problem of generalization is another important issue
\begin{itemize}
\item We will have a weaker guarantee that \(E_\text{OUT}\) is small
\item It is sometime balanced  by the advantage we get in approximating the target better
\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{The Linear Model (3)/screenshot_2018-09-04_07-51-06.png}
\end{center}
\section{Multinomial/Softmax Regression}
\label{sec:org34f59ac}
\subsection{Setup}
\label{sec:org96a92e6}
\begin{itemize}
\item Multinomial/Softmax Regression generalizes logistic regression to handle \(K\) classes instead of \(2\)
\begin{itemize}
\item A target value \(y\) is represented as a vector of length \(K\) with all zeroes except one which is called a one-in-\(K\) encoding
\item To store all the data points a matrix \(Y\) of size \(n \times K\) and the data matrix \(X\) is unchanged
\end{itemize}
\end{itemize}
\begin{equation}
X=\begin{pmatrix} 
1&- & x_1^T & - \\
\vdots & \vdots & \vdots \\
1&- & x_n^T & - \\
\end{pmatrix}\in \mathbb{R}^{n \times d}\quad\quad 
y=\begin{pmatrix}
- & y_1^T & -\\
- & \vdots &- \\
- & y_n^T & -\end{pmatrix}\in\{0,1\}^{n\times K}
\end{equation}

\begin{itemize}
\item To generalize to \(K\) classes we will use \(K\) weight vectors \(w_1,\dots,w_k\) each of length \(d\), one for each class.
\begin{itemize}
\item To classify data we can use the following algorith: Given data x, compute \(w_i^\intercal x\) for \(i=1,\dots, K\) and return the index of the largest value.
\item The list of weight vectors is packed into a matrix \(W\) of size \(d \times K\) by putting \(w_1\) in column one and so on.
\end{itemize}
\end{itemize}
$$
W=\begin{pmatrix} 
(w_1)_1  & \dots & (w_K)_1 \\
\vdots & \vdots & \vdots \\
(w_1)_d  & \dots & (w_K)_d \\
\end{pmatrix}\in \mathbb{R}^{d \times n}
$$
\begin{itemize}
\item This way we can compute the weighed sum for each class by the vector matrix product \(x^\intercal W\) and then pick argmax of that to do the classification. Pretty Neat!.

\item Numpy example
\end{itemize}
\begin{verbatim}
import numpy as np
# example with 3 classes and d = 10
W = np.random.rand(10, 3)
print('Shape w:', W.shape)
x = np.array([1., 2., 3., 4., 5., 6., 7., 8., 9., 10.0]).reshape(10, 1)
print('Shape x:', x.shape)
model_predictions = x.T @ W
print('model (unnormalzed log) predictions: - picke the larger one\n', model_predictions)
\end{verbatim}

\subsection{Probabilistic Outputs}
\label{sec:org990a302}
\begin{itemize}
\item Given a set of model parameters \(W\) and a data point \(x\) we want \(P(y=i\mid x, W)\) for \(i=1,\dots K\).

\item \textbf{Softmax} is used in our probabilistic model
\begin{itemize}
\item It takes as input a vector of length \(K\) and outputs another vector of the same length \(K\), that is a mapping from the \(K\) input numbers into \(K\) \textbf{probabilities}
\end{itemize}
\end{itemize}
\begin{equation}
\textrm{softmax}(x)_j =
\frac{e^{x_j}}
{\sum_{i=1}^K e^{x_i}}\quad
\textrm{ for }\quad j = 1, \dots, K.
\end{equation}
\begin{itemize}
\item where \(\textrm{softmax}(x)_j\) denote the \(j\)'th entry in the vector.
\begin{itemize}
\item The denominator acts as a normalization term that ensures that the probabilities sum to one
\item The exponentiation ensures all numbers are positive.
\item We get the following derivatives:
\end{itemize}
\end{itemize}
\begin{equation}
\frac
{\partial \;\textrm{softmax}(x)_i}
{\partial x_j} =
(\delta_{i,j} - \textrm{softmax}(x)_j)
\textrm{softmax}(x)_i\quad\quad\text{where}\quad\quad
\delta_{ij}=\begin{cases}1 &\text{if }i=j\\
0 & \text{else}
\end{cases}
\end{equation}

\begin{itemize}
\item The following is our probabilistic model
\end{itemize}
\begin{equation}
p(y \mid x, W) =
\textrm{softmax}(W^\intercal x) =
 \left \{
\begin{array}{l l}
 \textrm{softmax}(W^\intercal x)_1 & \text{ if } y = e_1,  \\
 \vdots & \\
 \textrm{softmax}(W^\intercal x)_K & \text { if } y = e_K.
\end{array}
\right.
\end{equation}

\begin{itemize}
\item We compute the likelihood of the data given a fixed matrix of parameters.
\begin{itemize}
\item The notation \([z]\) for the indicator function
\end{itemize}
\end{itemize}
$$
P(D \mid W) =
\prod_{(x,y)\in D}
\prod_{j=1}^K
\textrm{softmax}(W^\intercal x)_j^{[y_j=1]}
=
\prod_{(x,y)\in D}
y^\intercal
\textrm{softmax}(W^\intercal x)
.
$$

\begin{itemize}
\item This way of expressing is the same as we did for logistic regression.
\end{itemize}

\subsection{The Negative Log Likelihood}
\label{sec:orgb5109cd}
\begin{itemize}
\item The negative log likelihood of the data is minimized instead of maximizing the likelihood of the data and get a pointwise sum.
\end{itemize}
\begin{align}\textrm{NLL}(D\mid W) &=
-\sum_{(x,y)\in D}
\sum_{j=1}^K
[y_j=1]
\ln (\textrm{softmax}(W^\intercal x)_j)
\\
&=-\sum_{(x,y)\in D}
y^\intercal
\ln (\textrm{softmax}(W^\intercal x))
\end{align}


\begin{itemize}
\item In the last summation only one value will be nonzero:
\end{itemize}
\begin{equation}
  - \ln \textrm{softmax}(z)_j = \ln \left( \frac{e^{z_j}}{\sum_{i=1}^d e^{z_i}}\right) = - (z_j - \ln \sum_{i=1}^d e^{z_i})
\end{equation}

\begin{itemize}
\item The insample error is defined to be  \(E_\textrm{in} = \frac{1}{|D|} \textrm{NLL}\)
\begin{itemize}
\item Cannot be solved for a 0 analytically
\item To apply stochastic mini-batch gradient descent as for Logistic Regression all you really need is the gradient of the negative log likelihood function.
\begin{itemize}
\item The gradient is a \textbf{simple} generalization of the one used in logistic regression.
\item There is a set of parameters for each of \(K\) classes, \(W_j\) for \(j=1,\ldots,K\)
\item The gradient is
\end{itemize}
\end{itemize}
\end{itemize}
$$
\nabla \textrm{NLL}(W) =
-X^\intercal
(Y - \textrm{softmax}(XW)),
$$

\subsection{Implementation Issues}
\label{sec:orgbaaa06b}
\subsubsection{Numerical Issues with Softmax}
\label{sec:org0dc703c}
\begin{itemize}
\item There are some numerical issues with the softmax function
\end{itemize}

$$
\textrm{softmax}(x)_j = \frac{e^{x_j}}{\sum_{i=1}^K e^{x_i}} \textrm{ for } j=1,\ldots,K.
$$
\begin{itemize}
\item This is because this is a sum of exponentials and exponentiation of numbers tend to make them very large giving numerical problems.

\item The problematic part is the logarithm of the sum of exponentials.
\item We can move \(e^c\) for any constant \(c\) outside the sum easily, that is:
\end{itemize}
$$
\ln\left(\sum_i e^{x_i}\right) =
\ln\left(e^c \sum_i e^{x_i-c}\right) =
c + \ln\left(\sum_i e^{x_i -c}\right).
$$

\begin{itemize}
\item We need to find a good \(c\), and we choose \(c = \max_i x_i\)
\item Since \(e^{x_i}\) is the dominant term in the sum. We are less concerned with values being inadvertently rounded to zero since that does not
\end{itemize}

\subsubsection{One in k encoding}
\label{sec:org7eabc99}
\begin{itemize}
\item Representing a number \(k\) in \([1,\dots,k]\) as a vector of length may \(K\) be quite cumbersome.
\begin{itemize}
\item In general the input labels will just be a list/vector of numbers between 1 and k.
\item It is your job to transform it into a matrix if needed.
\item But this will be a very sparse matrix.
\item It may be worthwhile to consider whether it is possible to implement the operations with the matrix Y without actually creating the matrix.
\end{itemize}
\end{itemize}

\subsubsection{Always check your shapes}
\label{sec:org1a91ef8}
\begin{itemize}
\item If the shapes dont fit then
\begin{itemize}
\item If trying to implement softmax it is very useful to ensure you have full control over the shapes of all matrices and vectors you use.
\item If there is a shape mismatch then clearly there is a larger issue.
\item Checking shapes is a very efficient heuristic for catching bugs.
\end{itemize}
\end{itemize}

\subsubsection{Bias Variable}
\label{sec:orgb1c6505}
\begin{itemize}
\item If you need a Bias variable \(b\) (remember \(w^\intercal x + b\)) for each class you need to add a columns of ones to \(X\) and make \(W\) a \(d+1 \times K\) matrix.
\end{itemize}

\section{Overfitting (4)}
\label{sec:org6f9f8d9}
\subsection{When Does Overfitting Occur?}
\label{sec:org2dc6655}
\subsubsection{General}
\label{sec:orge584b3e}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-17_10-02-37.png}
\caption{\label{fig:orgbecfe3f}
Example of overfitting}
\end{figure}
\begin{itemize}
\item The main case of overfitting is when you pick the hypothesis with lower \(E_{in}\) and it results in higher \(E_{out}\)
\begin{itemize}
\item Means that \(E_{in}\) alone is no longer a good guide for learning
\item A typical overfitting scenario is when a complex model uses its addition degress of freedom to "learn" the noise
\end{itemize}
\end{itemize}

\subsubsection{Catalysts for Overfitting}
\label{sec:org747ae98}
\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-17_10-16-04.png}
\end{center}
\begin{itemize}
\item On a finite data set the algorithm inadvertently uses some of the degrees of freedom to fit the noise
\begin{itemize}
\item Can result in overfitting and a spurious final hypothesis
\end{itemize}

\item There are two types of noise which that algorithm cannot differentiate
\begin{itemize}
\item \textbf{Deterministic noise} will not change if the dataset was generated again
\begin{itemize}
\item Is different depending on which model we use
\item Related to the bias
\end{itemize}
\item \textbf{Stochastic noise} will change if the dataset was generated again
\begin{itemize}
\item Related to the variance
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Regularization}
\label{sec:orgecf7cd4}
\subsubsection{General}
\label{sec:orgcaca895}
\begin{itemize}
\item \textbf{Regularization} is a way to combat overfitting
\begin{itemize}
\item Constraints the learning algorithm to improve out-of-sample error
\item Especially when noise is present
\end{itemize}

\item A view of regularization is thought the VC bound, which bounds \(E_\text{out}\) using a model complexity penalty \(\Omega(\mathcal H)\):
\end{itemize}
\begin{equation}
    E_\text{out}(h) \leq E_\text{in}(h) + \Omega(\mathcal H) \ \text{for all } h \in \mathcal H
\end{equation}
\begin{itemize}
\item We are better off fitting the data using a simple \(\mathcal H\)

\item Instead on minimizing \(E_\text{in}(h)\) alone one minimizes the combination of \(E_\text{in}(h)\) and \(\Omega(h)\)
\begin{itemize}
\item Avoids overfitting by constraining the learning algorithm to fit data well using a simple hypotheses
\end{itemize}
\end{itemize}

\subsubsection{A Soft Order Constraint}
\label{sec:orgafc747a}
\begin{itemize}
\item A \textbf{Soft Order Constraint} can be defined as the hypotheses set
\end{itemize}
\begin{equation}
    \mathcal{C}=\{h \mid h(\pmb x) = \pmb x ^T\pmb x, \pmb w^T \pmb w \leq C\}
\end{equation}
\begin{itemize}
\item Solving for \(\pmb w_\text{reg}\):
\begin{itemize}
\item If \(\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C\) then \(\pmb w_\text{reg} = \pmb w_\text{lin}\) since \(\pmb w_\text{lin} \in \mathcal H(C)\)
\item If \(\pmb w_\text{lin} \notin \mathcal H(C)\) then not only is \(\pmb w_\text{lin}^T \pmb w_\text{lin} \leq C\) but \(\pmb w_\text{lin}^T \pmb w_\text{lin} = C\)
\begin{itemize}
\item The weights \(\pmb w\) must lie on the surface of thee \$sphere \(\pmb w^T \pmb w = C\)
\end{itemize}
\end{itemize}

\item If \(\pmb w_\text{reg}\) is to be optimal then for some positive parameter \(\lambda_C\)
\end{itemize}
\begin{equation}
  \nabla E_\text{in}(\pmb w_\text{reg}) = - 2 \lambda_C\pmb w_\text{reg}
\end{equation}
\begin{itemize}
\item \(\nabla E_\text{in}\) must be parallel to \(\pmb w_\text{reg}\)
\end{itemize}

For some \(\lambda_C > 0\) \(\pmb w_\text{reg}\) locally minimizes
\begin{equation}
    E_\text{in}(\pmb w) + \lambda_CW^TW
\end{equation}

\subsubsection{Weight Decay and Augmented Error}
\label{sec:org9f3418c}
\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-18_08-20-44.png}
\end{center}
\begin{itemize}
\item The \textbf{augmented error} is defined as
\end{itemize}
\begin{equation}
  E_\text{aug}(\pmb w) = E_\text{in}(\pmb w) + \lambda \pmb w^T \pmb w
\end{equation}
\begin{itemize}
\item where \(\lambda \geq 0\) is now a free parameter

\item The penalty term enforces a trade-off between making the in-sample error small and making the weights small
\begin{itemize}
\item Is also known as the \textbf{weight decay}
\item Minimizing the error together with the decay is known as \textbf{ridge regression}
\end{itemize}

\item If we can find the optimal \(\lambda^*\) we can minimize the out-of-sample error

\item In general the \textbf{augmented error} for a hypothesis set \(h \in \mathcal H\) is
\end{itemize}
\begin{equation}
    E_\text{aug}(h,\lambda,\Omega) = E_\text{in}(h) + \frac\lambda N \Omega(h)
\end{equation}
\begin{itemize}
\item For weight decay \(\Omega(h) = \pmb w^T\pmb w\)
\begin{itemize}
\item The need for regularization goes down as the number of data points goes up
\end{itemize}
\end{itemize}

\subsubsection{Choosing a Regularizer}
\label{sec:orgfcfc1b7}
\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-18_08-32-17.png}
\end{center}

\begin{itemize}
\item A uniform regularizer, is a penalizes the weights equally
\begin{itemize}
\item encourages all weights to be small uniformly
\item Example \(\Omega_\text{unif}(\pmb w)= \sum_{q=0}^{15}w_q^2\)
\end{itemize}
\item A lower-order regularizer
\begin{itemize}
\item Pairs more attention to the higher order weights
\item Example \(\Omega_\text{low}(\pmb w)= \sum_{q=0}^{15}qw_q^2\)
\end{itemize}

\item The price paid for overfitting is generally more severe than underfitting
\item The optimal  value for the regularization parameter increases with noise
\item No regularizer will be ideal for all settings
\begin{itemize}
\item Not even specific settings
\item The entire burden rest on picking the right \(\lambda\)
\end{itemize}
\item Some for of regularization is necessary as learning is quite sensitive to stochastic and deterministic noise
\end{itemize}

\subsection{Validation}
\label{sec:org15e8193}
\subsubsection{The Validation Set}
\label{sec:org745cf47}
\begin{itemize}
\item \textbf{Validation} tries to estimate the out-of-sample error directly

\item The idea of a \textbf{validation set} is almost identical to that of a test set
\begin{itemize}
\item A subset of the data is removed and not used in training
\item Will be used to make certain choice in the learning process
\begin{itemize}
\item Therefore not a test set
\end{itemize}
\end{itemize}

\item The \textbf{validation set} is created and used in the following way
\begin{enumerate}
\item Partition the data set \(\mathcal D\) using into a training set \(\mathcal D_\text{train}\) of size \((N-K)\) and a validation set \(\mathcal D_\text{val}\) of size \(K\)
\begin{itemize}
\item Any partitioning method which does not depend on the data will do
\end{itemize}
\item Run the learning algorithm using the training set \(\mathcal D_\text{train}\) to obtain a final hypothesis \(g^- \in \mathcal H\)
\item The validation error is then computed for \(g\) using the validation set \(\mathcal D_\text{val}\)
\end{enumerate}
\end{itemize}
\begin{equation}
  E_\text{val}(g^-)=\frac1K\sum_{\pmb x_n \in \mathcal D_\text{val}} e(g^-(\pmb x_n),y_n)
\end{equation}
\begin{itemize}
\item where \(e(g(\pmb x),y)\) is the pointwise error measure

\item The validation error is an \emph{unbiased} estimate of \(E_\text{out}\) because the final hypothesis \(g^-\) was created independently of the validation set
\begin{itemize}
\item The expected error of \(E_\text{val}\) is \(E_\text{out}\)
\item If \(K\) is neither too small nor too large \(E_\text{val}\) is a good estimate of \(E_\text{out}\)
\item A rule of thumb in practise is to set \(K=\frac N5\)
\end{itemize}

\item We should not output \(g^-\) we should output \(g\) which is trained on the entire hypothesis set \(D\)
\begin{itemize}
\item To estimate \(E_\text{out}\) we use that \(E_\text{out}(g) \leq E_\text{out}(g^-)\) because of the learning curve
\begin{itemize}
\item Is not rigorously proved
\item It is just very likely
\end{itemize}
\end{itemize}
\end{itemize}

\subsubsection{Model Selection}
\label{sec:orgeab9a89}
\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-18_09-48-43.png}
\end{center}
\begin{itemize}
\item The most important use of validation is for \textbf{model selection}
\begin{itemize}
\item Choosing linear or nonlinear, polynomial or not\ldots{}
\item It could any choice that affects the learning process
\end{itemize}

\item It can be used to estimate the out-of-sample error for more than one model, suppose we have \(M\) models \(\mathcal H_1, \dots, \mathcal H_M\) 
\begin{itemize}
\item Validation can be used to select one of these models
\item Use the training set \(\mathcal D_\text{train}\) to learn the final hypothesis \(g^-_m\) for each model
\item Evaluate each model on the validation set to obtain the validation errors \(E_1,\dots,E_M\) where
\end{itemize}
\end{itemize}
\begin{equation}
  E_m = E_\text{val}(g^-_m); \text{ for } m = 1,\dots,M
\end{equation}
\begin{itemize}
\item Then just select the model with the lowest validation error.
\item For suitable \(K\) even \(g^-_{m*}\) is better than in-sample selection of the model
\item The validation error can also be used to select a lambda by using \((\mathcal H, \lambda_1),(\mathcal H, \lambda_2),\dots,(\mathcal H \lambda_M)\) as our \(M\) different models
\item The more one uses the validation set to fine tune the model the more the it becomes like the training set
\end{itemize}

\subsubsection{Cross Validation}
\label{sec:org69d1432}
\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-18_20-20-44.png}
\end{center}

\begin{itemize}
\item There are \(N\) ways to partition a set of size \(N-1\) and a validation set of size \(1\). Let
\end{itemize}
\begin{equation}
  \mathcal D_n = (\pmb x_1, y_1), \dots, (\pmb x_{n-1}, y_{n-1}), (\pmb x_{n+1}, y_{n+1}), \dots, (\pmb x_N, y_N)
\end{equation}
\begin{itemize}
\item The final hypothesis learned from \(\mathcal D_n\) is denoted \(g_n^-\)
\item Let \(e_n\) be the error made by \(g_n^-\) on its validation set which is just a single point \(\{(\pmb x_n, y_n)\}\)
\item The cross validation estimate is the average value of the \(e_n\text{'s}\)
\end{itemize}
\begin{equation}
  E_\text{cv}=\frac1N \sum_{n=1}^Ne_n
\end{equation}

\begin{itemize}
\item \textbf{Theorem 4.4.} \(E_\text{cv}\) is an unbiased estimate of \(\bar E_\text{out}(N-1)\)
\begin{itemize}
\item The expectation of the model performance, \(\mathbb E[E_\text{out}])\), over data set of size \(N-1\)
\end{itemize}

\item The cross validation estimate will on average be an upper estimate for the out-of-sample error: \(E_\text{out}(g) \leq E_\text{cv}\)

\item Cross validation can be for model selection for a given set of models \(\mathcal H_1, \dots, \mathcal H_M\) in the same way as validation set
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Overfitting (4)/screenshot_2018-09-18_20-42-49.png}
\end{center}

\begin{itemize}
\item To get cross validation for \(M\) models and a data set \(D\) of size \(N\) is requires \(MN\) rounds of learning
\item If one could analytically obtain \(E_\text{cv}\) it would be a big bonus
\begin{itemize}
\item Analytical results are hard to come
\item An analytical method exists for linear models
\end{itemize}

\item The cross validation estimate can be analytically computed as
\end{itemize}
\begin{equation}
  E_\text{cv} = \frac1N\sum_{n=1}^N(\frac{\hat y_n-y_n}{1-H_nn(\lambda)})^2
\end{equation}
\begin{itemize}
\item where \(H(\lambda)=Z(Z^TZ+ \lambda I)^{-1}Z^T\)
\end{itemize}

\section{Support Vector Machines}
\label{sec:orgf8f7010}
\subsection{Notation}
\label{sec:orgbe3d822}
\begin{itemize}
\item The classifier considered will be a linear classifier for a binary classification problem with labels \(y\) and features \(x\)
\begin{itemize}
\item \(x \in \{-1,1\}\)
\item The classifier with parameters \(w\) and \(b\)  is written as
\end{itemize}
\end{itemize}
\begin{equation}
  h_{w,b}(x)=g(w^Tx+b)
\end{equation}
\begin{itemize}
\item where g = 1 if \(z \geq 0\) and \(g(z)= -1\) otherwise
\end{itemize}

\subsection{Functional and geometric margins}
\label{sec:org8a6b6fe}
\begin{itemize}
\item Given a training example \((x^{(i)},y^{(i)})\), the \textbf{functional margin} of \((w,b)\) is defined with respect to the training example
\end{itemize}
\begin{equation}
  \hat \gamma^{(i)}=y^{(i)}(w^Tx+b)
\end{equation}
\begin{itemize}
\item A large functional margin represents a confident and a correct prediction
\item Given a training set \(S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}\) the functional margin of \((w,b)\) with respect to \(S\) is
\end{itemize}
\begin{equation}
  \hat \gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \hat \gamma^{(i)}
\end{equation}

\begin{itemize}
\item The \textbf{geometric margin} of \((w,b)\) with respect to a training example \((x^{(i)},y^{(i)})\) to be
\end{itemize}
\begin{equation}
	\gamma^{(i)} = y{(i)} \Bigg( \bigg( \frac w{||w||} \bigg)^T x^{(i)} + \frac b{||w||}  \Bigg)
\end{equation}
\begin{itemize}
\item If \(||w|| == 1\) then the functional margin equals the geometric margin
\item Given a training set \(S = \{(x^{(i)},y^{(i)}); i=1,\dots,m\}\) the \textbf{geometric margin} of \((w,b)\) with respect to \(S\) is
\end{itemize}
\begin{equation}
	\gamma = ‎‎‎‎‎‎\min_{i=1,\dots,m} \gamma^{(i)}
\end{equation}

\subsection{The optimal margin classifier}
\label{sec:org195d775}
\begin{itemize}
\item The problem of finding a decision boundary which has the largest geometric margins is the following optimisation problem
\end{itemize}
\begin{equation}
  \begin{split} 
    \max_{\gamma,w,b} \ &\gamma\\
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq \gamma, \ i= 1, \dots, m \\
    &||w|| = 1
  \end{split}
\end{equation}
\begin{itemize}
\item this can be be turned into the following problem using functional margins and rescaling it
\end{itemize}
\begin{equation}
  \begin{split} 
   \max_{\gamma,w,b} \ &\frac12 ||w||^2\\ 
    \text{s.t.} \ & y^{(i)}(w^Tx^{(i)}+b) \geq 1, \ i= 1, \dots, m \\
  \end{split}
\end{equation}
\begin{itemize}
\item It is called the \textbf{optimal margin classifier}
\end{itemize}

\subsection{Lagrange duality}
\label{sec:org3de173b}
\begin{itemize}
\item Consider a problem of the following form:
\end{itemize}
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}
\begin{itemize}
\item The \textbf{Lagrangian} is defined to be
\end{itemize}
\begin{equation}
  \mathcal L(w,\beta) = f(w) + \sum_{i=1}^l\beta_ih_i(w)
\end{equation}
\begin{itemize}
\item The \(\beta_i\)'s are called the \textbf{Lagrange multipliers}
\begin{itemize}
\item We would the find and set \(\mathcal L\)'s partial derivatives to zero
\end{itemize}
\end{itemize}
\begin{equation}
  \frac{\partial \mathcal L}{\partial w_i} = 0; \frac{\partial \mathcal L}{\partial \beta_i} = 0
\end{equation}
and solve for \(w\) and \(\beta\) 

\begin{itemize}
\item The \textbf{primal} optimization problem is the following
\end{itemize}
\begin{equation}
  \begin{split} 
    \text{min}_w &\ f(w)  \\
    \text{s.t.} &\ g_i(w) \leq 0, \ i=1,\dots, k
								&\ h_i(w) = 0, \ i=1,\dots,l
  \end{split}
\end{equation}

\begin{itemize}
\item The primal optimization problem is solved by defining the \textbf{generalized Lagrangian}
\end{itemize}
\begin{equation}
  \mathcal L (w,\alpha,\beta) = f(w) + \sum_{i=1}^k \alpha_ig_i(w) + \sum_{i=1}^k \beta_ih_i(w)
\end{equation}
\begin{itemize}
\item The \(\alpha_i\)'s and \(\beta_i\)'s are the Lagrange multipliers.
\item Consider the quantity
\end{itemize}
\begin{equation}
    \theta_\mathcal{P}(w) = \max_{\alpha, \beta: \alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
\begin{itemize}
\item If \(w\) violates any of the primal constraints then one should be able to verify that
\end{itemize}
\begin{equation}
    \theta_\mathcal P (w) = \infty
\end{equation}
\begin{itemize}
\item If \(w\) does not violate the constraints then \(\theta_\mathcal P(w) = f(w)\) and therefore the minimization problem
\end{itemize}
\begin{equation}
  \min_w\theta_\mathcal P(w) = \min_w \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal L (w,\alpha,\beta)
\end{equation}
\begin{itemize}
\item is the same as the original problem
\begin{itemize}
\item the objective is called the \textbf{value} of the primal problem and is denoted \(p* = \min_w \theta _ \mathcal P (w)\)
\end{itemize}

\item If one define
\end{itemize}
\begin{equation}
    \theta_D(\alpha,\beta) = \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
\begin{itemize}
\item The "\(\mathcal D\)" subscript stands for dual
\begin{itemize}
\item This can be used to pose the \textbf{dual} optimization problem
\end{itemize}
\end{itemize}
\begin{equation}
  \max_{\alpha,\beta: \alpha_i  \geq 0} \theta_\mathcal D (\alpha, \beta) = \max_{\alpha,\beta: \alpha_i  \geq 0} \min_w \ \mathcal L (w,\alpha,\beta)
\end{equation}
\begin{itemize}
\item Which is exactly the same as the primal problem, except the order of the min and max has been exchanged
\begin{itemize}
\item The solution to the dual problem is defined as \(d^*\)
\item It holds that \(d^* \leq p^*\)
\end{itemize}

\item The KKT conditions on \(w^*\), \(\alpha^*\) and \(\beta^*\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-24_17-52-15.png}
\end{center}

\begin{itemize}
\item If some \(w^*\), \(\alpha^*\), \(\beta^*\) satisfy the KKT conditions they are also a solution to the primal and dual problems
\end{itemize}

\subsection{Optimal margin classifiers}
\label{sec:orgbc7f094}
\begin{itemize}
\item By using the KKT conditions obtain the following optimization problem, which gives us a decision boundary with the largest margins:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_07-46-53.png}
\end{center}

\begin{itemize}
\item Where \(w= \sum_{i=1}^m a_iy^{(i)}x^{(i)}\) 
\begin{itemize}
\item The prediction \(w^Tx+b\) can also be written as
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_07-50-38.png}
\end{center}
\begin{itemize}
\item Where post of the inner products will be zero except for the support vectors
\end{itemize}

\subsection{Kernels}
\label{sec:org1af0dec}
\begin{itemize}
\item Given a feature mapping \(\phi\) we define the corresponding \textbf{Kernel} to be
\end{itemize}
\begin{equation}
  K(x,z) = \phi(x)^T\phi(z)
\end{equation}
\begin{itemize}
\item Everywhere we previously had \(\langle x, z \rangle\) we could simple replace it with \(K(x,z)\) and the algorithm would now be learning using the features \(\phi\)

\item The \textbf{Gaussian kernel} which corresponds to an infinite dimensional feature mapping \(\phi\)
\end{itemize}
\begin{equation}
  K(x,z) = \exp(-\frac{||x-z||^2}{2\sigma^2})
\end{equation} 

\begin{itemize}
\item The matrix called the \textbf{Kernel matrix} is defined from some \(m\) data points \(\{x^{(1)}, \dots, x^{(m)}\}\) as the m-by-m matrix \(K\) where the \((i,j)\) entry is given by \(K_{ij}=K(x^{(i)},y^{(i)})\)

\item \textbf{Theorem (Mercer).} Let \(K: \mathbb R^n \times \mathbb R^n \mapsto \mathbb R\) be given. Then for \(K\) to be a valid (Mercer) kernel, it is necessary and sufficient that for any \(\{x^{(1)}, \dots, x^{(m)}\}\), \(m < \infty\), the corresponding kernel matrix is symmetric positive semi-definite
\end{itemize}

\subsection{Regularization and the non-separable case}
\label{sec:orgf165047}
\begin{itemize}
\item To make the algorithm work for non-linearly separable datasets and being less sensitive to outliers, the optimization can be reformulated as follows using regularization:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_08-30-17.png}
\end{center}
\begin{itemize}
\item Which means that examples are now permitted to have a functional margin less than 1

\item By using some of the KKT conditions one can obtain the following dual form of problem
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_08-36-39.png}
\end{center}

\subsection{The SMO algorithm}
\label{sec:org5c7423f}
\subsubsection{General}
\label{sec:orgc2275da}
\begin{itemize}
\item The SMO algorithm gives an efficient way of solving the dual problem arising from the derivation of the SVM
\end{itemize}

\subsubsection{Coordinate ascent}
\label{sec:org12c2e6f}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_08-44-24.png}
\caption{\label{fig:org6a7adb1}
Coordinate ascent example}
\end{figure}

\begin{itemize}
\item If one is trying to solve the unconstrained optimization problem
\end{itemize}
\begin{equation}
  \max_\alpha W(\alpha_1,\alpha_2, \dots,\alpha_m)
\end{equation}
\begin{itemize}
\item One can use the algorithm called \textbf{coordinate ascent:}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_08-47-20.png}
\end{center}
\begin{itemize}
\item Where one holds all the variables fixed except some \(a_i\)
\end{itemize}

\subsubsection{SMO}
\label{sec:org1819238}
\begin{itemize}
\item The SMO algorithm does the following
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Support Vector Machines/screenshot_2018-09-25_09-00-29.png}
\end{center}
\begin{itemize}
\item To test for convergence, one can test whether the KKT conditions are satisfied within some \emph{tol}
\begin{itemize}
\item The tol is the convergence tolerance parameter normally set around \(0.01\) to \(0.001\)
\end{itemize}
\end{itemize}
\section{Deep Feedforward Networks}
\label{sec:orgc4d9234}
\subsection{General}
\label{sec:org464a7d5}
\begin{itemize}
\item \textbf{Deep feedforward networks} are quintessential deep learning models 
\begin{itemize}
\item The goal of a feedforward network is to approximate some function \(f^*\)
\item It defines a mapping \(\pmb y = f(\pmb x; \pmb \theta)\) and learns the value of the parameters \(\pmb \theta\) that result in the best function approximation
\end{itemize}

\item It is called \textbf{feedforward} since information flows through the function being evaluated from \(\pmb x\), through intermediate computations used to define \(f\) and finally to the output \(\pmb y\)
\begin{itemize}
\item When feedforward neural networks are extended to include feedback connections, they are called \textbf{recurrent neural networks}
\end{itemize}

\item They are called \textbf{networks} because they typically are represented by composing together many different function
\begin{itemize}
\item It is associated with a DAG describing ow the functions are composed together
\item The different functions are called \textbf{layers}
\item The overall length of the function chain gives the \textbf{depth} of the model
\item The final layer is called the \textbf{output layer}
\item During the training we drive \(f(\pmb x)\) to match \(f^*(\pmb x)\)
\item The training data provides us with noisy, approximate examples of \(f^*(\pmb x)\) evaluated at different training points
\item The training data does not say what each individual layers should do
\begin{itemize}
\item That is the training algorithms job
\item They are called \textbf{hidden layers}
\end{itemize}
\item The dimensionality of these hidden layers determines the \textbf{width} of the model
\end{itemize}

\item The strategy of deep learning is the feature transform \(\phi\)
\begin{itemize}
\item We have a model \(=f(\pmb x; \pmb \theta, \pmb w) = \phi(\pmb x, \pmb \theta)^\top \pmb w\)
\item We have the parameters \(\pmb \theta\) that we use to learn \(\phi\) from a broad class of functions
\item We have the parameters \(\pmb w\) that map from \(\phi(\pmb x)\) to the desired output
\end{itemize}

\item Training a feedforward network requires making many of the same design decisions as are necessary for a linear model:
\begin{itemize}
\item Choosing the optimizer
\item The cost function
\item The form of the output units.
\end{itemize}
\end{itemize}

\subsection{Gradient-Based Learning}
\label{sec:org5ae0c8d}
\subsubsection{General}
\label{sec:orge207fab}
\begin{itemize}
\item The non-linearity of a neural network causes most interesting loss functions to become non-convex
\begin{itemize}
\item It means that NNs are usually are trained by using iterative, gradient-based optimizes that merely drive the cost function to a very low value
\item It is important to initialize all weights to random values becomes of the error function being non-convex, when using stochastic gradient descent 
\begin{itemize}
\item The biases may be initialized to zero or a small positive values
\end{itemize}
\end{itemize}
\end{itemize}

\subsubsection{Cost Functions}
\label{sec:org9433e0d}
\begin{enumerate}
\item General
\label{sec:orgedc73ae}
\begin{itemize}
\item Cost functions for neural network are more or less the same as those for other models, such as linear models
\begin{itemize}
\item The total cost function used to train a neural network will often combine one of the primary cost functions with a regularization term
\end{itemize}
\end{itemize}

\item Learning Conditional Distributions with Maximum Likelihood
\label{sec:orged7ffdf}
\begin{itemize}
\item Most NNs are trained using maximum likelihood
\begin{itemize}
\item The cost function is simply the NLL which is equivalently described as the cross-entropy between the training data and the model distribution
\item This cost function is given by:
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-09-30_16-58-07.png}
\end{center}
\begin{itemize}
\item The advantage of deriving the cost function from maximum likelihood is that it removes the burden of designing cost functions for each model
\begin{itemize}
\item Specifying a model \(p(\pmb y \mid \pmb x)\) automatically determines a cost function \(\log p(\pmb y \mid \pmb x)\)
\end{itemize}

\item Instead of learning a full probability distribution \(p(\pmb y \mid \pmb x; \pmb \theta)\), we often want to learn just one conditional statistic of \(\pmb y\) given \(\pmb x\)
\begin{itemize}
\item Such as predicting the mean of \(\pmb y\) given the predictor \(f(\pmb x; \pmb \theta)\)
\end{itemize}
\end{itemize}

\item Learning Conditional Statistics
\label{sec:org0904f2d}
\begin{itemize}
\item Instead of learning a full probability distribution \(p(\pmb y \mid \pmb x; \pmb \theta)\) one often want to learn just one condition statistic of \(\pmb y\) given \(\pmb x\)
\begin{itemize}
\item Such as predicting the mean of \(\pmb y\)
\item The cost function can be viewed as being a \textbf{functional} rather than just a function
\begin{itemize}
\item A mapping from functions to real numbers
\end{itemize}
\item The cost functional can be designed to have its minimum occur at some specific function we desire
\end{itemize}

\item Solving the optimization problem
\end{itemize}
\begin{equation}
  f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||^2
\end{equation}
yields
\begin{equation}
  f^*(\pmb x) = \mathbb E_{\pmb y \sim p_{data}(\pmb y \mid x)[\pmb y]}
\end{equation}

\begin{itemize}
\item The following function yields a function that predicts the \emph{median} value of \(\pmb y\) for each \(\pmb x\)
\end{itemize}
\begin{equation}
	f^*=\arg\min_f\mathbb E_{\pmb x, \pmb y \sim p_{data}}||\pmb y - f(\pmb x)||_1
\end{equation}
\begin{itemize}
\item This cost function is commonly call \textbf{mean absolute error}
\end{itemize}
\end{enumerate}

\subsubsection{Output Units}
\label{sec:org9a086f6}
\begin{enumerate}
\item General
\label{sec:orga0c0074}
\begin{itemize}
\item The choice of cost function is tightly coupled with the choice of output unit
\begin{itemize}
\item Most of the time one simply uses the cross-entropy between the data distribution and the model distribution
\item The choice of how to represent the output determines the cross-entropy function
\item Any kind of neural network that may be used as output can also be used as a hidden unit
\end{itemize}

\item The hidden features is defined by \(\pmb h = f(\pmb x ; \pmb \theta)\)
\begin{itemize}
\item The role of the output layer is to provide some additional transformation from the features to complete the task that the network must perform
\end{itemize}
\end{itemize}

\item Linear Units for Gaussian Output Distributions
\label{sec:orgec0e95f}
\begin{itemize}
\item Linear units is an output unit based on an affine transformation with no non-linearity
\begin{itemize}
\item Given features \(\pmb h\), a layer of linear output units produces a vector \(\hat{\pmb y}= \pmb W^T \pmb h + b\)
\item Linear output layers are often used to produce the mean of a conditional Gaussian distribution
\end{itemize}
\end{itemize}
\begin{equation}
  p(\pmb y \mid \pmb x) = \mathcal N(\pmb y; \hat{\pmb y}, \pmb I)
\end{equation}
\begin{itemize}
\item Maximizing the log-likelihood is the equivalent to minimizing the mean squared error
\end{itemize}
\end{enumerate}

\subsection{Hidden Units}
\label{sec:org1687427}
\subsubsection{General}
\label{sec:orgaec5b79}
\begin{itemize}
\item Some valid hidden units are not differentiable at all input points
\begin{itemize}
\item Such as \(g(z)=\max\{0,z\}\)
\item Solved by using the right or left differential
\end{itemize}
\end{itemize}

\subsubsection{Rectiﬁed Linear Units and Their Generalizations}
\label{sec:orgb3d2545}
\begin{itemize}
\item Rectified linear units use the activation function \(g(z) = \max\{0,z\}\)
\begin{itemize}
\item Easy to optimize because they are similar to linear units
\begin{itemize}
\item Only difference is that a rectified unit outputs zero across half its domain
\end{itemize}
\end{itemize}

\item Rectified linear units are typically used on top of an affine transformation
\end{itemize}
\begin{equation}
  \pmb h = g(\pmb W^\top \pmb x + \pmb b)
\end{equation}
\begin{itemize}
\item When initializing the parameters of the affine transformation it can be a good pratise to set all elements of \(\pmb b\) to a small positive value
\item A drawback to rectified linear units is that they cannot learn via gradient based methods on examples for which their activation is zero
\begin{itemize}
\item Some generalizations of rectified linear units guarantee that they receive gradient everywhere
\end{itemize}

\item \textbf{Maxout units} generalize rectified linear units further
\begin{itemize}
\item Instead of applying an element-wise function \(g(z)\) maxout units divide \(\pmb z\) into groups of \(k\) values
\item Each maxout unit the outputs the maximum element of one of these groups
\end{itemize}
\end{itemize}
\begin{equation}
  g(\pmb z)_i = \max_{j\in \mathbb G^{(u)}} z_j
\end{equation}
\begin{itemize}
\item where \(\mathbb G^{(I)}\) is the set of indices into the inputs for group \(i\)

\item A maxout unit can be seen as learning the activation function itself rather than just the relationship between units
\end{itemize}

\subsubsection{Logistic Sigmoid and Hyperbolic Tangent}
\label{sec:org1a98ebe}
\begin{itemize}
\item The widespread saturation of sigmoid units can make gradient-based learning very difficult
\begin{itemize}
\item Their use as hidden units in feedforward networks are discouraged
\item Training using the \(tanh\) function for hidden layers are easier
\end{itemize}
\end{itemize}

\subsection{Architecture Design}
\label{sec:org9a1687c}
\subsubsection{General}
\label{sec:orga419bf4}
\begin{itemize}
\item \textbf{Architecture} refers to the overall structure of the network
\begin{itemize}
\item How many units it should have
\item How these units should be connected to each other
\end{itemize}

\item Most NNs are organized into groups of units called layers
\begin{itemize}
\item They are often arranged in a chain structure where each layer is a function of the layer that preceded it
\item The ith layer is given by
\end{itemize}
\end{itemize}
\begin{equation}
  h^{(i)} = g^{(i)}(\pmb W ^{(i)T}\pmb h^{(i)} + \pmb b^{(i)})
\end{equation}
\begin{itemize}
\item where the first layer uses \(x\) instead of \(h^{(i)}\)

\item In chain based architectures, the main architectural considerations are to choose the the depth of the network and the width of each layer 
\begin{itemize}
\item The ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error.
\end{itemize}
\end{itemize}

\subsubsection{Universal Approximation Properties and Depth}
\label{sec:org6623b49}
\begin{itemize}
\item The universal approximation theorem means that regardless of what function we are trying to learn, we know that a large multi layered perceptron will be able to represent this function.
\begin{itemize}
\item However, we are not guaranteed that the training algorithm will be able to learn that function
\item Learning it can fail for two different reasons
\begin{enumerate}
\item The optimization algorithm used for training may not be able to find the value of the parameters corresponds to the desired function
\item The training algorithm might choose the wrong function due to overfitting
\end{enumerate}
\end{itemize}

\item Feedforward networks provide a universal system for representing functions, in the sense that, given a function, there exists a feedforward network that approximates the function.
\begin{itemize}
\item There is no universal procedure for examining a training set of specific examplesd and choosing a function that will generalize to point not in the training set
\end{itemize}

\item The universal approximation theorem says that there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.

\item A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.
\begin{itemize}
\item In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error
\end{itemize}

\item The number of linear regions carved out by a deep rectifier network with \(d\) inputs, depth \(l\), and \(n\) units per hidden layer is
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-01_17-14-50.png}
\end{center}

\begin{itemize}
\item Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions.
\end{itemize}

\subsubsection{Other Architectural Considerations}
\label{sec:orgf14d57d}
\begin{itemize}
\item The layers need not be connected in a chain, but it is the most common practice.
\begin{itemize}
\item Many architectures build a main chain but then add extra architectural features to it
\begin{itemize}
\item Such as skip connections going from layer i to layer i + 2 or higher.
\end{itemize}
\item These skip connections make it easier for the gradient to ﬂow from output layers to layers nearer the input.
\end{itemize}

\item A key consideration of architecture design is how to connect a pair of layers to each other
\begin{itemize}
\item The default way is having every input unit connected to every output unit
\item Strategies for reducing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network,
\begin{itemize}
\item They are often highly problem-dependent.
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Back-Propagation and Other Differentiation Algorithms}
\label{sec:org5b178f8}
\subsubsection{General}
\label{sec:orgbc05afd}
\begin{itemize}
\item \textbf{Forward propagation} is when the inputs \(\pmb x\) provide initial information that then propaget up to the hidden units at each layer and finally produces an output \(\hat{\pmb y}\)
\begin{itemize}
\item During training it can continue onward until it produces a scalar cost \(J(\pmb \theta)\)
\end{itemize}

\item The \textbf{back-propagation} algorithm (\textbf{backprop}) allows the information from the cost to flow backwards through the network in order to compute the gradient
\begin{itemize}
\item It is used to compute the gradient, another algorithm is used to do the learning e.g. stochastic gradient descent
\end{itemize}
\end{itemize}

\subsubsection{Computational Graphs}
\label{sec:org6bb40b5}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-01_17-49-44.png}
\caption{\label{fig:org9f740e3}
Computational Graph Examples}
\end{figure}

\begin{itemize}
\item An \textbf{operation} is a simple function of one or more variables
\begin{itemize}
\item Defined to return only a single output variable
\end{itemize}

\item The graph language is accompanied by a set of allowable operations
\begin{itemize}
\item If a variables \(y\) is computed by applying an operation to a variable \(x\), then there is drawn a directed edge from \(x\) to \(y\)
\item The output node is sometimes annotated with the name of the operation applied
\end{itemize}
\end{itemize}

\subsubsection{Chain Rule of Calculus}
\label{sec:org6aad393}
\begin{itemize}
\item Back-propagation computes the chain rule, with a specific order of operations that is highly efficient
\item The chain rule can generalize beyond the scalar case suppose that \(\pmb x \in \mathbb R^m, \pmb y \in \mathbb R^n\), \(g\) maps from \(\mathbb R^m\) to \(\mathbb R^n\), and \(f\) maps from \(\mathbb R^n\) to \(R\). If \(\pmb y = g(\pmb x)\) and \(z = f(\pmb y)\), then
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-01_17-55-43.png}
\end{center}
\begin{itemize}
\item In vector notation, this may equivalently be written as
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-01_17-56-15.png}
\end{center}
\begin{itemize}
\item where \(\frac{\partial \pmb y}{\partial \pmb x}\)  is the \(n \times m\) Jacobian matrix of \(g\)

\item The back-propagation algorithm consists of performing Jacobian-gradient product given by the chain rule for each operation in the graph
\item The back-propagation algorithm is typically applied to tensors of arbitrary dimensionality
\begin{itemize}
\item Is exactly the same as back-propagation with vector conceptually
\item Denoting a gradient of a value \(z\) with respoect to a tensor \$s.
\end{itemize}
\item To denote the gradient of a value z with respect to a tensor \(\pmb X\), we write \(\nabla_{\pmb X} z\)
\begin{itemize}
\item The indices into \(\pmb X\) have multiple coordinates
\end{itemize}

\item The chain rule for tensors:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-01_18-22-42.png}
\end{center}

\subsubsection{Recursively Applying the Chain Rule to Obtain Backprop}
\label{sec:org716fd86}
\begin{itemize}
\item Given a scalar \(u^{(n)}\) which is the quantity whose gradient we want to obtain with respect to all the \(n_i\) input nodes \(u^{(1)}\) to \(u^{(n_i)}\)
\begin{itemize}
\item We wish to compute \(\frac{\partial u^{(n)}}{\partial u^{(i)}}\) for \(i \in \{1,2,\dots,n_i\}\)
\item In backprop \(u^{(n)}\) will be the cost associated with an example or a minibatch
\item \(u^{(1)}\) to \(u^{(n_i)}\) correspond to the parameters of the model
\item The nodes of the graph is assumed to be order in such a way that we can compute their output one after the other
\item Each node \(u^{(i)}\) is associated with an operation \(f^{(i)}\) and is computed by evaluating the function
\end{itemize}
\end{itemize}
\begin{equation}
  u^{(i)} = f(\mathbb A ^{(i)})
\end{equation}
\begin{itemize}
\item where \(\mathbb A ^{(i)}\) is the set of all nodes that are parents of \(u^{(i)}\)
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-04-33.png}
\end{center}

\begin{itemize}
\item The forward propagation computation is put in a graph \(\mathcal G\)
\begin{itemize}
\item In order to perform back-propagation, one can constructs a computational graph that depends on \(\mathcal G\) and add to it an extra set of nodes
\begin{itemize}
\item These form a subgraph \(\mathcal B\) with one node per node of \(\mathcal G\)
\item Computation in \(\mathcal B\) proceeds in the reverse of the order of computation in \(\mathcal G\)
\item Each node of \(\mathcal B\) computes the derivative \(\frac{\partial u^{(n)}}{\partial u^{i}}\) associated with the forward graph node \(u^{(i)}\) using the chain rule
\end{itemize}
\item The subgraph \(\mathcal B\) contains one edge for each edge for each edge of \(\mathcal G\)
\begin{itemize}
\item The edge from \(u^{(j)}\) to \(u^{(i)}\) is associated with the computation of \(\frac{\partial u^{(i)}}{\partial u ^{(j)}}\)
\item The dot product is performed for each node between the gradient already computed with respect to nodes \(u^{(i)}\) that are children of \(u^{(j)}\) and the vector containing the partial derivatives \(\frac{\partial u^{(i)}}{\partial u ^{(j)}}\) for the same children nodes
\end{itemize}
\end{itemize}

\item The amount of computation required for performing back-propagration scales linearly with the number of edges in \(\mathcal G\)
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-18-42.png}
\end{center}

\subsubsection{Back-Propagation Computation in Fully-Connected MLP}
\label{sec:org032cc6e}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-22-46.png}
\end{center}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-23-03.png}
\end{center}

\subsubsection{Symbol-to-Symbol Derivatives}
\label{sec:org63af61d}
\begin{itemize}
\item \textbf{Symbolic Representations} is algebraic expressions and computational graphs that both operate on symbols, or variables that do not have speciﬁc values.

\item Some approaches to back-propagation take a computational graph and a set of numerical values for the inputs to the graph, then return a set of numerical values describing the gradient at those input values.
\begin{itemize}
\item This is called "symbol-to-number" diﬀerentiation
\end{itemize}

\item Another approach for backprop is to take a computational graph and add additional nodes to the graph that provide a symbolic description of the desired derivativess
\end{itemize}

\subsubsection{General Back-Propagation}
\label{sec:orge9b45b2}
\begin{itemize}
\item Each node in the graph \(\mathcal G\) corresponds to a variable
\begin{itemize}
\item This is described as being a tensor \(\mathbf{\mathsf V}\)
\item Tensor can in general have any number of dimensions
\item They subsume scalars, vectors, and matrices
\end{itemize}

\item It is assumed that each variable \(\mathbf{\mathsf V}\) is associated with the following subroutines:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-51-44.png}
\end{center} 

\begin{itemize}
\item Each operation \texttt{op} is also associated with a \texttt{bprop} operation
\begin{itemize}
\item This \texttt{bprop} operation can compute a Jacobian vector product
\item Formally, \texttt{op.bprop(inputs,X,G)} must return:
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_08-58-55.png}
\end{center}
\begin{itemize}
\item Inputs is a list of inputs that are supplied to the operation
\item \texttt{op.f} is the mathematical function that the operation implements
\item \(\mathbf{\mathsf X}\) is the input whose gradient we which to compute
\item \(\mathbf{\mathsf G}\) is the gradient on the output of the operation
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_09-04-05.png}
\end{center}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/screenshot_2018-10-02_09-04-43.png}
\end{center}

\begin{itemize}
\item The backprop algorithm uses dynamic programming to get a better running time
\end{itemize}

\subsection{Backpropagation equations}
\label{sec:org99aefeb}
\begin{itemize}
\item \(w_{ji}^l\) is the weight from neuron \(i\) in layer \(l-1\) to neuron \(j\) in layer \(l\)
\item \(a_j^1 = x_j\)
\item \(s_j^l=\sum_ia_i^{l-1}w_{j,i}^{l-1}+b_j^{l-1}\)
\item \(a_k^l = \Phi(s_k^l)\)
\item \(\delta_j^l=\frac{\partial e}{\partial s_j^l}\)
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/Screenshot-20181009083230-1068x588_2018-10-09_08-33-23.png}
\end{center}

\begin{center}
\includegraphics[width=.9\linewidth]{Deep Feedforward Networks/Screenshot-20181009084054-933x797_2018-10-09_08-41-57.png}
\end{center}

\section{Convolutional Networks}
\label{sec:org56bb8ce}
\subsection{General}
\label{sec:org8a95d38}
\begin{itemize}
\item \textbf{Convolutional Networks} (CNNs) are a specialized kind of neural network for processing data that has a grid-like topology
\begin{itemize}
\item \textbf{Convolution} is a specialized kind of linear operation
\item Convolutional networks are neural networks that use convolution in place of general matrix multiplication in at least one of their layers
\end{itemize}
\end{itemize}

\subsection{The Convolution Operation}
\label{sec:org75a4776}
\begin{itemize}
\item A convolution is in its most general form an operation on two functions of a real valued argument
\begin{itemize}
\item Example of a convolution: \(s(t) \int x(a)w(t-a)da\)
\item The convolution operation is typically denoted with an asterisk: \(s(t) = (x*w)(t)\)
\item The first argument to the convolution is often referred to as the \textbf{input}
\item The second argument is referred to as the \textbf{kernel}
\item The output is referred to as the \textbf{feature map}
\item In machine learning applications
\begin{itemize}
\item The input is usually a multidimensional array of data
\item The kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm.
\item The multidimensional dimensional arrays are referred to as tensors
\item Because each element of the input and kernel must be explicitly stored separately, it is often assumed that these functions are zero everywhere but in the finite set of points for which we store the value
\end{itemize}
\item Convolutions are often used over more than one axis at a time
\begin{itemize}
\item e.g. on a two-dimensional image \(I\) as input one would probably use a two dimensional kernel \(K\)
\end{itemize}
\item Convolution is commutative which means that for a two dimensional kernel \(K\) and a input \(I\): \(S(i,j) = (I*K)(i,j)=(K*I)(i,j)\)
\begin{itemize}
\item The last one is usually more straightforward to implement in a machine learning library, since there is less variation in the range of valid values for \(m\) and \(n\)
\end{itemize}
\item Many neural network libraries implement are related function called \textbf{cross-correlation}, which is the same as convolution but without flipping the kernel
\begin{itemize}
\item e.g. \(S(i,j) = (K*I)(i,j)= \sum_m\sum_nI(i+m,j+n)K(m,n)\)
\item Some also call this convolution
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Motivation}
\label{sec:org45f41f8}
\begin{itemize}
\item Convolution leverages three important ideas that can help improve a machine learning system:
\begin{itemize}
\item \textbf{Sparse interactions} is accomplished by making the kernel smaller than the input
\begin{itemize}
\item It is also referred to as \textbf{parse connectivity} \textbf{or *sparse weights}
\item e.g. one can detect small meaningful features in images such as edges with kernels
\item One needs to store fewer parameters which reduces the memory requirements of the model and improves its statistical efficiency
\begin{itemize}
\item Computing the output requires fewer operations
\item The improvements in efficiency are usually quite large
\end{itemize}
\end{itemize}

\item \textbf{Parameter sharing:} refers to using the same parameter for more than one function in the model
\begin{itemize}
\item One can say that a network has \textbf{tied weights}, because the value of the weight applied to one input is tied to the value of a weight applied elsewhere
\item Each member of the kernel is used at every position of the input
\end{itemize}

\item \textbf{Equivariant representations}: If \(g\) is any function that translates the input then that is shifts is, then the convolution function is equivalent to \(g\) 
\begin{itemize}
\item A function \(f\) is equivalent to a function \(g\) if \(f(g(x))=g(f(x))\)
\end{itemize}
\end{itemize}

\item Some kinds of data cannot be processed by neural networks deﬁned bymatrix multiplication with a ﬁxed-shape matrix. Convolution enables processing of some of these kinds of data.
\end{itemize}

\subsection{Pooling}
\label{sec:org94f1401}
\begin{center}
\includegraphics[width=.9\linewidth]{Convolutional Networks/screenshot_2018-10-08_10-23-35.png}
\end{center}

\begin{itemize}
\item A typical layer of a convolutional network consists of three stages
\begin{enumerate}
\item In the first stage the layer performs several convolutions in parallel to produce a set of linear activations
\item In the second stage each linear activation is run through a nonlinear activation, such as rectified linear activation function
\begin{itemize}
\item Is sometimes called the \textbf{detector stage}
\end{itemize}
\item The third stage we use a \textbf{pooling function} to modify the output of the layer further
\begin{itemize}
\item It replaces the output of the net at a certain location with a summary statistic of the nearby output
\begin{itemize}
\item e.g. the \textbf{max pooling operation} which reports the maximum output within a rectangular neighborhood
\end{itemize}
\item Pooling helps to make the representation approximately \textbf{invariant} to small translation of the input
\begin{itemize}
\item Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs
\item Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.
\end{itemize}
\item It is possible to use fewer pooling units than detector units by reporting summary statistics for pooling regions spaced \(k\) pixels apart rather than \(1\) pixels apart
\begin{itemize}
\item Improves computational efficiency of the network because the next layer has approximately \(k\) times fewer inputs to process
\item Can be used to handle images of variable size by changing how much it is space depending on the input size
\end{itemize}
\end{itemize}
\end{enumerate}
\end{itemize}

\section{Tree-Based Methods}
\label{sec:org04dce68}
\subsection{Background}
\label{sec:org879c29a}
\begin{itemize}
\item Three based methods partition the feature space into a set of rectables and then fit a simple model in each one
\begin{itemize}
\item They are simple yet powerful
\item One first split the space into two regions and models the response by the mean of \(Y\) in each region, then one or both of the regions are split into two more regions, this process is continued until some stopping rule is applied
\end{itemize}
\end{itemize}

\subsection{Regression Trees}
\label{sec:orge76d02c}
\begin{itemize}
\item The data consists of \(p\) inputs and a response, for each of \(N\) observations, that is \((x_i,y_i)\) for \(i=1,2,\dots,N\), with \(x_i=(x_{i1}, x_{i2}, \dots, x_{ip})\)
\item The algorithm needs to automatically decide on the splitting variables and split points and what topology (shape) the three should have
\item If one have a partition into \(M\) regions \(R_1,R_2,\dots,R_M\), and model the response as a constant \(c_m\) in each region:
\end{itemize}
\begin{equation}
  f(x) = \sum_{m=1}^Mc_mI(x \in R_m)
\end{equation}
\begin{itemize}
\item If the criterion is minimization of the sum of squares \(\sum(y_i-f(x_i))^2\), he best \(\hat c_m\) is just the average of \(y_i\) in the region \(R_m\):
\end{itemize}
\begin{equation}
  \hat c_m = \text{ave}(y_i \mid x_i \in R_m ).
\end{equation}
\begin{itemize}
\item Since the binary partition in terms of minimum sum of squares is generally computationally infeasible one needs a greedy algorithm:
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Tree-Based Methods/screenshot_2018-10-08_16-15-20.png}
\end{center}
\begin{itemize}
\item For each splitting variable, the determination of the split point \(s\) can be done very quickly by scanning through all the inputs
\begin{itemize}
\item Having found the best split, one partition the data into the two resulting regions and repeat the splitting process on each of the two regions, which is the repeat on all the resulting regions
\item The optimal tree size should be adaptively chosen from the data
\begin{itemize}
\item A preferred strategy is to grow a large \(T_0\) stopping the splitting process only when some minimum node size is reached
\item The large tree is then pruned using cost-complexity pruning
\end{itemize}
\end{itemize}

\item A \textbf{subtree} \(T \subset T_0\) is defined to be any tree that can be obtained by \textbf{pruning} \(T_0\)
\begin{itemize}
\item Pruning is collapsing any number of its internal (non-terminal nodes).
\item Terminal nodes is indexed by \(m\), with node \(m\) representing region \(R_m\)
\item \(|T|\) denotes the number of terminal nodes in \(T\)
\end{itemize}
\item Letting
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Tree-Based Methods/screenshot_2018-10-08_16-27-26.png}
\end{center}
\begin{itemize}
\item The idea is to find for each \(\alpha\) the subtree \(T_\alpha \subseteq T_0\) to minimize \(C_\alpha(T)\)
\begin{itemize}
\item The tuning parameter \(\alpha \geq 0\) governs the tradeof between tree size and its goodness of fit to the data
\item Larger values of \(\alpha\) result in smaller trees \(T_\alpha\) and the converse for smaller values of \(\alpha\)
\item With \(\alpha = 0\) the solution is the full tree \(T_0\)
\item For each \(\alpha\) there is a unique smallest subtree \(T_\alpha\) that minimizes \(C_\alpha(T)\)
\end{itemize}

\item If one want to find \(T_\alpha\) one uses \textbf{weakest link pruning}:
\begin{itemize}
\item One successively collapse the internal node that produces the smallest per-node increase in \(\sum_m N_mQ_m(T)\) until we produce P and continue until we produce the single-node (root) tree.
\item This gives a finite sequence of subtrees and \(T_\alpha\) must be one of these subtrees
\item Estimation of \(\alpha\) is achieved by five-fold or ten-fold cross-validation:
\begin{itemize}
\item The value \(\hat \alpha\) is chosen to minimize the cross-validated sum of squares
\item The final tree is \(T_{\hat \alpha}\)
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Classification Trees}
\label{sec:orgfcc2f17}
\begin{center}
\includegraphics[width=.9\linewidth]{Tree-Based Methods/screenshot_2018-10-08_16-49-37.png}
\end{center}


\begin{itemize}
\item If the target is classification outcome taking values \(1,2,\dots, K\) the only changes needed is the tree algorithm is the criteria for splitting nodes and pruning the tree
\begin{itemize}
\item In a nodes \(m\), representing a region \(R_m\) with \(N_m\) observations, let
\end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{Tree-Based Methods/screenshot_2018-10-08_16-47-14.png}
\end{center}
\begin{itemize}
\item The Gini index and cross-entropy are differentiable and are therefore more amenable to numerical optimization
\begin{itemize}
\item They are often used for growing the three
\item To guide the cost-complexity pruning any of the three measures can be used but typically it is the misclassification rate
\end{itemize}
\end{itemize}

\subsection{Other Issues}
\label{sec:org5034331}
\begin{itemize}
\item \textbf{Categorial Predictors}
\begin{itemize}
\item When splitting a predictor having \(q\) possible unordered values, there are \(2^{q-1}-1\) possible partitions of the \(q\) values into two groups
\begin{itemize}
\item The computations become prohibitive for large \(q\)
\end{itemize}
\item With a \(0-1\)  outcome the computation simplifies
\begin{itemize}
\item One orders the predictor classes according to the proportion falling in outcome class \(1\)
\item This gives the optimal split in terms of cross-entropy or Gini index- among all possible \(2^{q-1}-1\) splits
\end{itemize}
\end{itemize}

\item \textbf{The Loss Matrix}: A \(KxK\) loss matrix \(\pmb L\), is defined with \(L_{kk'}\) being the loss incurred for classifying a class \(k\) observation as class \(k'\)
\begin{itemize}
\item Typically no loss is incurred for correct classifications, that is \(L_{kk}= 0 \ \forall k\)
\item To incorporate P the losses into the modeling process, one could modify the Gini index to \(\sum_{k\ne k'} L_{kk'} \hat p_{mk} \hat p_{mk'}\)
\item This does not help in the two-class case and a better approch is to weight the observations in class \(k\) by \(L_{kk'}\)
\end{itemize}

\item \textbf{Missing Predictor Values:} If some of the data has some missing predictor values in some of the values
\begin{itemize}
\item There are two better approaches than throwing the data away
\begin{enumerate}
\item The first is applicable to variable predictors, where one simply makes a new category for \emph{"missing"}
\begin{itemize}
\item This might make one discover that the observation with missing values for some measurement behave differently that those with nonmissing values
\end{itemize}
\item The second is a more general approach which is the construction of surrogate variables
\begin{itemize}
\item When considering a predictor for a split, we use only the observations for which that predictor is not missing
\item Having chosen the best (primary) predictor and split point, we form a list of surrogate predictors and split points.
\begin{itemize}
\item The first surrogate is the predictor and corresponding split point that best mimics the split of the training data achieved by the primary split.
\item The second surrogate is the predictor and corresponding split point that does second best, and so on.
\end{itemize}
\end{itemize}
\end{enumerate}
\end{itemize}

\item \textbf{Why Binary Splits?}
\begin{itemize}
\item The problem with using multiway splits is that it fragment the data too quickly, leaving insufficient data at the next level down.
\item Since multiway splits can be achieved by a series of binary splits, the binary splits are preferred.
\end{itemize}
\end{itemize}

\section{Random forests}
\label{sec:org6165b83}
\subsection{Introduction}
\label{sec:orgc0955a0}
\begin{itemize}
\item \textbf{Bagging} or \textbf{bootstrap aggregation} is a technique for reducing the variance of an estimated prediction function
\begin{itemize}
\item Bagging works seems to work especially well for high-variance, low-ias procedures such as trees
\item For regression we simply fit the same regression tree many time to bootstrapsampled versions of the training data and average the result
\item For classification, a \textbf{committee} of trees each cast a vote for the predicted class
\end{itemize}

\item \textbf{Random forests} is a substantial modification of bagging that builds a large collection of de-correlated trees and averages them
\begin{itemize}
\item On many problems its performance is very similar to boosting
\item They are simpler to train and tune than the boosting example
\end{itemize}
\end{itemize}

\subsection{Bootstrap aggregating technique}
\label{sec:orgddc5c31}
\begin{itemize}
\item Given a standard training set \(D\) of size \(n\)
\begin{itemize}
\item Bagging generates \(m\) new training set \(D_i\) each of size \(n'\) by sampling from \(D\) uniformly and with replacement
\begin{itemize}
\item This is know as a \textbf{bootstrap} sample
\end{itemize}
\item The \(m\) models are fitted using the bootstrap sample and combine by
\begin{itemize}
\item Averaging the output (for regresion)
\item Voting (for classification)
\end{itemize}
\end{itemize}
\end{itemize}

\subsection{Definition of Random Forests}
\label{sec:org2f43ccd}
\begin{center}
\includegraphics[width=.9\linewidth]{Random forests/screenshot_2018-10-08_20-10-57.png}
\end{center}

\begin{itemize}
\item The essential idea in bagging is to average many noisy but approximately unbiased models
\begin{itemize}
\item This reduces the variance
\item Trees are ideal candidates for bagging since:
\begin{itemize}
\item They can capture complex interaction structures in the data
\item If grown sufficiently deep, they have relative low bias
\end{itemize}
\item Since trees are very noisy they benefit greatly from averaging
\item The bias of bagged trees is the same as that of the individual (bootstrap) trees
\begin{itemize}
\item The only hope of improvement is through variance reduction
\end{itemize}
\end{itemize}

\item An average of \(B\) independent identically distributed random variables, each with variance \(\sigma ^2\) has variance \(\frac1B\sigma^2\)
\begin{itemize}
\item If the variables are simply identically distributed with positive correlation \(\rho\) the variance of the average is
\end{itemize}
\end{itemize}
\begin{equation}
  \rho \sigma^2 + \frac{1-\rho}B \sigma^2
\end{equation}
\begin{itemize}
\item As \(B\) increases, the second terms disappears but the first remains
\begin{itemize}
\item The size of the correlation of pairs of bagged trees limits the benefits of averaging
\item The idea in random forest is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance to much
\begin{itemize}
\item This is achieved in the tree-growing process through random selection of the input variables
\item Typical values for \(m\) are \(\sqrt p\) or even as low as \(1\)
\item Reducing \(m\) will reduce the correlation between any pair of trees in the ensemble and therefore reduce the variance of the average
\end{itemize}
\item After \(B\) such trees \(\{T(x;\Theta_b)\}^B_1\) are grown the random forest (regression) predictor is
\end{itemize}
\end{itemize}
\begin{equation}
  \hat f_{rf}^B(x) = \frac1B\sum_{b=1}^BT(x;\Theta_b)
\end{equation}
\begin{itemize}
\item Where \(\Theta_b\) characterizes the bth random forest tree in terms of split variables, cutpoints at each nodes, and terminal-node values
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{Random forests/screenshot_2018-10-09_08-08-09.png}
\end{center}
\end{document}